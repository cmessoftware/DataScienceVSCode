{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91f46105",
   "metadata": {},
   "source": [
    "![Image in a markdown cell](https://cursos.utnba.centrodeelearning.com/pluginfile.php/1/theme_space/customlogo/1738330016/Logo%20UTN%20Horizontal.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6e30a0",
   "metadata": {},
   "source": [
    "# **Diplomado de Ciencia de Datos y AnÃ¡lisis Avanzado**\n",
    "# **Unidad 5: Modelado Predictivo I**: RegresiÃ³n y ClasificaciÃ³n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4b78f7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064b6a92",
   "metadata": {},
   "source": [
    "# **Proyecto de Competencia Kaggle: PredicciÃ³n de Abandono de Clientes**\n",
    "\n",
    "## **Curso:** Diplomado en Ciencia de Datos\n",
    "\n",
    "# **Nombres de los Miembros del Equipo:**\n",
    "### *   [Nombre Completo del Miembro 1]\n",
    "### *   [Nombre Completo del Miembro 2]\n",
    "### *   [Nombre Completo del Miembro 3]\n",
    "\n",
    "# **Objetivo:**\n",
    "## El objetivo de este proyecto es construir y evaluar varios modelos de clasificaciÃ³n para predecir si un cliente de una compaÃ±Ã­a de telecomunicaciones abandonarÃ¡ o no el servicio (churn). El rendimiento final del mejor modelo se medirÃ¡ en la competencia de Kaggle a travÃ©s de la **mÃ©trica ROC AUC**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee70eac",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeaefbbe",
   "metadata": {},
   "source": [
    "# **Enlace para unirse a la competencia**\n",
    "### **USE EL ENLACE PARA UNIRSE POR EQUIPO, NO DE MANERA INDIVIDUAL**\n",
    "\n",
    "https://www.kaggle.com/t/57b70c381e4d451b8ae38e164b91a2aa\n",
    "\n",
    "\n",
    "### **Por favor siga las indicaciones que se suministran en la plataforma**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2042205",
   "metadata": {},
   "source": [
    "# **0. ConfiguraciÃ³n Inicial e ImportaciÃ³n de LibrerÃ­as**\n",
    "\n",
    "## En esta secciÃ³n, importaremos todas las librerÃ­as necesarias para el proyecto. Es una buena prÃ¡ctica tener todas las importaciones en la primera celda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7af8c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importaciones bÃ¡sicas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configurar visualizaciones\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "# Importar mÃ³dulos del proyecto\n",
    "from TP5 import data_loader, dataset_splitter, eda, models, metrics\n",
    "\n",
    "print(\"âœ… Todas las librerÃ­as y mÃ³dulos importados correctamente\")\n",
    "print(\"ðŸš€ Proyecto listo para ejecutar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6955fb29",
   "metadata": {},
   "source": [
    "# **1. Carga de Datos**\n",
    "\n",
    "## Cargaremos los datasets proporcionados para la competencia usando nuestro mÃ³dulo de carga de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86647726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar datos usando el mÃ³dulo data_loader\n",
    "print(\"ðŸ“¥ Cargando datos de la competencia...\")\n",
    "\n",
    "try:\n",
    "    # Intentar cargar datos reales de la competencia\n",
    "    train_data, test_data, sample_submission = data_loader.load_competition_data()\n",
    "    print(\"âœ… Datos de competencia cargados exitosamente\")\n",
    "except FileNotFoundError:\n",
    "    print(\"âš ï¸ Archivos de competencia no encontrados, generando datos de ejemplo...\")\n",
    "    # Generar datos de ejemplo para desarrollo\n",
    "    train_data, test_data, sample_submission = data_loader.create_sample_data()\n",
    "    print(\"âœ… Datos de ejemplo generados exitosamente\")\n",
    "\n",
    "# Mostrar informaciÃ³n de los datos\n",
    "data_info = data_loader.get_data_info(train_data, test_data)\n",
    "print(f\"\\nðŸ“Š InformaciÃ³n de los datos:\")\n",
    "for key, value in data_info.items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "\n",
    "print(f\"\\nðŸ“‹ Primeras 5 filas del dataset de entrenamiento:\")\n",
    "display(train_data.head())\n",
    "\n",
    "print(f\"\\nðŸ“‹ Primeras 5 filas del dataset de prueba:\")\n",
    "display(test_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b7ce4e",
   "metadata": {},
   "source": [
    "# **2. DivisiÃ³n de Datos**\n",
    "\n",
    "## Dividiremos los datos en conjuntos de entrenamiento, validaciÃ³n y prueba usando nuestro mÃ³dulo dataset_splitter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261a8b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar datos para divisiÃ³n\n",
    "print(\"ðŸ“Š Preparando datos para divisiÃ³n...\")\n",
    "\n",
    "# Separar caracterÃ­sticas y variable objetivo\n",
    "if 'Churn' in train_data.columns:\n",
    "    X = train_data.drop(['Churn'], axis=1)\n",
    "    y = train_data['Churn']\n",
    "else:\n",
    "    # Si la columna objetivo tiene otro nombre, ajustar aquÃ­\n",
    "    target_col = train_data.columns[-1]  # Asumir que la Ãºltima columna es el target\n",
    "    X = train_data.drop([target_col], axis=1)\n",
    "    y = train_data[target_col]\n",
    "\n",
    "# Dividir datos usando dataset_splitter\n",
    "splitter = dataset_splitter.DataSplitter(test_size=0.2, val_size=0.2, random_state=42)\n",
    "\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = splitter.split_data(X, y)\n",
    "\n",
    "print(f\"âœ… Datos divididos exitosamente:\")\n",
    "print(f\"   - Entrenamiento: {X_train.shape[0]:,} muestras\")\n",
    "print(f\"   - ValidaciÃ³n: {X_val.shape[0]:,} muestras\")\n",
    "print(f\"   - Prueba: {X_test.shape[0]:,} muestras\")\n",
    "print(f\"   - CaracterÃ­sticas: {X_train.shape[1]}\")\n",
    "\n",
    "# Mostrar distribuciÃ³n del target en cada conjunto\n",
    "print(f\"\\nðŸŽ¯ DistribuciÃ³n de Churn por conjunto:\")\n",
    "print(f\"   - Entrenamiento: {y_train.mean()*100:.1f}% churn\")\n",
    "print(f\"   - ValidaciÃ³n: {y_val.mean()*100:.1f}% churn\")\n",
    "print(f\"   - Prueba: {y_test.mean()*100:.1f}% churn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae40074",
   "metadata": {},
   "source": [
    "# **3. AnÃ¡lisis Exploratorio de Datos (EDA)**\n",
    "\n",
    "## Realizaremos un anÃ¡lisis exploratorio completo usando nuestro mÃ³dulo EDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c250d03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizar EDA completo usando el mÃ³dulo eda\n",
    "print(\"ðŸ” Iniciando AnÃ¡lisis Exploratorio de Datos...\")\n",
    "\n",
    "# InformaciÃ³n bÃ¡sica del dataset\n",
    "eda.basic_info(train_data)\n",
    "\n",
    "# AnÃ¡lisis de la variable objetivo\n",
    "eda.analyze_target(train_data, 'Churn' if 'Churn' in train_data.columns else train_data.columns[-1])\n",
    "\n",
    "# AnÃ¡lisis de caracterÃ­sticas numÃ©ricas\n",
    "eda.analyze_numerical_features(train_data)\n",
    "\n",
    "# AnÃ¡lisis de caracterÃ­sticas categÃ³ricas\n",
    "eda.analyze_categorical_features(train_data)\n",
    "\n",
    "# AnÃ¡lisis de correlaciones\n",
    "eda.correlation_analysis(train_data)\n",
    "\n",
    "print(\"\\nâœ… AnÃ¡lisis Exploratorio completado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488f8f40",
   "metadata": {},
   "source": [
    "# **4. Preprocesamiento de Datos**\n",
    "\n",
    "## Prepararemos los datos para el modelado usando ChurnPredictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6a3b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocesamiento de datos usando ChurnPredictor\n",
    "from TP5.models import ChurnPredictor\n",
    "\n",
    "print(\"ðŸ”§ Iniciando preprocesamiento de datos...\")\n",
    "\n",
    "# Inicializar el predictor\n",
    "predictor = ChurnPredictor(random_state=42)\n",
    "\n",
    "# Crear el preprocesador\n",
    "preprocessor = predictor.create_preprocessor(X_train)\n",
    "\n",
    "print(\"âœ… Preprocesador configurado exitosamente\")\n",
    "print(f\"ðŸ“Š CaracterÃ­sticas a procesar: {X_train.shape[1]}\")\n",
    "\n",
    "# Mostrar informaciÃ³n del preprocesador\n",
    "print(\"\\nðŸ”§ ConfiguraciÃ³n del preprocesador:\")\n",
    "numeric_features = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_features = X_train.select_dtypes(include=['object']).columns\n",
    "print(f\"   - CaracterÃ­sticas numÃ©ricas: {len(numeric_features)}\")\n",
    "print(f\"   - CaracterÃ­sticas categÃ³ricas: {len(categorical_features)}\")\n",
    "print(f\"   - NumÃ©ricas: {list(numeric_features)}\")\n",
    "print(f\"   - CategÃ³ricas: {list(categorical_features)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044762e8",
   "metadata": {},
   "source": [
    "# **5. Entrenamiento de Modelos**\n",
    "\n",
    "## Entrenaremos mÃºltiples modelos de Machine Learning usando ChurnPredictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75936488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamiento de modelos usando ChurnPredictor\n",
    "print(\"ðŸ¤– Iniciando entrenamiento de modelos...\")\n",
    "\n",
    "# Crear los modelos\n",
    "models_dict = predictor.create_models()\n",
    "\n",
    "# Entrenar todos los modelos\n",
    "predictor.train_models(X_train, y_train)\n",
    "\n",
    "print(\"\\nðŸŽ‰ Entrenamiento completado para todos los modelos:\")\n",
    "for model_name in models_dict.keys():\n",
    "    print(f\"   âœ… {model_name}\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Modelos entrenados con {len(X_train):,} muestras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e9d0d7",
   "metadata": {},
   "source": [
    "# **6. EvaluaciÃ³n de Modelos**\n",
    "\n",
    "## Evaluaremos todos los modelos y seleccionaremos el mejor usando mÃ©tricas completas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4aeca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EvaluaciÃ³n de modelos\n",
    "from TP5.metrics import MetricsCalculator\n",
    "\n",
    "print(\"ðŸ“Š Evaluando modelos...\")\n",
    "\n",
    "# Evaluar con el predictor\n",
    "results = predictor.evaluate_models(X_val, y_val)\n",
    "\n",
    "# Obtener el mejor modelo\n",
    "best_model_name, best_model = predictor.get_best_model('ROC_AUC')\n",
    "\n",
    "# Generar reporte completo\n",
    "predictor.generate_model_report(X_val, y_val)\n",
    "\n",
    "# Usar el calculador de mÃ©tricas para anÃ¡lisis detallado del mejor modelo\n",
    "calc = MetricsCalculator()\n",
    "\n",
    "# Predicciones del mejor modelo\n",
    "y_pred = best_model.predict(X_val)\n",
    "y_pred_proba = best_model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# Reporte detallado\n",
    "detailed_report = calc.generate_detailed_report(\n",
    "    y_val, y_pred, y_pred_proba,\n",
    "    class_names=['No Churn', 'Churn'],\n",
    "    model_name=best_model_name\n",
    ")\n",
    "\n",
    "print(f\"\\nðŸ† Mejor modelo seleccionado: {best_model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aabd26c",
   "metadata": {},
   "source": [
    "# **7. OptimizaciÃ³n de HiperparÃ¡metros**\n",
    "\n",
    "## Optimizaremos los hiperparÃ¡metros del mejor modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631ace6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OptimizaciÃ³n de hiperparÃ¡metros para el mejor modelo\n",
    "from TP5.models import hyperparameter_tuning\n",
    "\n",
    "print(f\"ðŸ”§ Optimizando hiperparÃ¡metros para {best_model_name}...\")\n",
    "\n",
    "# Definir grillas de parÃ¡metros segÃºn el modelo\n",
    "if 'Logistic' in best_model_name:\n",
    "    param_grid = {\n",
    "        'classifier__C': [0.1, 1.0, 10.0],\n",
    "        'classifier__solver': ['liblinear', 'lbfgs']\n",
    "    }\n",
    "elif 'KNN' in best_model_name:\n",
    "    param_grid = {\n",
    "        'classifier__n_neighbors': [3, 5, 7, 9],\n",
    "        'classifier__weights': ['uniform', 'distance']\n",
    "    }\n",
    "elif 'Random' in best_model_name:\n",
    "    param_grid = {\n",
    "        'classifier__n_estimators': [50, 100, 200],\n",
    "        'classifier__max_depth': [None, 10, 20],\n",
    "        'classifier__min_samples_split': [2, 5]\n",
    "    }\n",
    "else:\n",
    "    # Para Naive Bayes u otros\n",
    "    param_grid = {\n",
    "        'classifier__var_smoothing': [1e-9, 1e-8, 1e-7]\n",
    "    }\n",
    "\n",
    "# Realizar bÃºsqueda de hiperparÃ¡metros\n",
    "grid_search = hyperparameter_tuning(\n",
    "    best_model, param_grid, X_train, y_train,\n",
    "    cv=5, scoring='roc_auc'\n",
    ")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ HiperparÃ¡metros optimizados:\")\n",
    "print(f\"   - Mejor score CV: {grid_search.best_score_:.4f}\")\n",
    "print(f\"   - Mejores parÃ¡metros: {grid_search.best_params_}\")\n",
    "\n",
    "# Actualizar el mejor modelo con los parÃ¡metros optimizados\n",
    "optimized_model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluar modelo optimizado\n",
    "y_pred_opt = optimized_model.predict(X_val)\n",
    "y_pred_proba_opt = optimized_model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# Calcular mÃ©tricas del modelo optimizado\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "opt_auc = roc_auc_score(y_val, y_pred_proba_opt)\n",
    "opt_acc = accuracy_score(y_val, y_pred_opt)\n",
    "\n",
    "print(f\"\\nðŸ“ˆ Mejora con optimizaciÃ³n:\")\n",
    "print(f\"   - ROC AUC original: {results[best_model_name]['ROC_AUC']:.4f}\")\n",
    "print(f\"   - ROC AUC optimizado: {opt_auc:.4f}\")\n",
    "print(f\"   - Mejora: {opt_auc - results[best_model_name]['ROC_AUC']:.4f}\")\n",
    "\n",
    "# Guardar el modelo optimizado\n",
    "best_model = optimized_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16a0128",
   "metadata": {},
   "source": [
    "# **8. Predicciones Finales y Submission**\n",
    "\n",
    "## Generaremos las predicciones finales y crearemos el archivo de submission para Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50bc062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicciones finales y creaciÃ³n del archivo de submission\n",
    "from TP5.models import create_submission_file\n",
    "\n",
    "print(\"ðŸ“„ Generando predicciones finales...\")\n",
    "\n",
    "# Combinar datos de entrenamiento y validaciÃ³n para el entrenamiento final\n",
    "X_full_train = pd.concat([X_train, X_val], ignore_index=True)\n",
    "y_full_train = pd.concat([y_train, y_val], ignore_index=True)\n",
    "\n",
    "print(f\"ðŸ“Š Datos para entrenamiento final: {len(X_full_train):,} muestras\")\n",
    "\n",
    "# Preparar datos de prueba (usar test_data si es competencia real, o X_test si es simulaciÃ³n)\n",
    "if 'customerID' in test_data.columns:\n",
    "    # Competencia real\n",
    "    X_test_final = test_data.drop(['customerID'], axis=1)\n",
    "    test_ids = test_data['customerID']\n",
    "else:\n",
    "    # Datos simulados\n",
    "    X_test_final = X_test\n",
    "    test_ids = pd.Series([f'T{i:04d}' for i in range(len(X_test))])\n",
    "\n",
    "# Crear archivo de submission\n",
    "submission_df = create_submission_file(\n",
    "    model=best_model,\n",
    "    X_train=X_full_train,\n",
    "    y_train=y_full_train,\n",
    "    X_test=X_test_final,\n",
    "    test_ids=test_ids,\n",
    "    filename=\"submission_grupoM.csv\"\n",
    ")\n",
    "\n",
    "# Mostrar primeras predicciones\n",
    "print(f\"\\nðŸ“‹ Primeras 10 predicciones:\")\n",
    "display(submission_df.head(10))\n",
    "\n",
    "# EstadÃ­sticas de las predicciones\n",
    "predictions = submission_df.iloc[:, 1].values\n",
    "print(f\"\\nðŸ“Š EstadÃ­sticas de predicciones:\")\n",
    "print(f\"   - Predicciones de churn (>0.5): {np.sum(predictions > 0.5):,} ({np.mean(predictions > 0.5)*100:.1f}%)\")\n",
    "print(f\"   - Predicciones de no churn (â‰¤0.5): {np.sum(predictions <= 0.5):,} ({np.mean(predictions <= 0.5)*100:.1f}%)\")\n",
    "print(f\"   - Rango: [{predictions.min():.4f}, {predictions.max():.4f}]\")\n",
    "\n",
    "print(f\"\\nâœ… Archivo de submission 'submission_grupoM.csv' creado exitosamente\")\n",
    "print(f\"ðŸŽ¯ Listo para subir a Kaggle!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6955c3d",
   "metadata": {},
   "source": [
    "# **9. AnÃ¡lisis Final y Conclusiones**\n",
    "\n",
    "## Resumen completo del proyecto y resultados obtenidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb28916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AnÃ¡lisis final y conclusiones\n",
    "print(\"ðŸ“‹ RESUMEN FINAL DEL PROYECTO\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Resumen de datos\n",
    "print(f\"\\nðŸ“Š Resumen de Datos:\")\n",
    "print(f\"   - Muestras de entrenamiento: {len(train_data):,}\")\n",
    "print(f\"   - Muestras de prueba: {len(test_data):,}\")\n",
    "print(f\"   - CaracterÃ­sticas: {X_train.shape[1]}\")\n",
    "print(f\"   - Tasa de churn en entrenamiento: {y_train.mean()*100:.1f}%\")\n",
    "\n",
    "# Resumen de modelos\n",
    "print(f\"\\nðŸ¤– Modelos Evaluados:\")\n",
    "for model_name, metrics in results.items():\n",
    "    print(f\"   - {model_name}: ROC AUC = {metrics['ROC_AUC']:.4f}\")\n",
    "\n",
    "print(f\"\\nðŸ† Mejor Modelo: {best_model_name}\")\n",
    "print(f\"   - ROC AUC optimizado: {opt_auc:.4f}\")\n",
    "print(f\"   - Accuracy optimizado: {opt_acc:.4f}\")\n",
    "\n",
    "# Insights importantes\n",
    "print(f\"\\nðŸ’¡ Insights Clave:\")\n",
    "print(f\"   - El modelo {best_model_name} mostrÃ³ el mejor rendimiento\")\n",
    "print(f\"   - La optimizaciÃ³n de hiperparÃ¡metros mejorÃ³ el rendimiento\")\n",
    "print(f\"   - Las caracterÃ­sticas mÃ¡s importantes fueron identificadas en el EDA\")\n",
    "print(f\"   - El preprocesamiento automÃ¡tico manejÃ³ correctamente los datos\")\n",
    "\n",
    "# PrÃ³ximos pasos\n",
    "print(f\"\\nðŸš€ PrÃ³ximos Pasos:\")\n",
    "print(f\"   1. Subir 'submission_grupoM.csv' a Kaggle\")\n",
    "print(f\"   2. Analizar feature importance del modelo final\")\n",
    "print(f\"   3. Probar tÃ©cnicas de ensemble\")\n",
    "print(f\"   4. Implementar validaciÃ³n cruzada estratificada\")\n",
    "print(f\"   5. AnÃ¡lisis de errores para mejorar el modelo\")\n",
    "\n",
    "print(f\"\\nâœ… Proyecto completado exitosamente!\")\n",
    "print(f\"ðŸŽ“ UTN - AnÃ¡lisis de Datos Avanzado - Unidad 5\")\n",
    "print(f\"ðŸ‘¥ Grupo M - PredicciÃ³n de Fuga de Clientes\")\n",
    "\n",
    "# Generar reporte final de EDA\n",
    "print(f\"\\nðŸ“„ Generando reporte final de EDA...\")\n",
    "eda_report = eda.generate_eda_report(train_data)\n",
    "print(f\"âœ… Reporte EDA completo generado\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
