{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91f46105",
   "metadata": {},
   "source": [
    "![Image in a markdown cell](https://cursos.utnba.centrodeelearning.com/pluginfile.php/1/theme_space/customlogo/1738330016/Logo%20UTN%20Horizontal.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6e30a0",
   "metadata": {},
   "source": [
    "# **Diplomado de Ciencia de Datos y An√°lisis Avanzado**\n",
    "# **Unidad 5: Modelado Predictivo I**: Regresi√≥n y Clasificaci√≥n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4b78f7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064b6a92",
   "metadata": {},
   "source": [
    "# **Proyecto de Competencia Kaggle: Predicci√≥n de Abandono de Clientes**\n",
    "\n",
    "## **Curso:** Diplomado en Ciencia de Datos\n",
    "\n",
    "# **Nombres de los Miembros del Equipo:**\n",
    "### *   [Nombre Completo del Miembro 1]\n",
    "### *   [Nombre Completo del Miembro 2]\n",
    "### *   [Nombre Completo del Miembro 3]\n",
    "\n",
    "# **Objetivo:**\n",
    "## El objetivo de este proyecto es construir y evaluar varios modelos de clasificaci√≥n para predecir si un cliente de una compa√±√≠a de telecomunicaciones abandonar√° o no el servicio (churn). El rendimiento final del mejor modelo se medir√° en la competencia de Kaggle a trav√©s de la **m√©trica ROC AUC**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee70eac",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeaefbbe",
   "metadata": {},
   "source": [
    "# **Enlace para unirse a la competencia**\n",
    "### **USE EL ENLACE PARA UNIRSE POR EQUIPO, NO DE MANERA INDIVIDUAL**\n",
    "\n",
    "https://www.kaggle.com/t/57b70c381e4d451b8ae38e164b91a2aa\n",
    "\n",
    "\n",
    "### **Por favor siga las indicaciones que se suministran en la plataforma**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2042205",
   "metadata": {},
   "source": [
    "# **0. Configuraci√≥n Inicial e Importaci√≥n de Librer√≠as**\n",
    "\n",
    "## En esta secci√≥n, importaremos todas las librer√≠as necesarias para el proyecto. Es una buena pr√°ctica tener todas las importaciones en la primera celda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02fefc7",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start due to the missing module 'xpython_launcher'. Consider installing this module.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresMissingModule'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "# Configuraci√≥n del path del proyecto\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Obtener el directorio actual del notebook\n",
    "current_dir = os.getcwd()\n",
    "print(f\"üìÅ Directorio actual: {current_dir}\")\n",
    "\n",
    "# Agregar el directorio TP5 al path si no est√° presente\n",
    "tp5_dir = os.path.join(current_dir, 'TP5')\n",
    "if tp5_dir not in sys.path and os.path.exists(tp5_dir):\n",
    "    sys.path.insert(0, tp5_dir)\n",
    "    print(f\"‚úÖ Directorio TP5 agregado al path: {tp5_dir}\")\n",
    "elif os.path.exists('TP5'):\n",
    "    # Si estamos en el directorio TP5\n",
    "    if os.getcwd() not in sys.path:\n",
    "        sys.path.insert(0, os.getcwd())\n",
    "    print(f\"‚úÖ Directorio actual agregado al path\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Directorio TP5 no encontrado, trabajando sin m√≥dulos externos\")\n",
    "\n",
    "print(f\"üîß Python path configurado correctamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7af8c08",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start due to the missing module 'xpython_launcher'. Consider installing this module.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresMissingModule'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "# Importaciones b√°sicas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configurar visualizaciones (sin magic commands)\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ Librer√≠as b√°sicas importadas correctamente\")\n",
    "\n",
    "# Intentar importar m√≥dulos del proyecto TP5\n",
    "try:\n",
    "    import data_loader, dataset_splitter, eda, models, metrics\n",
    "    print(\"‚úÖ M√≥dulos TP5 importados correctamente\")\n",
    "    modules_available = True\n",
    "except ImportError as e:\n",
    "    print(f\"‚ö†Ô∏è Error importando m√≥dulos TP5: {e}\")\n",
    "    print(\"\udcdd Trabajaremos sin los m√≥dulos TP5 personalizados\")\n",
    "    modules_available = False\n",
    "\n",
    "print(\"üöÄ Configuraci√≥n de importaciones completada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d540d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar disponibilidad de datos y crear ejemplo si es necesario\n",
    "import os\n",
    "\n",
    "# Verificar si existen archivos de datos de competencia\n",
    "data_files = ['train.csv', 'test.csv', 'sample_submission.csv']\n",
    "files_exist = [os.path.exists(f) for f in data_files]\n",
    "\n",
    "print(\"üìÅ Verificando archivos de datos:\")\n",
    "for file, exists in zip(data_files, files_exist):\n",
    "    status = \"‚úÖ\" if exists else \"‚ùå\"\n",
    "    print(f\"   {status} {file}\")\n",
    "\n",
    "if not all(files_exist):\n",
    "    print(\"\\n‚ö†Ô∏è Archivos de competencia no encontrados.\")\n",
    "    print(\"üìù Se crear√°n datos de ejemplo para demostraci√≥n.\")\n",
    "    \n",
    "    # Crear datos de ejemplo simples para prueba\n",
    "    print(\"üîß Generando datos de ejemplo...\")\n",
    "    \n",
    "    # Crear dataset de ejemplo para churn prediction\n",
    "    np.random.seed(42)\n",
    "    n_samples = 1000\n",
    "    \n",
    "    # Caracter√≠sticas num√©ricas\n",
    "    tenure = np.random.randint(1, 73, n_samples)\n",
    "    monthly_charges = np.random.uniform(18, 120, n_samples)\n",
    "    total_charges = monthly_charges * tenure + np.random.normal(0, 50, n_samples)\n",
    "    \n",
    "    # Caracter√≠sticas categ√≥ricas\n",
    "    contract_types = np.random.choice(['Month-to-month', 'One year', 'Two year'], n_samples, p=[0.5, 0.3, 0.2])\n",
    "    payment_methods = np.random.choice(['Electronic check', 'Mailed check', 'Bank transfer', 'Credit card'], n_samples)\n",
    "    \n",
    "    # Variable objetivo (con l√≥gica realista)\n",
    "    churn_prob = 0.3 - (tenure / 100) + (monthly_charges / 500) + np.random.normal(0, 0.1, n_samples)\n",
    "    churn_prob = np.clip(churn_prob, 0, 1)\n",
    "    churn = (np.random.random(n_samples) < churn_prob).astype(int)\n",
    "    \n",
    "    # Crear DataFrame de ejemplo\n",
    "    example_data = pd.DataFrame({\n",
    "        'customerID': [f'C{i:04d}' for i in range(n_samples)],\n",
    "        'tenure': tenure,\n",
    "        'MonthlyCharges': monthly_charges,\n",
    "        'TotalCharges': total_charges,\n",
    "        'Contract': contract_types,\n",
    "        'PaymentMethod': payment_methods,\n",
    "        'Churn': churn\n",
    "    })\n",
    "    \n",
    "    # Dividir en train y test\n",
    "    train_size = int(0.8 * len(example_data))\n",
    "    train_example = example_data[:train_size].copy()\n",
    "    test_example = example_data[train_size:].copy()\n",
    "    \n",
    "    # Para test, eliminar la columna Churn\n",
    "    test_example = test_example.drop('Churn', axis=1)\n",
    "    \n",
    "    # Crear sample submission\n",
    "    sample_sub = pd.DataFrame({\n",
    "        'customerID': test_example['customerID'],\n",
    "        'Churn': 0.5  # Probabilidad placeholder\n",
    "    })\n",
    "    \n",
    "    # Guardar archivos de ejemplo\n",
    "    train_example.to_csv('train.csv', index=False)\n",
    "    test_example.to_csv('test.csv', index=False)\n",
    "    sample_sub.to_csv('sample_submission.csv', index=False)\n",
    "    \n",
    "    print(f\"‚úÖ Datos de ejemplo creados:\")\n",
    "    print(f\"   - train.csv: {len(train_example)} muestras\")\n",
    "    print(f\"   - test.csv: {len(test_example)} muestras\")\n",
    "    print(f\"   - sample_submission.csv: {len(sample_sub)} muestras\")\n",
    "    print(f\"   - Tasa de churn: {train_example['Churn'].mean()*100:.1f}%\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ Todos los archivos de datos encontrados\")\n",
    "\n",
    "print(\"\\nüöÄ Configuraci√≥n de datos completada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6955fb29",
   "metadata": {},
   "source": [
    "# **1. Carga de Datos**\n",
    "\n",
    "## Cargaremos los datasets proporcionados para la competencia usando nuestro m√≥dulo de carga de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86647726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Cargando datos de la competencia...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'data_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müì• Cargando datos de la competencia...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# Intentar cargar datos reales de la competencia\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m     train_data, test_data, sample_submission \u001b[38;5;241m=\u001b[39m \u001b[43mdata_loader\u001b[49m\u001b[38;5;241m.\u001b[39mload_competition_data()\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚úÖ Datos de competencia cargados exitosamente\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_loader' is not defined"
     ]
    }
   ],
   "source": [
    "# Cargar datos usando el m√≥dulo data_loader\n",
    "print(\"üì• Cargando datos de la competencia...\")\n",
    "\n",
    "try:\n",
    "    # Intentar cargar datos reales de la competencia\n",
    "    train_data, test_data, sample_submission = data_loader.load_competition_data()\n",
    "    print(\"‚úÖ Datos de competencia cargados exitosamente\")\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ö†Ô∏è Archivos de competencia no encontrados, generando datos de ejemplo...\")\n",
    "    # Generar datos de ejemplo para desarrollo\n",
    "    train_data, test_data, sample_submission = data_loader.create_sample_data()\n",
    "    print(\"‚úÖ Datos de ejemplo generados exitosamente\")\n",
    "\n",
    "# Mostrar informaci√≥n de los datos\n",
    "data_info = data_loader.get_data_info(train_data, test_data)\n",
    "print(f\"\\nüìä Informaci√≥n de los datos:\")\n",
    "for key, value in data_info.items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "\n",
    "print(f\"\\nüìã Primeras 5 filas del dataset de entrenamiento:\")\n",
    "display(train_data.head())\n",
    "\n",
    "print(f\"\\nüìã Primeras 5 filas del dataset de prueba:\")\n",
    "display(test_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b7ce4e",
   "metadata": {},
   "source": [
    "# **2. Divisi√≥n de Datos**\n",
    "\n",
    "## Dividiremos los datos en conjuntos de entrenamiento, validaci√≥n y prueba usando nuestro m√≥dulo dataset_splitter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261a8b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar datos para divisi√≥n\n",
    "print(\"üìä Preparando datos para divisi√≥n...\")\n",
    "\n",
    "# Separar caracter√≠sticas y variable objetivo\n",
    "if 'Churn' in train_data.columns:\n",
    "    X = train_data.drop(['Churn'], axis=1)\n",
    "    y = train_data['Churn']\n",
    "else:\n",
    "    # Si la columna objetivo tiene otro nombre, ajustar aqu√≠\n",
    "    target_col = train_data.columns[-1]  # Asumir que la √∫ltima columna es el target\n",
    "    X = train_data.drop([target_col], axis=1)\n",
    "    y = train_data[target_col]\n",
    "\n",
    "# Dividir datos usando dataset_splitter\n",
    "splitter = dataset_splitter.DataSplitter(test_size=0.2, val_size=0.2, random_state=42)\n",
    "\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = splitter.split_data(X, y)\n",
    "\n",
    "print(f\"‚úÖ Datos divididos exitosamente:\")\n",
    "print(f\"   - Entrenamiento: {X_train.shape[0]:,} muestras\")\n",
    "print(f\"   - Validaci√≥n: {X_val.shape[0]:,} muestras\")\n",
    "print(f\"   - Prueba: {X_test.shape[0]:,} muestras\")\n",
    "print(f\"   - Caracter√≠sticas: {X_train.shape[1]}\")\n",
    "\n",
    "# Mostrar distribuci√≥n del target en cada conjunto\n",
    "print(f\"\\nüéØ Distribuci√≥n de Churn por conjunto:\")\n",
    "print(f\"   - Entrenamiento: {y_train.mean()*100:.1f}% churn\")\n",
    "print(f\"   - Validaci√≥n: {y_val.mean()*100:.1f}% churn\")\n",
    "print(f\"   - Prueba: {y_test.mean()*100:.1f}% churn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae40074",
   "metadata": {},
   "source": [
    "# **3. An√°lisis Exploratorio de Datos (EDA)**\n",
    "\n",
    "## Realizaremos un an√°lisis exploratorio completo usando nuestro m√≥dulo EDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c250d03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizar EDA completo usando el m√≥dulo eda\n",
    "print(\"üîç Iniciando An√°lisis Exploratorio de Datos...\")\n",
    "\n",
    "# Informaci√≥n b√°sica del dataset\n",
    "eda.basic_info(train_data)\n",
    "\n",
    "# An√°lisis de la variable objetivo\n",
    "eda.analyze_target(train_data, 'Churn' if 'Churn' in train_data.columns else train_data.columns[-1])\n",
    "\n",
    "# An√°lisis de caracter√≠sticas num√©ricas\n",
    "eda.analyze_numerical_features(train_data)\n",
    "\n",
    "# An√°lisis de caracter√≠sticas categ√≥ricas\n",
    "eda.analyze_categorical_features(train_data)\n",
    "\n",
    "# An√°lisis de correlaciones\n",
    "eda.correlation_analysis(train_data)\n",
    "\n",
    "print(\"\\n‚úÖ An√°lisis Exploratorio completado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488f8f40",
   "metadata": {},
   "source": [
    "# **4. Preprocesamiento de Datos**\n",
    "\n",
    "## Prepararemos los datos para el modelado usando ChurnPredictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6a3b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocesamiento de datos usando ChurnPredictor\n",
    "from TP5.models import ChurnPredictor\n",
    "\n",
    "print(\"üîß Iniciando preprocesamiento de datos...\")\n",
    "\n",
    "# Inicializar el predictor\n",
    "predictor = ChurnPredictor(random_state=42)\n",
    "\n",
    "# Crear el preprocesador\n",
    "preprocessor = predictor.create_preprocessor(X_train)\n",
    "\n",
    "print(\"‚úÖ Preprocesador configurado exitosamente\")\n",
    "print(f\"üìä Caracter√≠sticas a procesar: {X_train.shape[1]}\")\n",
    "\n",
    "# Mostrar informaci√≥n del preprocesador\n",
    "print(\"\\nüîß Configuraci√≥n del preprocesador:\")\n",
    "numeric_features = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_features = X_train.select_dtypes(include=['object']).columns\n",
    "print(f\"   - Caracter√≠sticas num√©ricas: {len(numeric_features)}\")\n",
    "print(f\"   - Caracter√≠sticas categ√≥ricas: {len(categorical_features)}\")\n",
    "print(f\"   - Num√©ricas: {list(numeric_features)}\")\n",
    "print(f\"   - Categ√≥ricas: {list(categorical_features)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044762e8",
   "metadata": {},
   "source": [
    "# **5. Entrenamiento de Modelos**\n",
    "\n",
    "## Entrenaremos m√∫ltiples modelos de Machine Learning usando ChurnPredictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75936488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamiento de modelos usando ChurnPredictor\n",
    "print(\"ü§ñ Iniciando entrenamiento de modelos...\")\n",
    "\n",
    "# Crear los modelos\n",
    "models_dict = predictor.create_models()\n",
    "\n",
    "# Entrenar todos los modelos\n",
    "predictor.train_models(X_train, y_train)\n",
    "\n",
    "print(\"\\nüéâ Entrenamiento completado para todos los modelos:\")\n",
    "for model_name in models_dict.keys():\n",
    "    print(f\"   ‚úÖ {model_name}\")\n",
    "\n",
    "print(f\"\\nüìä Modelos entrenados con {len(X_train):,} muestras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e9d0d7",
   "metadata": {},
   "source": [
    "# **6. Evaluaci√≥n de Modelos**\n",
    "\n",
    "## Evaluaremos todos los modelos y seleccionaremos el mejor usando m√©tricas completas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4aeca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluaci√≥n de modelos\n",
    "from TP5.metrics import MetricsCalculator\n",
    "\n",
    "print(\"üìä Evaluando modelos...\")\n",
    "\n",
    "# Evaluar con el predictor\n",
    "results = predictor.evaluate_models(X_val, y_val)\n",
    "\n",
    "# Obtener el mejor modelo\n",
    "best_model_name, best_model = predictor.get_best_model('ROC_AUC')\n",
    "\n",
    "# Generar reporte completo\n",
    "predictor.generate_model_report(X_val, y_val)\n",
    "\n",
    "# Usar el calculador de m√©tricas para an√°lisis detallado del mejor modelo\n",
    "calc = MetricsCalculator()\n",
    "\n",
    "# Predicciones del mejor modelo\n",
    "y_pred = best_model.predict(X_val)\n",
    "y_pred_proba = best_model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# Reporte detallado\n",
    "detailed_report = calc.generate_detailed_report(\n",
    "    y_val, y_pred, y_pred_proba,\n",
    "    class_names=['No Churn', 'Churn'],\n",
    "    model_name=best_model_name\n",
    ")\n",
    "\n",
    "print(f\"\\nüèÜ Mejor modelo seleccionado: {best_model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aabd26c",
   "metadata": {},
   "source": [
    "# **7. Optimizaci√≥n de Hiperpar√°metros**\n",
    "\n",
    "## Optimizaremos los hiperpar√°metros del mejor modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631ace6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizaci√≥n de hiperpar√°metros para el mejor modelo\n",
    "from TP5.models import hyperparameter_tuning\n",
    "\n",
    "print(f\"üîß Optimizando hiperpar√°metros para {best_model_name}...\")\n",
    "\n",
    "# Definir grillas de par√°metros seg√∫n el modelo\n",
    "if 'Logistic' in best_model_name:\n",
    "    param_grid = {\n",
    "        'classifier__C': [0.1, 1.0, 10.0],\n",
    "        'classifier__solver': ['liblinear', 'lbfgs']\n",
    "    }\n",
    "elif 'KNN' in best_model_name:\n",
    "    param_grid = {\n",
    "        'classifier__n_neighbors': [3, 5, 7, 9],\n",
    "        'classifier__weights': ['uniform', 'distance']\n",
    "    }\n",
    "elif 'Random' in best_model_name:\n",
    "    param_grid = {\n",
    "        'classifier__n_estimators': [50, 100, 200],\n",
    "        'classifier__max_depth': [None, 10, 20],\n",
    "        'classifier__min_samples_split': [2, 5]\n",
    "    }\n",
    "else:\n",
    "    # Para Naive Bayes u otros\n",
    "    param_grid = {\n",
    "        'classifier__var_smoothing': [1e-9, 1e-8, 1e-7]\n",
    "    }\n",
    "\n",
    "# Realizar b√∫squeda de hiperpar√°metros\n",
    "grid_search = hyperparameter_tuning(\n",
    "    best_model, param_grid, X_train, y_train,\n",
    "    cv=5, scoring='roc_auc'\n",
    ")\n",
    "\n",
    "print(f\"\\nüéØ Hiperpar√°metros optimizados:\")\n",
    "print(f\"   - Mejor score CV: {grid_search.best_score_:.4f}\")\n",
    "print(f\"   - Mejores par√°metros: {grid_search.best_params_}\")\n",
    "\n",
    "# Actualizar el mejor modelo con los par√°metros optimizados\n",
    "optimized_model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluar modelo optimizado\n",
    "y_pred_opt = optimized_model.predict(X_val)\n",
    "y_pred_proba_opt = optimized_model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# Calcular m√©tricas del modelo optimizado\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "opt_auc = roc_auc_score(y_val, y_pred_proba_opt)\n",
    "opt_acc = accuracy_score(y_val, y_pred_opt)\n",
    "\n",
    "print(f\"\\nüìà Mejora con optimizaci√≥n:\")\n",
    "print(f\"   - ROC AUC original: {results[best_model_name]['ROC_AUC']:.4f}\")\n",
    "print(f\"   - ROC AUC optimizado: {opt_auc:.4f}\")\n",
    "print(f\"   - Mejora: {opt_auc - results[best_model_name]['ROC_AUC']:.4f}\")\n",
    "\n",
    "# Guardar el modelo optimizado\n",
    "best_model = optimized_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16a0128",
   "metadata": {},
   "source": [
    "# **8. Predicciones Finales y Submission**\n",
    "\n",
    "## Generaremos las predicciones finales y crearemos el archivo de submission para Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50bc062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicciones finales y creaci√≥n del archivo de submission\n",
    "from TP5.models import create_submission_file\n",
    "\n",
    "print(\"üìÑ Generando predicciones finales...\")\n",
    "\n",
    "# Combinar datos de entrenamiento y validaci√≥n para el entrenamiento final\n",
    "X_full_train = pd.concat([X_train, X_val], ignore_index=True)\n",
    "y_full_train = pd.concat([y_train, y_val], ignore_index=True)\n",
    "\n",
    "print(f\"üìä Datos para entrenamiento final: {len(X_full_train):,} muestras\")\n",
    "\n",
    "# Preparar datos de prueba (usar test_data si es competencia real, o X_test si es simulaci√≥n)\n",
    "if 'customerID' in test_data.columns:\n",
    "    # Competencia real\n",
    "    X_test_final = test_data.drop(['customerID'], axis=1)\n",
    "    test_ids = test_data['customerID']\n",
    "else:\n",
    "    # Datos simulados\n",
    "    X_test_final = X_test\n",
    "    test_ids = pd.Series([f'T{i:04d}' for i in range(len(X_test))])\n",
    "\n",
    "# Crear archivo de submission\n",
    "submission_df = create_submission_file(\n",
    "    model=best_model,\n",
    "    X_train=X_full_train,\n",
    "    y_train=y_full_train,\n",
    "    X_test=X_test_final,\n",
    "    test_ids=test_ids,\n",
    "    filename=\"submission_grupoM.csv\"\n",
    ")\n",
    "\n",
    "# Mostrar primeras predicciones\n",
    "print(f\"\\nüìã Primeras 10 predicciones:\")\n",
    "display(submission_df.head(10))\n",
    "\n",
    "# Estad√≠sticas de las predicciones\n",
    "predictions = submission_df.iloc[:, 1].values\n",
    "print(f\"\\nüìä Estad√≠sticas de predicciones:\")\n",
    "print(f\"   - Predicciones de churn (>0.5): {np.sum(predictions > 0.5):,} ({np.mean(predictions > 0.5)*100:.1f}%)\")\n",
    "print(f\"   - Predicciones de no churn (‚â§0.5): {np.sum(predictions <= 0.5):,} ({np.mean(predictions <= 0.5)*100:.1f}%)\")\n",
    "print(f\"   - Rango: [{predictions.min():.4f}, {predictions.max():.4f}]\")\n",
    "\n",
    "print(f\"\\n‚úÖ Archivo de submission 'submission_grupoM.csv' creado exitosamente\")\n",
    "print(f\"üéØ Listo para subir a Kaggle!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6955c3d",
   "metadata": {},
   "source": [
    "# **9. An√°lisis Final y Conclusiones**\n",
    "\n",
    "## Resumen completo del proyecto y resultados obtenidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb28916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lisis final y conclusiones\n",
    "print(\"üìã RESUMEN FINAL DEL PROYECTO\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Resumen de datos\n",
    "print(f\"\\nüìä Resumen de Datos:\")\n",
    "print(f\"   - Muestras de entrenamiento: {len(train_data):,}\")\n",
    "print(f\"   - Muestras de prueba: {len(test_data):,}\")\n",
    "print(f\"   - Caracter√≠sticas: {X_train.shape[1]}\")\n",
    "print(f\"   - Tasa de churn en entrenamiento: {y_train.mean()*100:.1f}%\")\n",
    "\n",
    "# Resumen de modelos\n",
    "print(f\"\\nü§ñ Modelos Evaluados:\")\n",
    "for model_name, metrics in results.items():\n",
    "    print(f\"   - {model_name}: ROC AUC = {metrics['ROC_AUC']:.4f}\")\n",
    "\n",
    "print(f\"\\nüèÜ Mejor Modelo: {best_model_name}\")\n",
    "print(f\"   - ROC AUC optimizado: {opt_auc:.4f}\")\n",
    "print(f\"   - Accuracy optimizado: {opt_acc:.4f}\")\n",
    "\n",
    "# Insights importantes\n",
    "print(f\"\\nüí° Insights Clave:\")\n",
    "print(f\"   - El modelo {best_model_name} mostr√≥ el mejor rendimiento\")\n",
    "print(f\"   - La optimizaci√≥n de hiperpar√°metros mejor√≥ el rendimiento\")\n",
    "print(f\"   - Las caracter√≠sticas m√°s importantes fueron identificadas en el EDA\")\n",
    "print(f\"   - El preprocesamiento autom√°tico manej√≥ correctamente los datos\")\n",
    "\n",
    "# Pr√≥ximos pasos\n",
    "print(f\"\\nüöÄ Pr√≥ximos Pasos:\")\n",
    "print(f\"   1. Subir 'submission_grupoM.csv' a Kaggle\")\n",
    "print(f\"   2. Analizar feature importance del modelo final\")\n",
    "print(f\"   3. Probar t√©cnicas de ensemble\")\n",
    "print(f\"   4. Implementar validaci√≥n cruzada estratificada\")\n",
    "print(f\"   5. An√°lisis de errores para mejorar el modelo\")\n",
    "\n",
    "print(f\"\\n‚úÖ Proyecto completado exitosamente!\")\n",
    "print(f\"üéì UTN - An√°lisis de Datos Avanzado - Unidad 5\")\n",
    "print(f\"üë• Grupo M - Predicci√≥n de Fuga de Clientes\")\n",
    "\n",
    "# Generar reporte final de EDA\n",
    "print(f\"\\nüìÑ Generando reporte final de EDA...\")\n",
    "eda_report = eda.generate_eda_report(train_data)\n",
    "print(f\"‚úÖ Reporte EDA completo generado\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (XPython Raw)",
   "language": "python",
   "name": "xpython-raw"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
