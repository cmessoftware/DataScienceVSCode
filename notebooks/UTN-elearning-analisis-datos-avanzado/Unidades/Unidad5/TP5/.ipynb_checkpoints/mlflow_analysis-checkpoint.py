#!/usr/bin/env python3
"""
MLflow Experiment Analysis Dashboard
====================================

Script para analizar y comparar experimentos de MLflow de manera interactiva.
Uso: python mlflow_analysis.py
"""

import mlflow
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from pathlib import Path
import argparse

def setup_mlflow():
    """Configurar MLflow tracking"""
    mlflow_tracking_dir = Path.cwd() / "mlruns"
    mlflow.set_tracking_uri(f"file://{mlflow_tracking_dir}")
    return mlflow_tracking_dir

def get_experiment_data(experiment_name="Churn_Prediction_TP5"):
    """Obtener datos del experimento"""
    try:
        experiment = mlflow.get_experiment_by_name(experiment_name)
        if experiment is None:
            print(f"‚ùå Experimento '{experiment_name}' no encontrado")
            return None
        
        runs = mlflow.search_runs(experiment_ids=[experiment.experiment_id])
        return experiment, runs
    except Exception as e:
        print(f"‚ùå Error obteniendo datos del experimento: {e}")
        return None

def create_comparison_dashboard(runs_df):
    """Crear dashboard de comparaci√≥n visual"""
    if runs_df.empty:
        print("‚ö†Ô∏è No hay runs para mostrar")
        return
    
    # Preparar datos
    metrics_cols = [col for col in runs_df.columns if col.startswith('metrics.')]
    if not metrics_cols:
        print("‚ö†Ô∏è No se encontraron m√©tricas en los runs")
        return
    
    # Extraer informaci√≥n relevante
    model_col = 'tags.model_type' if 'tags.model_type' in runs_df.columns else None
    if model_col is None:
        print("‚ö†Ô∏è No se encontr√≥ informaci√≥n de tipos de modelo")
        return
    
    # Crear figura
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    fig.suptitle('üìä Dashboard de Comparaci√≥n de Modelos ML', fontsize=16, fontweight='bold')
    
    # M√©tricas principales
    main_metrics = ['metrics.roc_auc', 'metrics.accuracy', 'metrics.f1_score', 
                   'metrics.precision', 'metrics.recall']
    
    available_metrics = [m for m in main_metrics if m in runs_df.columns]
    
    for i, metric in enumerate(available_metrics[:6]):  # M√°ximo 6 gr√°ficos
        row, col = divmod(i, 3)
        ax = axes[row, col]
        
        # Preparar datos para el gr√°fico
        data = runs_df[[model_col, metric]].dropna()
        if data.empty:
            ax.text(0.5, 0.5, 'No data', ha='center', va='center', transform=ax.transAxes)
            continue
        
        # Crear gr√°fico de barras
        metric_name = metric.replace('metrics.', '').upper()
        models = data[model_col].values
        values = data[metric].values
        
        # Truncar nombres largos
        models_short = [m[:12] + '...' if len(m) > 15 else m for m in models]
        
        bars = ax.bar(models_short, values, alpha=0.7)
        ax.set_title(f'{metric_name}', fontweight='bold')
        ax.set_ylabel(metric_name)
        ax.tick_params(axis='x', rotation=45)
        ax.grid(axis='y', alpha=0.3)
        
        # Highlight del mejor modelo
        if len(values) > 0:
            best_idx = np.argmax(values)
            bars[best_idx].set_color('gold')
            bars[best_idx].set_edgecolor('orange')
            bars[best_idx].set_linewidth(2)
    
    # Ocultar subplots vac√≠os
    for i in range(len(available_metrics), 6):
        row, col = divmod(i, 3)
        axes[row, col].set_visible(False)
    
    plt.tight_layout()
    plt.show()

def print_summary_table(runs_df):
    """Imprimir tabla resumen de experimentos"""
    if runs_df.empty:
        print("‚ö†Ô∏è No hay runs para mostrar")
        return
    
    # Seleccionar columnas relevantes
    summary_cols = []
    
    # Modelo
    if 'tags.model_type' in runs_df.columns:
        summary_cols.append('tags.model_type')
    
    # M√©tricas principales
    metric_mapping = {
        'metrics.roc_auc': 'ROC_AUC',
        'metrics.accuracy': 'Accuracy',
        'metrics.precision': 'Precision',
        'metrics.recall': 'Recall',
        'metrics.f1_score': 'F1_Score'
    }
    
    for mlflow_col, display_col in metric_mapping.items():
        if mlflow_col in runs_df.columns:
            summary_cols.append(mlflow_col)
    
    if not summary_cols:
        print("‚ö†Ô∏è No se encontraron columnas relevantes para el resumen")
        return
    
    # Crear tabla resumen
    summary_df = runs_df[summary_cols].copy()
    
    # Renombrar columnas
    rename_dict = {'tags.model_type': 'Model'}
    rename_dict.update({k: v for k, v in metric_mapping.items() if k in summary_df.columns})
    summary_df = summary_df.rename(columns=rename_dict)
    
    # Redondear m√©tricas
    metric_cols = [col for col in summary_df.columns if col != 'Model']
    for col in metric_cols:
        if col in summary_df.columns:
            summary_df[col] = summary_df[col].round(4)
    
    # Ordenar por ROC AUC si est√° disponible
    if 'ROC_AUC' in summary_df.columns:
        summary_df = summary_df.sort_values('ROC_AUC', ascending=False)
    
    print("\nüìã Tabla Resumen de Experimentos:")
    print("=" * 80)
    print(summary_df.to_string(index=False))
    print("=" * 80)

def find_best_models(runs_df, top_n=3):
    """Encontrar los mejores modelos"""
    if runs_df.empty or 'metrics.roc_auc' not in runs_df.columns:
        print("‚ö†Ô∏è No se puede determinar los mejores modelos")
        return
    
    best_models = runs_df.nlargest(top_n, 'metrics.roc_auc')
    
    print(f"\nüèÜ Top {top_n} Mejores Modelos:")
    print("=" * 60)
    
    for i, (idx, row) in enumerate(best_models.iterrows(), 1):
        model_name = row.get('tags.model_type', 'Unknown')
        roc_auc = row.get('metrics.roc_auc', 0)
        accuracy = row.get('metrics.accuracy', 0)
        f1_score = row.get('metrics.f1_score', 0)
        run_id = row.get('run_id', 'Unknown')
        
        print(f"\n{i}. ü•á {model_name}")
        print(f"   üìä ROC AUC: {roc_auc:.4f}")
        print(f"   üìä Accuracy: {accuracy:.4f}")
        print(f"   üìä F1-Score: {f1_score:.4f}")
        print(f"   üîó Run ID: {run_id[:8]}...")

def analyze_optimization_impact(runs_df):
    """Analizar el impacto de la optimizaci√≥n"""
    # Buscar modelos originales y optimizados
    original_runs = runs_df[~runs_df['tags.model_type'].str.contains('OPTIMIZED', na=False)]
    optimized_runs = runs_df[runs_df['tags.model_type'].str.contains('OPTIMIZED', na=False)]
    
    if original_runs.empty or optimized_runs.empty:
        print("‚ö†Ô∏è No se encontraron pares de modelos originales/optimizados para comparar")
        return
    
    print("\nüî¨ An√°lisis de Impacto de Optimizaci√≥n:")
    print("=" * 50)
    
    # Comparar m√©tricas
    metrics_to_compare = ['metrics.roc_auc', 'metrics.accuracy', 'metrics.f1_score']
    
    for metric in metrics_to_compare:
        if metric in runs_df.columns:
            original_avg = original_runs[metric].mean()
            optimized_avg = optimized_runs[metric].mean()
            improvement = optimized_avg - original_avg
            
            metric_name = metric.replace('metrics.', '').upper()
            print(f"\nüìà {metric_name}:")
            print(f"   - Original promedio: {original_avg:.4f}")
            print(f"   - Optimizado promedio: {optimized_avg:.4f}")
            print(f"   - Mejora: {improvement:+.4f} ({improvement/original_avg*100:+.2f}%)")

def main():
    """Funci√≥n principal"""
    parser = argparse.ArgumentParser(description='Analizar experimentos MLflow')
    parser.add_argument('--experiment', '-e', default='Churn_Prediction_TP5',
                       help='Nombre del experimento a analizar')
    parser.add_argument('--no-plots', action='store_true',
                       help='No mostrar gr√°ficos')
    args = parser.parse_args()
    
    print("üîß MLflow Experiment Analysis Dashboard")
    print("=" * 50)
    
    # Configurar MLflow
    tracking_dir = setup_mlflow()
    print(f"üìÅ MLflow Tracking URI: {mlflow.get_tracking_uri()}")
    
    # Obtener datos del experimento
    result = get_experiment_data(args.experiment)
    if result is None:
        return
    
    experiment, runs_df = result
    print(f"üß™ Experimento: {experiment.name}")
    print(f"üìä Total de runs: {len(runs_df)}")
    
    if runs_df.empty:
        print("‚ö†Ô∏è No hay runs en este experimento")
        return
    
    # An√°lisis detallado
    print_summary_table(runs_df)
    find_best_models(runs_df)
    analyze_optimization_impact(runs_df)
    
    # Gr√°ficos (si no est√° deshabilitado)
    if not args.no_plots:
        create_comparison_dashboard(runs_df)
    
    # Instrucciones finales
    print(f"\nüåê Para acceder al MLflow UI completo:")
    print(f"   1. Ejecuta: mlflow ui")
    print(f"   2. Abre: http://localhost:5000")
    print(f"   3. Busca el experimento: '{experiment.name}'")
    
    print("\n‚úÖ An√°lisis completado")

if __name__ == "__main__":
    main()
