{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image in a markdown cell](https://cursos.utnba.centrodeelearning.com/pluginfile.php/1/theme_space/customlogo/1738330016/Logo%20UTN%20Horizontal.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Diplomado de Ciencia de Datos y AnÃ¡lisis Avanzado**\n",
    "# **Unidad 5: Modelado Predictivo I**: RegresiÃ³n y ClasificaciÃ³n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Proyecto de Competencia Kaggle: PredicciÃ³n de Abandono de Clientes**\n",
    "\n",
    "## **Curso:** Diplomado en Ciencia de Datos\n",
    "\n",
    "# ***Nombres de los Miembros del Equipo: Grupo M***\n",
    "### *   Lucia Cortes\n",
    "### *   Maria Fernanda Farias\n",
    "### *   Alejandro Gomez Grosschadl\n",
    "### *   Favio Ruggieri\n",
    "### *   Sergio Salanitri\n",
    "### *   Karina Calvo\n",
    "\n",
    "# **Objetivo:**\n",
    "## El objetivo de este proyecto es construir y evaluar varios modelos de clasificaciÃ³n para predecir si un cliente de una compaÃ±Ã­a de telecomunicaciones abandonarÃ¡ o no el servicio (churn). El rendimiento final del mejor modelo se medirÃ¡ en la competencia de Kaggle a travÃ©s de la **mÃ©trica ROC AUC**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Enlace para unirse a la competencia**\n",
    "### **USE EL ENLACE PARA UNIRSE POR EQUIPO, NO DE MANERA INDIVIDUAL**\n",
    "\n",
    "https://www.kaggle.com/t/57b70c381e4d451b8ae38e164b91a2aa\n",
    "\n",
    "\n",
    "### **Por favor siga las indicaciones que se suministran en la plataforma**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. **ConfiguraciÃ³n Inicial e ImportaciÃ³n de LibrerÃ­as**\n",
    "\n",
    "## En esta secciÃ³n, importaremos todas las librerÃ­as necesarias para el proyecto. Es una buena prÃ¡ctica tener todas las importaciones en la primera celda.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importaciones bÃ¡sicas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV,train_test_split\n",
    "from sklearn.metrics import (roc_auc_score, accuracy_score, precision_score, \n",
    "                           recall_score, f1_score, classification_report, \n",
    "                           confusion_matrix, roc_curve)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Tuple, Any\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# Configurar visualizaciones\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "# Importar mÃ³dulos del proyecto (si existen)\n",
    "try:\n",
    "    import data_loader\n",
    "    import dataset_splitter\n",
    "    import eda\n",
    "    import models\n",
    "    import metrics\n",
    "    print(\"âœ… MÃ³dulos del proyecto importados correctamente\")\n",
    "except ImportError as e:\n",
    "    print(f\"âš ï¸ Algunos mÃ³dulos del proyecto no estÃ¡n disponibles: {e}\")\n",
    "    print(\"ðŸ’¡ Puedes trabajar directamente en el notebook por ahora\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  **1. Carga de Datos**\n",
    "\n",
    "## Cargaremos los datasets proporcionados para la competencia: `train.csv`, `test.csv` y `sample_submission.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    X_train = pd.read_csv('train.csv')\n",
    "    X_test = pd.read_csv('test.csv')\n",
    "    sample_submission_df = pd.read_csv('sample_submission.csv')\n",
    "except FileNotFoundError:\n",
    "    print(\"AsegÃºrate de que los archivos .csv de la competencia estÃ©n en el mismo directorio que este cuaderno.\")\n",
    "    # Si usas Colab, puedes subir los archivos al entorno de ejecuciÃ³n.\n",
    "    exit()\n",
    "\n",
    "print(\"Forma del dataset de entrenamiento:\", X_train.shape)\n",
    "print(\"Forma del dataset de prueba:\", X_test.shape)\n",
    "\n",
    "print(\"\\nPrimeras 5 filas del dataset de entrenamiento:\")\n",
    "display(X_train.head())\n",
    "\n",
    "print(\"\\nPrimeras 5 filas del dataset de prueba:\")\n",
    "display(X_test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2**. AnÃ¡lisis Exploratorio de Datos (EDA)**\n",
    "\n",
    "### En esta fase, exploraremos el dataset de entrenamiento para entender mejor nuestros datos, encontrar patrones, identificar valores faltantes y visualizar relaciones entre las caracterÃ­sticas y la variable objetivo (`Churn`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objetivo: conocer distribuciÃ³n de datos, target, tipos de columnas.\n",
    "\n",
    "Variables como Contract, InternetService, PaymentMethod requieren OneHotEncoding o LabelEncoding.\n",
    "\n",
    "Target Churn: dataset mÃ¡s desbalanceado (~20% churn). #Verificar el desbalanceo.\n",
    "\n",
    "## DescripciÃ³n de parÃ¡metros\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# InformaciÃ³n general del dataset\n",
    "print(\"ðŸ“Š INFORMACIÃ“N GENERAL DEL DATASET\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Dimensiones: {X_train.shape}\")\n",
    "print(f\"Columnas: {list(X_train.columns)}\")\n",
    "print(\"\\nðŸ“‹ Tipos de datos:\")\n",
    "print(X_train.dtypes)\n",
    "\n",
    "# InformaciÃ³n sobre valores faltantes\n",
    "print(\"\\nðŸ” VALORES FALTANTES:\")\n",
    "print(\"=\" * 30)\n",
    "missing_values = X_train.isnull().sum()\n",
    "print(missing_values[missing_values > 0])\n",
    "if missing_values.sum() == 0:\n",
    "    print(\"âœ… No hay valores faltantes\")\n",
    "\n",
    "# DistribuciÃ³n de la variable objetivo\n",
    "print(\"\\nðŸŽ¯ DISTRIBUCIÃ“N DE LA VARIABLE OBJETIVO (Churn):\")\n",
    "print(\"=\" * 50)\n",
    "y_train = X_train['Churn']\n",
    "churn_counts = y_train.value_counts()\n",
    "#DefiniciÃ³n de variable objetivo\n",
    "churn_pct = y_train.value_counts(normalize=True) * 100\n",
    "print(f\"No Churn (0): {churn_counts[0]} ({churn_pct[0]:.1f}%)\")\n",
    "print(f\"Churn (1): {churn_counts[1]} ({churn_pct[1]:.1f}%)\")\n",
    "\n",
    "# VisualizaciÃ³n de la distribuciÃ³n del target\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "y_train.value_counts().plot(kind='bar', color=['lightblue', 'salmon'])\n",
    "plt.title('DistribuciÃ³n de Churn')\n",
    "plt.xlabel('Churn')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "y_train.value_counts(normalize=True).plot(kind='pie', autopct='%1.1f%%', colors=['lightblue', 'salmon'])\n",
    "plt.title('ProporciÃ³n de Churn')\n",
    "plt.ylabel('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#Mostrar frecuenbcia de variables categÃ³ricas\n",
    "categorical_columns = ['gender', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod']\n",
    "\n",
    "# Mostrar frecuencias de variables categÃ³ricas en una grilla 4x4\n",
    "fig, axes = plt.subplots(4, 4, figsize=(20, 16))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(categorical_columns):\n",
    "    X_train[col].value_counts().plot(kind='bar', color='deeppink', ax=axes[i])\n",
    "    axes[i].set_title(f'Frecuencia de {col}')\n",
    "    axes[i].set_xlabel(col)\n",
    "    axes[i].set_ylabel('Conteo')\n",
    "    axes[i].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Eliminar ejes vacÃ­os si hay menos de 16 columnas\n",
    "for j in range(len(categorical_columns), 16):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ”§ CONFIGURACIÃ“N Y ENTRENAMIENTO DEL CHURNPREDICTOR\n",
    "\n",
    "from models import ChurnPredictor\n",
    "\n",
    "print(\"ðŸš€ Inicializando ChurnPredictor...\")\n",
    "\n",
    "# Usar los datos ORIGINALES (antes del preprocesamiento) para crear el preprocesador\n",
    "predictor = ChurnPredictor(random_state=42)\n",
    "\n",
    "# Verificar y limpiar datos antes del preprocesamiento\n",
    "print(\"ðŸ” Verificando datos antes del preprocesamiento...\")\n",
    "\n",
    "# Asumir que X_train son los datos originales (con customerID y sin procesar)\n",
    "# Necesitamos remover customerID de X_train si existe\n",
    "if 'customerID' in X_train.columns:\n",
    "    X_train_clean = X_train.drop(['customerID'], axis=1)\n",
    "else:\n",
    "    X_train_clean = X_train.copy()\n",
    "\n",
    "#customerIDs lo guardo para usarn en la generaciÃ³n del archivo de submit.\n",
    "customer_ids = X_test['customerID']\n",
    "\n",
    "# Necesitamos remover customerID de X_test si existe\n",
    "if 'customerID' in X_test.columns:\n",
    "    X_test_clean = X_test.drop(['customerID'], axis=1)\n",
    "else:\n",
    "    X_test_clean = X_test.copy()\n",
    "\n",
    "\n",
    "# SINCRONIZACIÃ“N FINAL: Asegurar que X e y tengan el mismo nÃºmero de muestras\n",
    "if X_train_clean.shape[0] != y_train.shape[0]:\n",
    "    print(\"âš ï¸ Sincronizando datos finales:\")\n",
    "    print(f\"   - X_train_clean: {X_train_clean.shape[0]} â†’ \", end=\"\")\n",
    "    print(f\"   - y_train: {y_train.shape[0]} â†’ \", end=\"\")\n",
    "    \n",
    "    min_samples = min(X_train_clean.shape[0], y_train.shape[0])\n",
    "    X_train_clean = X_train_clean.iloc[:min_samples]\n",
    "    y_train_sync = y_train.iloc[:min_samples] if hasattr(y_train, 'iloc') else y_train[:min_samples]\n",
    "    \n",
    "    print(f\"Sincronizados a {min_samples} muestras\")\n",
    "else:\n",
    "    y_train_sync = y_train\n",
    "    print(\"âœ… Datos ya estÃ¡n sincronizados\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3. Preprocesamiento de Datos**\n",
    "\n",
    "## Prepararemos los datos para que puedan ser utilizados por los modelos de Machine Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANTE: Mapear y_train_sync para consistencia de tipos\n",
    "print(\"\\nðŸ”§ Mapeando y_train_sync a formato numÃ©rico...\")\n",
    "y_train_sync = predictor.map_target(y_train_sync)\n",
    "\n",
    "# Crear el preprocesador con los datos originales\n",
    "preprocessor = predictor.create_preprocessor(X_train_clean)\n",
    "\n",
    "print(\"âœ… Preprocesador configurado exitosamente\")\n",
    "print(f\"ðŸ“Š CaracterÃ­sticas procesadas: {X_train_clean.shape[1]}\")\n",
    "print(f\"ðŸ“Š Muestras para entrenamiento: {X_train_clean.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 3.1 - Preprocesamiento de datos usando los modelos creados\n",
    "# from models import ChurnPredictor\n",
    "\n",
    "# # Inicializar el predictor\n",
    "# predictor = ChurnPredictor(random_state=42)\n",
    "\n",
    "# # Crear el preprocesador\n",
    "# predictor.create_preprocessor(X_train)\n",
    "\n",
    "# print(\"âœ… Preprocesador configurado exitosamente\")\n",
    "# X_features = X_train.shape[1]\n",
    "# print(f\"ðŸ“Š CaracterÃ­sticas a procesar: {X_features}\")\n",
    "\n",
    "# #Mostrar estado de columnas luego del preprocesamiento.\n",
    "# predictor.inspect_transformed_columns(\n",
    "#     X_original=X_train,\n",
    "#     columns=['Partner', 'Dependents', 'Contract', 'PaymentMethod']\n",
    "# )\n",
    "\n",
    "# # Mostrar informaciÃ³n del preprocesador\n",
    "# print(\"\\nðŸ”§ ConfiguraciÃ³n del preprocesador:\")\n",
    "# numeric_features = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
    "# categorical_features = X_train.select_dtypes(include=['object']).columns                      \n",
    "                                         \n",
    "# print(f\"   - CaracterÃ­sticas numÃ©ricas: {len(numeric_features)} : {numeric_features}\")\n",
    "# print(f\"   - CaracterÃ­sticas categÃ³ricas: {len(categorical_features)}: {categorical_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Calculo de correlaciones\n",
    "from eda import correlation_analysis,show_correlation_respect_to_feature\n",
    "\n",
    "print(X_train_clean.columns)\n",
    "\n",
    "#show_correlation_respect_to_feature(X_train_clean,'Churn')\n",
    "\n",
    "target_cols = ['SeniorCitizen', 'Partner', 'Dependents', 'tenure', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod', 'MonthlyCharges', 'TotalCharges']\n",
    "correlation_analysis(X_train,target_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 3.1 - Preprocesamiento de datos usando los modelos creados\n",
    "# # RECARGAR MÃ“DULO MODELS CON CORRECCIONES (VERSIÃ“N ACTUALIZADA)\n",
    "# import importlib\n",
    "# import sys\n",
    "\n",
    "# # Limpiar mÃ³dulo del cache\n",
    "# if 'models' in sys.modules:\n",
    "#     del sys.modules['models']\n",
    "\n",
    "# # Importar y recargar\n",
    "# import models\n",
    "# importlib.reload(models)\n",
    "# from models import ChurnPredictor\n",
    "\n",
    "# print(\"ðŸ”„ MÃ³dulo models recargado con correcciones aplicadas\")\n",
    "# print(\"âœ… Correcciones incluidas:\")\n",
    "# print(\"   - Fix para transformer 'bin' no existente\")\n",
    "# print(\"   - Manejo robusto de columnas transformadas\")\n",
    "# print(\"   - CorrecciÃ³n en map_target para numpy arrays\")\n",
    "\n",
    "# # Reinicializar el predictor\n",
    "# predictor = ChurnPredictor(random_state=42)\n",
    "\n",
    "# # Crear el preprocesador\n",
    "# predictor.create_preprocessor(X_train_clean)\n",
    "\n",
    "# predictor.inspect_transformed_columns(\n",
    "#      X_original=X_train,\n",
    "#      columns=['Partner', 'Dependents', 'Contract', 'PaymentMethod']\n",
    "# )\n",
    "\n",
    "# # Crear los modelos (esto automÃ¡ticamente usa el preprocesador)\n",
    "# models_dict = predictor.create_models()\n",
    "\n",
    "# # ENTRENAR con los datos sincronizados\n",
    "# print(\"\\nðŸŽ¯ Iniciando entrenamiento con datos sincronizados...\")\n",
    "# predictor.train_models(X_train_clean, y_train_sync)\n",
    "\n",
    "# print(\"\\nðŸŽ‰ Entrenamiento completado para todos los modelos:\")\n",
    "# for model_name in models_dict.keys():\n",
    "#     print(f\"   âœ… {model_name}\")\n",
    "\n",
    "# print(f\"\\nðŸ“Š Resumen del entrenamiento:\")\n",
    "# print(f\"   - Datos de entrenamiento: {X_train_clean.shape}\")\n",
    "# print(f\"   - Etiquetas: {y_train_sync.shape if hasattr(y_train_sync, 'shape') else len(y_train_sync)}\")\n",
    "# print(f\"   - Modelos entrenados: {len(models_dict)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 3.2 - InspecciÃ³n de transformaciones de columnas\n",
    "# # La funciÃ³n inspect_transformed_columns es un MÃ‰TODO de la clase ChurnPredictor\n",
    "# # Se debe llamar usando la instancia: predictor.inspect_transformed_columns()\n",
    "\n",
    "# print(\"ðŸ” Inspeccionando transformaciones de columnas especÃ­ficas...\")\n",
    "\n",
    "# try:\n",
    "#     # Usar el MÃ‰TODO de la instancia predictor (no como funciÃ³n independiente)\n",
    "#     predictor.inspect_transformed_columns(\n",
    "#         X_original=X_train,\n",
    "#         columns=['Partner', 'Dependents', 'Contract', 'PaymentMethod'],\n",
    "#         fit=False  # False porque ya hicimos fit anteriormente\n",
    "#     )\n",
    "# except Exception as e:\n",
    "#     print(f\"âš ï¸ Error al inspeccionar columnas: {e}\")\n",
    "#     print(\"ðŸ’¡ Continuando con el anÃ¡lisis bÃ¡sico...\")\n",
    "    \n",
    "#     # AnÃ¡lisis alternativo si la funciÃ³n falla\n",
    "#     print(\"\\nðŸ“Š AnÃ¡lisis bÃ¡sico de columnas seleccionadas:\")\n",
    "#     selected_cols = ['Partner', 'Dependents', 'Contract', 'PaymentMethod']\n",
    "#     for col in selected_cols:\n",
    "#         if col in X_train.columns:\n",
    "#             print(f\"   - {col}: {X_train[col].unique()}\")\n",
    "#         else:\n",
    "#             print(f\"   - {col}: âŒ No encontrada en el dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 3.1.1 Crear preprocesador para LogisticRegression\n",
    "# # RECARGAR MÃ“DULO MODELS CON CORRECCIONES (VERSIÃ“N ACTUALIZADA)\n",
    "# import importlib\n",
    "# import sys\n",
    "\n",
    "# # Limpiar mÃ³dulo del cache\n",
    "# if 'models' in sys.modules:\n",
    "#     del sys.modules['models']\n",
    "\n",
    "# # Importar y recargar\n",
    "# import models\n",
    "# importlib.reload(models)\n",
    "# from models import ChurnPredictor\n",
    "\n",
    "# print(\"ðŸ”„ MÃ³dulo models recargado con correcciones aplicadas\")\n",
    "# print(\"âœ… Correcciones incluidas:\")\n",
    "# print(\"   - Fix para transformer 'bin' no existente\")\n",
    "# print(\"   - Manejo robusto de columnas transformadas\")\n",
    "# print(\"   - CorrecciÃ³n en map_target para numpy arrays\")\n",
    "\n",
    "# # Reinicializar el predictor\n",
    "# predictor = ChurnPredictor(random_state=42)\n",
    "\n",
    "# # Crear el preprocesador\n",
    "# predictor.create_preprocessor(X_train_clean)\n",
    "\n",
    "# predictor.inspect_transformed_columns(\n",
    "#      X_original=X_train,\n",
    "#      columns=['Partner', 'Dependents', 'Contract', 'PaymentMethod']\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **4. Modelado y EvaluaciÃ³n**\n",
    "\n",
    "## Ahora entrenaremos y evaluaremos los tres modelos requeridos:\n",
    "## RegresiÃ³n LogÃ­stica, k-NN y Naive Bayes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.1. PreparaciÃ³n de datos de validaciÃ³n de train y test.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda Ãºnica para importar siempre la versiÃ³n mÃ¡s reciente de tu clase\n",
    "\n",
    "import sys\n",
    "import importlib\n",
    "\n",
    "# Ruta a tu mÃ³dulo (ajustÃ¡ si es necesario)\n",
    "module_name = \"models\"\n",
    "\n",
    "# Eliminar del cachÃ© de mÃ³dulos si ya estaba cargado\n",
    "if module_name in sys.modules:\n",
    "    del sys.modules[module_name]\n",
    "\n",
    "# Importar y recargar\n",
    "import models\n",
    "importlib.reload(models)\n",
    "\n",
    "# Instanciar la clase\n",
    "if predictor is None:\n",
    "    predictor = models.ChurnPredictor()\n",
    "    print('Clase predicto instanciada')\n",
    "\n",
    "print('Se actualizaron las instancias necesarias')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 Modelado \n",
    "print(\"ðŸ¤– Iniciando entrenamiento de modelos...\")\n",
    "\n",
    "# Preparar datos para la divisiÃ³n train/validation\n",
    "print(\"ðŸ”§ Preparando datos para divisiÃ³n train/validation...\")\n",
    "\n",
    "# Cargar datos originales si es necesario\n",
    "try:\n",
    "    # Separar features y target\n",
    "    y = X_train[\"Churn\"]\n",
    "    print(f\"ðŸ“Š Variable objetivo extraÃ­da: {y.shape}\")\n",
    "    \n",
    "    # Extraer caracterÃ­sticas (X) - remover Churn y customerID\n",
    "    columns_to_drop = ['Churn']\n",
    "    if 'customerID' in X_train.columns:\n",
    "        columns_to_drop.append('customerID')\n",
    "  \n",
    "    X = X_train.drop(columns_to_drop, axis=1)\n",
    "    print(f\"ðŸ“Š CaracterÃ­sticas extraÃ­das: {X.shape}\")\n",
    "    print(f\"ðŸ“‹ Columnas removidas: {columns_to_drop}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error preparando datos: {e}\")\n",
    "    print(\"ðŸ’¡ AsegÃºrate de que el dataset estÃ© cargado correctamente\")\n",
    "\n",
    "# Dividir datos en entrenamiento y validaciÃ³n interna\n",
    "print(\"\\nðŸ”„ Dividiendo datos en train/validation interno...\")\n",
    "X_train_split, X_val, y_train_split, y_val = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.2 Crear los preprocesadores para datos de validaciÃ³n**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2.1 Crear preprocesador para el modelo LogisticRegression\n",
    "\n",
    "#Preprocesador para LogisticRegression\n",
    "try:\n",
    "    preprocessor_logistic_regression = predictor.create_preprocessor_logistic_regression(X_train_split)\n",
    "    \n",
    "    print(\"âœ… Preprocesador para LogisticRegression configurado exitosamente\")\n",
    "    X_val_features = X_train_split.shape[1]\n",
    "    print(f\"ðŸ“Š CaracterÃ­sticas a procesar: {X_val_features}\")\n",
    "    \n",
    "    #Mostrar estado de columnas luego del preprocesamiento.\n",
    "    # predictor.inspect_transformed_columns(\n",
    "    #     X_original=X_train_split,\n",
    "    #     columns=['Partner', 'Dependents', 'Contract', 'PaymentMethod']\n",
    "    # )\n",
    "    \n",
    "    # Mostrar informaciÃ³n del preprocesador\n",
    "    print(\"\\nðŸ”§ ConfiguraciÃ³n del preprocesador:\")\n",
    "    numeric_features = X_train_split.select_dtypes(include=['int64', 'float64']).columns\n",
    "    categorical_features = X_train_split.select_dtypes(include=['object']).columns                      \n",
    "                                             \n",
    "    print(f\"   - CaracterÃ­sticas numÃ©ricas: {len(numeric_features)} : {numeric_features}\")\n",
    "    print(f\"   - CaracterÃ­sticas categÃ³ricas: {len(categorical_features)}: {categorical_features}\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error preprocesando los datos: {e}\")\n",
    "    print(\"ðŸ’¡ AsegÃºrate de que el dataset estÃ© cargado correctamente\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2.2 Crear preprocesador para el modelo KNN\n",
    "\n",
    "#Preprocesador para KNN\n",
    "try:\n",
    "    preprocessor_knn = predictor.create_preprocessor_knn(X_train_split)\n",
    "    \n",
    "    print(\"âœ… Preprocesador para KNN configurado exitosamente\")\n",
    "    X_val_features = X_train_split.shape[1]\n",
    "    print(f\"ðŸ“Š CaracterÃ­sticas a procesar: {X_val_features}\")\n",
    "    \n",
    "    #Mostrar estado de columnas luego del preprocesamiento.\n",
    "    # predictor.inspect_transformed_columns(\n",
    "    #     X_original=X_train_split,\n",
    "    #     columns=['Partner', 'Dependents', 'Contract', 'PaymentMethod']\n",
    "    # )\n",
    "    \n",
    "    # Mostrar informaciÃ³n del preprocesador\n",
    "    print(\"\\nðŸ”§ ConfiguraciÃ³n del preprocesador:\")\n",
    "    numeric_features = X_train_split.select_dtypes(include=['int64', 'float64']).columns\n",
    "    categorical_features = X_train_split.select_dtypes(include=['object']).columns                      \n",
    "                                             \n",
    "    print(f\"   - CaracterÃ­sticas numÃ©ricas: {len(numeric_features)} : {numeric_features}\")\n",
    "    print(f\"   - CaracterÃ­sticas categÃ³ricas: {len(categorical_features)}: {categorical_features}\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error preprocesando los datos: {e}\")\n",
    "    print(\"ðŸ’¡ AsegÃºrate de que el dataset estÃ© cargado correctamente\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2.3 Crear preprocesador para el modelo RandomForest\n",
    "\n",
    "#Preprocesador para RandomForest\n",
    "try:\n",
    "    preprocessor_random_forest = predictor.create_preprocessor_random_forest(X_train_split)\n",
    "    \n",
    "    print(\"âœ… Preprocesador para RandomForest configurado exitosamente\")\n",
    "    X_val_features = X_train_split.shape[1]\n",
    "    print(f\"ðŸ“Š CaracterÃ­sticas a procesar: {X_val_features}\")\n",
    "    \n",
    "    #Mostrar estado de columnas luego del preprocesamiento.\n",
    "    # predictor.inspect_transformed_columns(\n",
    "    #     X_original=X_val,\n",
    "    #     columns=['Partner', 'Dependents', 'Contract', 'PaymentMethod']\n",
    "    # )\n",
    "    \n",
    "    # Mostrar informaciÃ³n del preprocesador\n",
    "    print(\"\\nðŸ”§ ConfiguraciÃ³n del preprocesador:\")\n",
    "    numeric_features = X_train_split.select_dtypes(include=['int64', 'float64']).columns\n",
    "    categorical_features = X_train_split.select_dtypes(include=['object']).columns                      \n",
    "                                             \n",
    "    print(f\"   - CaracterÃ­sticas numÃ©ricas: {len(numeric_features)} : {numeric_features}\")\n",
    "    print(f\"   - CaracterÃ­sticas categÃ³ricas: {len(categorical_features)}: {categorical_features}\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error preprocesando los datos: {e}\")\n",
    "    print(\"ðŸ’¡ AsegÃºrate de que el dataset estÃ© cargado correctamente\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2.4 Crear preprocesador para el modelo Naives Bayes\n",
    "\n",
    "#Preprocesador para Naives Bayes\n",
    "try:\n",
    "    preprocessor_naive_bayes = predictor.create_preprocessor_naive_bayes(X_train_split)\n",
    "    \n",
    "    print(\"âœ… Preprocesador para Naives Bayes configurado exitosamente\")\n",
    "    X_val_features = X_train_split.shape[1]\n",
    "    print(f\"ðŸ“Š CaracterÃ­sticas a procesar: {X_val_features}\")\n",
    "    \n",
    "    #Mostrar estado de columnas luego del preprocesamiento.\n",
    "    # predictor.inspect_transformed_columns(\n",
    "    #     X_original=X_train_split,\n",
    "    #     columns=['Partner', 'Dependents', 'Contract', 'PaymentMethod']\n",
    "    # )\n",
    "    \n",
    "    # Mostrar informaciÃ³n del preprocesador\n",
    "    print(\"\\nðŸ”§ ConfiguraciÃ³n del preprocesador:\")\n",
    "    numeric_features = X_train_split.select_dtypes(include=['int64', 'float64']).columns\n",
    "    categorical_features = X_train_split.select_dtypes(include=['object']).columns                      \n",
    "                                             \n",
    "    print(f\"   - CaracterÃ­sticas numÃ©ricas: {len(numeric_features)} : {numeric_features}\")\n",
    "    print(f\"   - CaracterÃ­sticas categÃ³ricas: {len(categorical_features)}: {categorical_features}\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error preprocesando los datos: {e}\")\n",
    "    print(\"ðŸ’¡ AsegÃºrate de que el dataset estÃ© cargado correctamente\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2.4 Crear preprocesador para el modelo XGBoost\n",
    "\n",
    "#Preprocesador para XGBoost\n",
    "try:\n",
    "    preprocessor_xgboost = predictor.create_preprocessor_xgboost(X_train_split)\n",
    "    \n",
    "    print(\"âœ… Preprocesador para XGBoost configurado exitosamente\")\n",
    "    X_val_features = X_train_split.shape[1]\n",
    "    print(f\"ðŸ“Š CaracterÃ­sticas a procesar: {X_val_features}\")\n",
    "    \n",
    "    #Mostrar estado de columnas luego del preprocesamiento.\n",
    "    # predictor.inspect_transformed_columns(\n",
    "    #     X_original=X_train_split,\n",
    "    #     columns=['Partner', 'Dependents', 'Contract', 'PaymentMethod']\n",
    "    # )\n",
    "    \n",
    "    # Mostrar informaciÃ³n del preprocesador\n",
    "    print(\"\\nðŸ”§ ConfiguraciÃ³n del preprocesador:\")\n",
    "    numeric_features = X_train_split.select_dtypes(include=['int64', 'float64']).columns\n",
    "    categorical_features = X_train_split.select_dtypes(include=['object']).columns                      \n",
    "                                             \n",
    "    print(f\"   - CaracterÃ­sticas numÃ©ricas: {len(numeric_features)} : {numeric_features}\")\n",
    "    print(f\"   - CaracterÃ­sticas categÃ³ricas: {len(categorical_features)}: {categorical_features}\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error preprocesando los datos: {e}\")\n",
    "    print(\"ðŸ’¡ AsegÃºrate de que el dataset estÃ© cargado correctamente\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2.5 Crear preprocesador para el modelo SVM\n",
    "\n",
    "#Preprocesador para SVM\n",
    "try:\n",
    "    preprocessor_svm = predictor.create_preprocessor_svm(X_train_split)\n",
    "    \n",
    "    print(\"âœ… Preprocesador para SVM configurado exitosamente\")\n",
    "    X_val_features = X_train_split.shape[1]\n",
    "    print(f\"ðŸ“Š CaracterÃ­sticas a procesar: {X_val_features}\")\n",
    "    \n",
    "    #Mostrar estado de columnas luego del preprocesamiento.\n",
    "    predictor.inspect_transformed_columns(\n",
    "        X_original=X_train_split,\n",
    "        columns=['Partner', 'Dependents', 'Contract', 'PaymentMethod']\n",
    "    )\n",
    "    \n",
    "    # Mostrar informaciÃ³n del preprocesador\n",
    "    print(\"\\nðŸ”§ ConfiguraciÃ³n del preprocesador:\")\n",
    "    numeric_features = X_train_split.select_dtypes(include=['int64', 'float64']).columns\n",
    "    categorical_features = X_train_split.select_dtypes(include=['object']).columns                      \n",
    "                                             \n",
    "    print(f\"   - CaracterÃ­sticas numÃ©ricas: {len(numeric_features)} : {numeric_features}\")\n",
    "    print(f\"   - CaracterÃ­sticas categÃ³ricas: {len(categorical_features)}: {categorical_features}\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error preprocesando los datos: {e}\")\n",
    "    print(\"ðŸ’¡ AsegÃºrate de que el dataset estÃ© cargado correctamente\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3.1 - **Crear los modelos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.3.1 Crear los modelos\n",
    "\n",
    "# Datos a evaluar\n",
    "print(\"\\nðŸ“Š X_val : \")\n",
    "display(X_val.head(5))\n",
    "print(\"\\nðŸ“Š y_val original\")\n",
    "display(y_val.head(5))   \n",
    "\n",
    "# IMPORTANTE: Mapear TANTO y_train_split como y_val para consistencia de tipos\n",
    "print(\"\\nðŸ“Š Mapeando datos para entrenamiento y evaluaciÃ³n...\")\n",
    "y_train_mapped = predictor.map_target(y_train_split)  # Mapear datos de entrenamiento\n",
    "y_val_mapped = predictor.map_target(y_val)           # Mapear datos de validaciÃ³n\n",
    "\n",
    "print(\"\\nðŸ“Š y_train_mapped\")\n",
    "display(y_train_mapped.head(5))   \n",
    "print(\"\\nðŸ“Š y_val_mapped\")\n",
    "display(y_val_mapped.head(5))   \n",
    "\n",
    "models = predictor.create_models(X_train)\n",
    "\n",
    "\n",
    "# Actualizar y_val para evaluaciÃ³n posterior\n",
    "y_val = y_val_mapped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3.2 - **Entrenar los modelos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar los modelos (esto automÃ¡ticamente usa el preprocesador optimizado para cada caso)\n",
    "#Recrear modelos con preprocessor fitted\n",
    "\n",
    "models = predictor.create_models(X_train_split)\n",
    "print(f\"âœ… Modelos recreados: {list(models.keys())}\")\n",
    "\n",
    "# Entrenar modelos\n",
    "print(\"\\nðŸŽ¯ Entrenando modelos con datos limpios...\")\n",
    "try:\n",
    "    predictor.train_models(X_train_split, y_train_mapped)\n",
    "    print(\"âœ… Entrenamiento completado sin errores\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error en entrenamiento: {e}\")\n",
    "\n",
    "print(\"\\nðŸš€ Paso 3: Verificar que los modelos estÃ¡n listos para evaluaciÃ³n\")\n",
    "\n",
    "# Verificar estado de los modelos\n",
    "for name, model in predictor.models.items():\n",
    "    print(f\"\\nðŸ” Verificando {name}:\")\n",
    "    \n",
    "    # Verificar que el pipeline tiene preprocessor fitted\n",
    "    if hasattr(model, 'named_steps') and 'preprocessor' in model.named_steps:\n",
    "        preprocessor = model.named_steps['preprocessor']\n",
    "        if hasattr(preprocessor, 'transformers_'):\n",
    "            print(f\"   âœ… Preprocessor fitted correctamente\")\n",
    "        else:\n",
    "            print(f\"   âŒ Preprocessor no fitted\")\n",
    "    \n",
    "    # Test de predicciÃ³n simple\n",
    "    try:\n",
    "        # Tomar una muestra pequeÃ±a para test\n",
    "        sample_X = X_val.head(5)\n",
    "        sample_pred = model.predict(sample_X)\n",
    "        sample_proba = model.predict_proba(sample_X)\n",
    "        print(f\"   âœ… PredicciÃ³n test exitosa: {len(sample_pred)} predicciones\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ Error en test de predicciÃ³n: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.4 - EvaluaciÃ³n de modelos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.4.1 EvaluaciÃ³n de modelos\n",
    "try:\n",
    "    from metrics import MetricsCalculator\n",
    "    \n",
    "    #ðŸ“Š Evaluando modelos...\"\n",
    "    results = predictor.evaluate_models(X_val, y_val)\n",
    "    best_model_name, best_model = predictor.get_best_model('ROC_AUC',results)\n",
    "    predictor.generate_model_report(X_val, y_val)\n",
    "    calc = MetricsCalculator()\n",
    "    y_pred = best_model.predict(X_val)\n",
    "    y_pred_proba = best_model.predict_proba(X_val)[:, 1]\n",
    "    calc.compare_models(results)\n",
    "    detailed_report = calc.generate_detailed_report(y_val, y_pred, y_pred_proba, class_names=['No Churn', 'Churn'], model_name=best_model_name)\n",
    "    print(f\"\\nðŸ† Mejor modelo seleccionado: {best_model_name}\")\n",
    "except ImportError:\n",
    "    print(\"âš ï¸ MÃ³dulo metrics no disponible, evaluando modelos bÃ¡sicamente...\")\n",
    "    results = predictor.evaluate_models(X_val, y_val)\n",
    "    best_model_name, best_model = predictor.get_best_model('ROC_AUC')\n",
    "    print(f\"\\nðŸ† Mejor modelo seleccionado: {best_model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  **5. SelecciÃ³n de Modelo y GeneraciÃ³n de Submission para Kaggle**\n",
    "\n",
    "## Basado en tus resultados de validaciÃ³n, elige el mejor modelo . Luego, re-entrÃ©nalo usando **todos los datos de `train.csv`** y Ãºsalo para hacer predicciones sobre `test.csv`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **FunciÃ³n para generar el archivo de submission**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicciones finales y creaciÃ³n del archivo de submission\n",
    "from submission import create_submission_file\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"ðŸ“„ Generando predicciones finales...\")\n",
    "\n",
    "# Generar timestamp unico para evitar sobrescribir archivos\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "submission_file = (\n",
    "    f\"submissions\\\\submission_grupoM_{timestamp}.csv\"\n",
    ")\n",
    "print(f\"[FILE] Archivo de submission: {submission_file}\")\n",
    "\n",
    "print(f\"ðŸ“Š Datos para entrenamiento final: {len(X_train_clean):,} muestras\")\n",
    "print(f'Best Model: {best_model}')\n",
    "\n",
    "# Crear archivo de submission\n",
    "submission_df = create_submission_file(\n",
    "    final_model=best_model,\n",
    "    X_train_full=X_train_clean,  # Solo caracterÃ­sticas\n",
    "    y_train_full=y_train_sync,   # Variable objetivo sincronizada.\n",
    "    X_test_full=X_test_clean, # Solo caracterÃ­sticas de test\n",
    "    customer_ids=customer_ids,\n",
    "    filename=submission_file\n",
    ")\n",
    "\n",
    "# Mostrar primeras predicciones\n",
    "print(f\"\\nðŸ“‹ Primeras 10 predicciones:\")\n",
    "print(submission_df.head(10))\n",
    "\n",
    "# EstadÃ­sticas de las predicciones\n",
    "predictions = submission_df.iloc[:, 1].values\n",
    "print(f\"\\nðŸ“Š EstadÃ­sticas de predicciones:\")\n",
    "print(f\"   - Predicciones de churn (>0.5): {np.sum(predictions > 0.5):,} ({np.mean(predictions > 0.5)*100:.1f}%)\")\n",
    "print(f\"   - Predicciones de no churn (â‰¤0.5): {np.sum(predictions <= 0.5):,} ({np.mean(predictions <= 0.5)*100:.1f}%)\")\n",
    "print(f\"   - Rango: [{predictions.min():.4f}, {predictions.max():.4f}]\")\n",
    "\n",
    "print(f\"\\nâœ… Archivo de submission {submission_file} creado exitosamente\")\n",
    "print(f\"ðŸŽ¯ Listo para subir a Kaggle!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q5Q5ZRMqom4o"
   },
   "outputs": [],
   "source": [
    "# OptimizaciÃ³n de hiperparÃ¡metros para el mejor modelo\n",
    "from models import hyperparameter_tuning\n",
    "\n",
    "print(f\"ðŸ”§ Optimizando hiperparÃ¡metros para {best_model_name}...\")\n",
    "\n",
    "from scipy.stats import loguniform\n",
    "#Randomized (rÃ¡pido):\n",
    "logit_dist = {\n",
    "    \"classifier__C\": loguniform(1e-3, 1e2),\n",
    "    \"classifier__solver\": [\"lbfgs\", \"liblinear\", \"saga\"],  # segÃºn  preprocesamiento\n",
    "    \"classifier__penalty\": [\"l2\"],                         # (lbfgs/liblinear/saga)\n",
    "    \"classifier__class_weight\": [None, \"balanced\"],\n",
    "    \"classifier__max_iter\": [2000],\n",
    "}\n",
    "#Grid fino (alrededor del mejor C):\n",
    "logit_grid = {\n",
    "    \"classifier__C\": [0.05, 0.1, 0.2, 0.5, 1, 2, 5],\n",
    "    \"classifier__solver\": [\"lbfgs\", \"liblinear\"],\n",
    "    \"classifier__class_weight\": [None, \"balanced\"],\n",
    "}\n",
    "\n",
    "# Definir grillas de parÃ¡metros segÃºn el modelo\n",
    "if 'Logistic' in best_model_name:\n",
    "    print(f'Tuneando hiper parametros para {best_model_name}')\n",
    "    param_grid = logit_grid #or logit_dist\n",
    "elif 'KNN' in best_model_name:\n",
    "    print(f'Tuneando hiper parametros para {best_model_name}')\n",
    "    param_grid = {\n",
    "        'classifier__n_neighbors': [3, 5, 7, 9],\n",
    "        'classifier__weights': ['uniform', 'distance']\n",
    "    }\n",
    "elif 'Random' in best_model_name:\n",
    "    print(f'Tuneando hiper parametros para {best_model_name}')\n",
    "    param_grid = {\n",
    "        'classifier__n_estimators': [50, 100, 200],\n",
    "        'classifier__max_depth': [None, 10, 20],\n",
    "        'classifier__min_samples_split': [2, 5]\n",
    "    }\n",
    "else:\n",
    "    # Para Naive Bayes u otros\n",
    "    print(f'Tuneando hiper parametros para {best_model_name}')\n",
    "    param_grid = {\n",
    "        'classifier__var_smoothing': [1e-9, 1e-8, 1e-7]\n",
    "    }\n",
    "\n",
    "# Realizar bÃºsqueda de hiperparÃ¡metros\n",
    "grid_search = hyperparameter_tuning(\n",
    "    best_model, param_grid, X_train, y_train,\n",
    "    cv=5, scoring='roc_auc'\n",
    ")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ HiperparÃ¡metros optimizados:\")\n",
    "print(f\"   - Mejor score CV: {grid_search.best_score_:.4f}\")\n",
    "print(f\"   - Mejores parÃ¡metros: {grid_search.best_params_}\")\n",
    "\n",
    "# Actualizar el mejor modelo con los parÃ¡metros optimizados\n",
    "optimized_model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluar modelo optimizado\n",
    "y_pred_opt = optimized_model.predict(X_val)\n",
    "y_pred_proba_opt = optimized_model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# Calcular mÃ©tricas del modelo optimizado\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "opt_auc = roc_auc_score(y_val, y_pred_proba_opt)\n",
    "opt_acc = accuracy_score(y_val, y_pred_opt)\n",
    "\n",
    "print(f\"\\nðŸ“ˆ Mejora con optimizaciÃ³n:\")\n",
    "print(f\"   - ROC AUC original: {results[best_model_name]['ROC_AUC']:.4f}\")\n",
    "print(f\"   - ROC AUC optimizado: {opt_auc:.4f}\")\n",
    "print(f\"   - Mejora: {opt_auc - results[best_model_name]['ROC_AUC']:.4f}\")\n",
    "\n",
    "# Guardar el modelo optimizado\n",
    "best_model = optimized_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  **6. Conclusiones (Opcional pero Recomendado)**\n",
    "\n",
    "## Escribe un breve resumen de tus hallazgos.\n",
    "* ## Â¿QuÃ© modelo funcionÃ³ mejor y por quÃ© crees que fue asÃ­?\n",
    "* ## Â¿CuÃ¡les fueron las caracterÃ­sticas mÃ¡s importantes o los descubrimientos mÃ¡s interesantes del EDA?\n",
    "* ## Â¿QuÃ© desafÃ­os encontraron y cÃ³mo los resolvieron?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ§ª PRUEBA DE KERNEL - CELDA SIMPLE\n",
    "print(\"ðŸš€ Â¡Kernel funcionando correctamente!\")\n",
    "print(\"âœ… VS Code Jupyter Extension conectada\")\n",
    "\n",
    "# Test bÃ¡sico de Python\n",
    "import sys\n",
    "print(f\"ðŸ“ Python version: {sys.version}\")\n",
    "print(f\"ðŸ“‚ Python executable: {sys.executable}\")\n",
    "\n",
    "# Test de matemÃ¡ticas bÃ¡sicas\n",
    "resultado = 2 + 2\n",
    "print(f\"ðŸ§® Test matemÃ¡tico: 2 + 2 = {resultado}\")\n",
    "\n",
    "# Test de pandas\n",
    "try:\n",
    "    import pandas as pd\n",
    "    print(f\"ðŸ“Š Pandas version: {pd.__version__}\")\n",
    "    print(\"âœ… Pandas importado correctamente\")\n",
    "except ImportError:\n",
    "    print(\"âŒ Error importando pandas\")\n",
    "\n",
    "print(\"\\nðŸŽ‰ Â¡Kernel listo para trabajar!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ”§ DIAGNÃ“STICO Y CORRECCIÃ“N COMPLETA DEL PROBLEMA DE COLUMNSTRANSFORMER\n",
    "# Problema identificado: Los transformers no tienen atributo 'transformers_' porque no han sido fitted\n",
    "\n",
    "print(\"ðŸ” DIAGNÃ“STICO COMPLETO DEL PROBLEMA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. Verificar estado del predictor y modelos\n",
    "print(\"\\n1ï¸âƒ£ Estado actual del predictor:\")\n",
    "print(f\"   - Predictor existe: {predictor is not None}\")\n",
    "if hasattr(predictor, 'models'):\n",
    "    print(f\"   - Modelos creados: {len(predictor.models) if predictor.models else 0}\")\n",
    "    if predictor.models:\n",
    "        print(f\"   - Nombres de modelos: {list(predictor.models.keys())}\")\n",
    "else:\n",
    "    print(\"   - No hay modelos en el predictor\")\n",
    "\n",
    "# 2. Verificar estado del preprocessor\n",
    "print(\"\\n2ï¸âƒ£ Estado del preprocessor:\")\n",
    "if hasattr(predictor, 'preprocessor') and predictor.preprocessor is not None:\n",
    "    print(f\"   - Preprocessor existe: {type(predictor.preprocessor).__name__}\")\n",
    "    \n",
    "    # Verificar si estÃ¡ fitted\n",
    "    if hasattr(predictor.preprocessor, 'transformers_'):\n",
    "        print(\"   âœ… Preprocessor estÃ¡ fitted (tiene transformers_)\")\n",
    "    else:\n",
    "        print(\"   âŒ Preprocessor NO estÃ¡ fitted (falta transformers_)\")\n",
    "        print(\"   ðŸ”§ Esto causa el error en evaluate_models\")\n",
    "else:\n",
    "    print(\"   âŒ No hay preprocessor configurado\")\n",
    "\n",
    "# 3. Verificar datos de entrenamiento\n",
    "print(\"\\n3ï¸âƒ£ Estado de los datos:\")\n",
    "print(f\"   - X_train_split shape: {X_train_split.shape}\")\n",
    "print(f\"   - y_train_mapped shape: {y_train_mapped.shape if hasattr(y_train_mapped, 'shape') else len(y_train_mapped)}\")\n",
    "print(f\"   - X_val shape: {X_val.shape}\")\n",
    "print(f\"   - y_val shape: {y_val.shape if hasattr(y_val, 'shape') else len(y_val)}\")\n",
    "\n",
    "print(\"\\nðŸ”§ APLICANDO CORRECCIÃ“N...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# SOLUCIÃ“N: Re-entrenar completamente con verificaciÃ³n paso a paso\n",
    "print(\"\\nðŸš€ Paso 1: Recrear preprocessor y verificar fitting\")\n",
    "\n",
    "# Crear preprocessor y fittear explÃ­citamente\n",
    "predictor.create_preprocessor(X_train_split)\n",
    "\n",
    "# Verificar que el preprocessor se creÃ³ correctamente\n",
    "if hasattr(predictor, 'preprocessor') and predictor.preprocessor is not None:\n",
    "    print(\"âœ… Preprocessor recreado exitosamente\")\n",
    "    \n",
    "    # Fit del preprocessor de manera explÃ­cita\n",
    "    print(\"ðŸ”§ Fitting preprocessor...\")\n",
    "    predictor.preprocessor.fit(X_train_split)\n",
    "    \n",
    "    # Verificar que ahora tiene transformers_\n",
    "    if hasattr(predictor.preprocessor, 'transformers_'):\n",
    "        print(\"âœ… Preprocessor fitted correctamente (transformers_ disponible)\")\n",
    "    else:\n",
    "        print(\"âŒ AÃºn hay problemas con el fitting del preprocessor\")\n",
    "else:\n",
    "    print(\"âŒ Error recreando preprocessor\")\n",
    "\n",
    "print(\"\\nðŸš€ Paso 2: Recrear y entrenar modelos\")\n",
    "\n",
    "# Recrear modelos con preprocessor fitted\n",
    "models = predictor.create_models(X_train_split)\n",
    "print(f\"âœ… Modelos recreados: {list(models.keys())}\")\n",
    "\n",
    "# Entrenar modelos\n",
    "print(\"\\nðŸŽ¯ Entrenando modelos con datos limpios...\")\n",
    "try:\n",
    "    predictor.train_models(X_train_split, y_train_mapped)\n",
    "    print(\"âœ… Entrenamiento completado sin errores\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error en entrenamiento: {e}\")\n",
    "\n",
    "print(\"\\nðŸš€ Paso 3: Verificar que los modelos estÃ¡n listos para evaluaciÃ³n\")\n",
    "\n",
    "# Verificar estado de los modelos\n",
    "for name, model in predictor.models.items():\n",
    "    print(f\"\\nðŸ” Verificando {name}:\")\n",
    "    \n",
    "    # Verificar que el pipeline tiene preprocessor fitted\n",
    "    if hasattr(model, 'named_steps') and 'preprocessor' in model.named_steps:\n",
    "        preprocessor = model.named_steps['preprocessor']\n",
    "        if hasattr(preprocessor, 'transformers_'):\n",
    "            print(f\"   âœ… Preprocessor fitted correctamente\")\n",
    "        else:\n",
    "            print(f\"   âŒ Preprocessor no fitted\")\n",
    "    \n",
    "    # Test de predicciÃ³n simple\n",
    "    try:\n",
    "        # Tomar una muestra pequeÃ±a para test\n",
    "        sample_X = X_val.head(5)\n",
    "        sample_pred = model.predict(sample_X)\n",
    "        sample_proba = model.predict_proba(sample_X)\n",
    "        print(f\"   âœ… PredicciÃ³n test exitosa: {len(sample_pred)} predicciones\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ Error en test de predicciÃ³n: {e}\")\n",
    "\n",
    "print(\"\\nðŸŽ¯ CORRECCIÃ“N COMPLETADA - LISTA PARA EVALUACIÃ“N\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ” DEPURACIÃ“N DETALLADA DEL ERROR \"A given column is not a column of the dataframe\"\n",
    "\n",
    "print(\"ðŸ” DEPURANDO EL ERROR DE CREATE_MODELS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. Verificar las columnas disponibles\n",
    "print(\"1ï¸âƒ£ Verificando columnas en X_train_split:\")\n",
    "print(f\"   - Shape: {X_train_split.shape}\")\n",
    "print(f\"   - Columnas: {list(X_train_split.columns)}\")\n",
    "print(f\"   - Contiene 'Churn': {'Churn' in X_train_split.columns}\")\n",
    "print(f\"   - Contiene 'customerID': {'customerID' in X_train_split.columns}\")\n",
    "\n",
    "# 2. Probar FeatureEngineer paso a paso\n",
    "print(\"\\n2ï¸âƒ£ Probando FeatureEngineer...\")\n",
    "try:\n",
    "    from models import FeatureEngineer\n",
    "    \n",
    "    fe = FeatureEngineer()\n",
    "    print(\"   âœ… FeatureEngineer instanciado\")\n",
    "    \n",
    "    # Aplicar transform\n",
    "    X_engineered = fe.transform(X_train_split)\n",
    "    print(f\"   âœ… Transform exitoso - Nueva shape: {X_engineered.shape}\")\n",
    "    print(f\"   - Nuevas columnas: {set(X_engineered.columns) - set(X_train_split.columns)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   âŒ Error en FeatureEngineer: {e}\")\n",
    "    print(f\"   Tipo de error: {type(e).__name__}\")\n",
    "\n",
    "# 3. Probar el mÃ©todo _normalize_service_values\n",
    "print(\"\\n3ï¸âƒ£ Probando _normalize_service_values...\")\n",
    "try:\n",
    "    X_normalized = predictor._normalize_service_values(X_train_split)\n",
    "    print(f\"   âœ… NormalizaciÃ³n exitosa - Shape: {X_normalized.shape}\")\n",
    "    \n",
    "    # Verificar algunos cambios especÃ­ficos\n",
    "    for col in ['OnlineSecurity', 'MultipleLines']:\n",
    "        if col in X_train_split.columns:\n",
    "            original_values = set(X_train_split[col].unique())\n",
    "            normalized_values = set(X_normalized[col].unique())\n",
    "            if original_values != normalized_values:\n",
    "                print(f\"   - {col}: {original_values} â†’ {normalized_values}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   âŒ Error en _normalize_service_values: {e}\")\n",
    "\n",
    "# 4. Probar _pick_preprocessor para cada modelo\n",
    "print(\"\\n4ï¸âƒ£ Probando _pick_preprocessor para cada modelo...\")\n",
    "\n",
    "# Aplicar normalizaciÃ³n y feature engineering primero\n",
    "try:\n",
    "    fe = FeatureEngineer()\n",
    "    X_for_schema = fe.transform(predictor._normalize_service_values(X_train_split))\n",
    "    print(f\"   âœ… X_for_schema preparado - Shape: {X_for_schema.shape}\")\n",
    "    \n",
    "    models_to_test = [\"Logistic_Regression\", \"Random_Forest\", \"Naive_Bayes\", \"KNN\"]\n",
    "    \n",
    "    for model_name in models_to_test:\n",
    "        try:\n",
    "            preprocessor = predictor._pick_preprocessor(model_name, X_for_schema)\n",
    "            print(f\"   âœ… {model_name}: Preprocessor creado exitosamente\")\n",
    "            \n",
    "            # Probar fit del preprocessor\n",
    "            preprocessor.fit(X_for_schema)\n",
    "            print(f\"   âœ… {model_name}: Preprocessor fitted exitosamente\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ {model_name}: Error - {e}\")\n",
    "            print(f\"      Tipo de error: {type(e).__name__}\")\n",
    "            \n",
    "            # InformaciÃ³n adicional para debugging\n",
    "            if \"column\" in str(e).lower():\n",
    "                print(f\"      Columnas disponibles en X_for_schema: {list(X_for_schema.columns)}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"   âŒ Error preparando X_for_schema: {e}\")\n",
    "\n",
    "print(\"\\nðŸŽ¯ DIAGNÃ“STICO COMPLETADO\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
