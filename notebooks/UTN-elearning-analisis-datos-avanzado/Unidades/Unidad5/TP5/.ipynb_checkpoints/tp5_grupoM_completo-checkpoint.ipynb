{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image in a markdown cell](https://cursos.utnba.centrodeelearning.com/pluginfile.php/1/theme_space/customlogo/1738330016/Logo%20UTN%20Horizontal.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Diplomado de Ciencia de Datos y An√°lisis Avanzado**\n",
    "# **Unidad 5: Modelado Predictivo I**: Regresi√≥n y Clasificaci√≥n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Proyecto de Competencia Kaggle: Predicci√≥n de Abandono de Clientes**\n",
    "\n",
    "## **Curso:** Diplomado en Ciencia de Datos\n",
    "\n",
    "# ***Nombres de los Miembros del Equipo: Grupo M***\n",
    "### *   Lucia Cortes\n",
    "### *   Maria Fernanda Farias\n",
    "### *   Alejandro Gomez Grosschadl\n",
    "### *   Favio Ruggieri\n",
    "### *   Sergio Salanitri\n",
    "### *   Karina Calvo\n",
    "\n",
    "# **Objetivo:**\n",
    "## El objetivo de este proyecto es construir y evaluar varios modelos de clasificaci√≥n para predecir si un cliente de una compa√±√≠a de telecomunicaciones abandonar√° o no el servicio (churn). El rendimiento final del mejor modelo se medir√° en la competencia de Kaggle a trav√©s de la **m√©trica ROC AUC**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Enlace para unirse a la competencia**\n",
    "### **USE EL ENLACE PARA UNIRSE POR EQUIPO, NO DE MANERA INDIVIDUAL**\n",
    "\n",
    "https://www.kaggle.com/t/57b70c381e4d451b8ae38e164b91a2aa\n",
    "\n",
    "\n",
    "### **Por favor siga las indicaciones que se suministran en la plataforma**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. **Configuraci√≥n Inicial e Importaci√≥n de Librer√≠as**\n",
    "\n",
    "## En esta secci√≥n, importaremos todas las librer√≠as necesarias para el proyecto. Es una buena pr√°ctica tener todas las importaciones en la primera celda.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importaciones b√°sicas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV,train_test_split\n",
    "from sklearn.metrics import (roc_auc_score, accuracy_score, precision_score, \n",
    "                           recall_score, f1_score, classification_report, \n",
    "                           confusion_matrix, roc_curve)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Tuple, Any\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# Configurar visualizaciones\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "# Importar m√≥dulos del proyecto (si existen)\n",
    "try:\n",
    "    import data_loader\n",
    "    import dataset_splitter\n",
    "    import eda\n",
    "    import models\n",
    "    import metrics\n",
    "    print(\"‚úÖ M√≥dulos del proyecto importados correctamente\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ö†Ô∏è Algunos m√≥dulos del proyecto no est√°n disponibles: {e}\")\n",
    "    print(\"üí° Puedes trabajar directamente en el notebook por ahora\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  **1. Carga de Datos**\n",
    "\n",
    "## Cargaremos los datasets proporcionados para la competencia: `train.csv`, `test.csv` y `sample_submission.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    X_train = pd.read_csv('train.csv')\n",
    "    X_test = pd.read_csv('test.csv')\n",
    "    sample_submission_df = pd.read_csv('sample_submission.csv')\n",
    "except FileNotFoundError:\n",
    "    print(\"Aseg√∫rate de que los archivos .csv de la competencia est√©n en el mismo directorio que este cuaderno.\")\n",
    "    # Si usas Colab, puedes subir los archivos al entorno de ejecuci√≥n.\n",
    "    exit()\n",
    "\n",
    "print(\"Forma del dataset de entrenamiento:\", X_train.shape)\n",
    "print(\"Forma del dataset de prueba:\", X_test.shape)\n",
    "\n",
    "print(\"\\nPrimeras 5 filas del dataset de entrenamiento:\")\n",
    "display(X_train.head())\n",
    "\n",
    "print(\"\\nPrimeras 5 filas del dataset de prueba:\")\n",
    "display(X_test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2**. An√°lisis Exploratorio de Datos (EDA)**\n",
    "\n",
    "### En esta fase, exploraremos el dataset de entrenamiento para entender mejor nuestros datos, encontrar patrones, identificar valores faltantes y visualizar relaciones entre las caracter√≠sticas y la variable objetivo (`Churn`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objetivo: conocer distribuci√≥n de datos, target, tipos de columnas.\n",
    "\n",
    "Variables como Contract, InternetService, PaymentMethod requieren OneHotEncoding o LabelEncoding.\n",
    "\n",
    "Target Churn: dataset m√°s desbalanceado (~20% churn). #Verificar el desbalanceo.\n",
    "\n",
    "## Descripci√≥n de par√°metros\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Informaci√≥n general del dataset\n",
    "print(\"üìä INFORMACI√ìN GENERAL DEL DATASET\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Dimensiones: {X_train.shape}\")\n",
    "print(f\"Columnas: {list(X_train.columns)}\")\n",
    "print(\"\\nüìã Tipos de datos:\")\n",
    "print(X_train.dtypes)\n",
    "\n",
    "# Informaci√≥n sobre valores faltantes\n",
    "print(\"\\nüîç VALORES FALTANTES:\")\n",
    "print(\"=\" * 30)\n",
    "missing_values = X_train.isnull().sum()\n",
    "print(missing_values[missing_values > 0])\n",
    "if missing_values.sum() == 0:\n",
    "    print(\"‚úÖ No hay valores faltantes\")\n",
    "\n",
    "# Distribuci√≥n de la variable objetivo\n",
    "print(\"\\nüéØ DISTRIBUCI√ìN DE LA VARIABLE OBJETIVO (Churn):\")\n",
    "print(\"=\" * 50)\n",
    "y_train = X_train['Churn']\n",
    "churn_counts = y_train.value_counts()\n",
    "#Definici√≥n de variable objetivo\n",
    "churn_pct = y_train.value_counts(normalize=True) * 100\n",
    "print(f\"No Churn (0): {churn_counts[0]} ({churn_pct[0]:.1f}%)\")\n",
    "print(f\"Churn (1): {churn_counts[1]} ({churn_pct[1]:.1f}%)\")\n",
    "\n",
    "# Visualizaci√≥n de la distribuci√≥n del target\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "y_train.value_counts().plot(kind='bar', color=['lightblue', 'salmon'])\n",
    "plt.title('Distribuci√≥n de Churn')\n",
    "plt.xlabel('Churn')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "y_train.value_counts(normalize=True).plot(kind='pie', autopct='%1.1f%%', colors=['lightblue', 'salmon'])\n",
    "plt.title('Proporci√≥n de Churn')\n",
    "plt.ylabel('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#Mostrar frecuenbcia de variables categ√≥ricas\n",
    "categorical_columns = ['gender', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod']\n",
    "\n",
    "# Mostrar frecuencias de variables categ√≥ricas en una grilla 4x4\n",
    "fig, axes = plt.subplots(4, 4, figsize=(20, 16))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(categorical_columns):\n",
    "    X_train[col].value_counts().plot(kind='bar', color='deeppink', ax=axes[i])\n",
    "    axes[i].set_title(f'Frecuencia de {col}')\n",
    "    axes[i].set_xlabel(col)\n",
    "    axes[i].set_ylabel('Conteo')\n",
    "    axes[i].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Eliminar ejes vac√≠os si hay menos de 16 columnas\n",
    "for j in range(len(categorical_columns), 16):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß CONFIGURACI√ìN Y ENTRENAMIENTO DEL CHURNPREDICTOR\n",
    "\n",
    "from models import ChurnPredictor\n",
    "\n",
    "print(\"üöÄ Inicializando ChurnPredictor...\")\n",
    "\n",
    "# Usar los datos ORIGINALES (antes del preprocesamiento) para crear el preprocesador\n",
    "predictor = ChurnPredictor(random_state=42)\n",
    "\n",
    "# Verificar y limpiar datos antes del preprocesamiento\n",
    "print(\"üîç Verificando datos antes del preprocesamiento...\")\n",
    "\n",
    "# Asumir que X_train son los datos originales (con customerID y sin procesar)\n",
    "# Necesitamos remover customerID de X_train si existe\n",
    "if 'customerID' in X_train.columns:\n",
    "    X_train_clean = X_train.drop(['customerID'], axis=1)\n",
    "else:\n",
    "    X_train_clean = X_train.copy()\n",
    "\n",
    "#customerIDs lo guardo para usarn en la generaci√≥n del archivo de submit.\n",
    "customer_ids = X_test['customerID']\n",
    "\n",
    "# Necesitamos remover customerID de X_test si existe\n",
    "if 'customerID' in X_test.columns:\n",
    "    X_test_clean = X_test.drop(['customerID'], axis=1)\n",
    "else:\n",
    "    X_test_clean = X_test.copy()\n",
    "\n",
    "\n",
    "# SINCRONIZACI√ìN FINAL: Asegurar que X e y tengan el mismo n√∫mero de muestras\n",
    "if X_train_clean.shape[0] != y_train.shape[0]:\n",
    "    print(\"‚ö†Ô∏è Sincronizando datos finales:\")\n",
    "    print(f\"   - X_train_clean: {X_train_clean.shape[0]} ‚Üí \", end=\"\")\n",
    "    print(f\"   - y_train: {y_train.shape[0]} ‚Üí \", end=\"\")\n",
    "    \n",
    "    min_samples = min(X_train_clean.shape[0], y_train.shape[0])\n",
    "    X_train_clean = X_train_clean.iloc[:min_samples]\n",
    "    y_train_sync = y_train.iloc[:min_samples] if hasattr(y_train, 'iloc') else y_train[:min_samples]\n",
    "    \n",
    "    print(f\"Sincronizados a {min_samples} muestras\")\n",
    "else:\n",
    "    y_train_sync = y_train\n",
    "    print(\"‚úÖ Datos ya est√°n sincronizados\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3. Preprocesamiento de Datos**\n",
    "\n",
    "## Prepararemos los datos para que puedan ser utilizados por los modelos de Machine Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANTE: Mapear y_train_sync para consistencia de tipos\n",
    "print(\"\\nüîß Mapeando y_train_sync a formato num√©rico...\")\n",
    "y_train_sync = predictor.map_target(y_train_sync)\n",
    "\n",
    "# Crear el preprocesador con los datos originales\n",
    "preprocessor = predictor.create_preprocessor(X_train_clean)\n",
    "\n",
    "print(\"‚úÖ Preprocesador configurado exitosamente\")\n",
    "print(f\"üìä Caracter√≠sticas procesadas: {X_train_clean.shape[1]}\")\n",
    "print(f\"üìä Muestras para entrenamiento: {X_train_clean.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 3.1 - Preprocesamiento de datos usando los modelos creados\n",
    "# from models import ChurnPredictor\n",
    "\n",
    "# # Inicializar el predictor\n",
    "# predictor = ChurnPredictor(random_state=42)\n",
    "\n",
    "# # Crear el preprocesador\n",
    "# predictor.create_preprocessor(X_train)\n",
    "\n",
    "# print(\"‚úÖ Preprocesador configurado exitosamente\")\n",
    "# X_features = X_train.shape[1]\n",
    "# print(f\"üìä Caracter√≠sticas a procesar: {X_features}\")\n",
    "\n",
    "# #Mostrar estado de columnas luego del preprocesamiento.\n",
    "# predictor.inspect_transformed_columns(\n",
    "#     X_original=X_train,\n",
    "#     columns=['Partner', 'Dependents', 'Contract', 'PaymentMethod']\n",
    "# )\n",
    "\n",
    "# # Mostrar informaci√≥n del preprocesador\n",
    "# print(\"\\nüîß Configuraci√≥n del preprocesador:\")\n",
    "# numeric_features = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
    "# categorical_features = X_train.select_dtypes(include=['object']).columns                      \n",
    "                                         \n",
    "# print(f\"   - Caracter√≠sticas num√©ricas: {len(numeric_features)} : {numeric_features}\")\n",
    "# print(f\"   - Caracter√≠sticas categ√≥ricas: {len(categorical_features)}: {categorical_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Calculo de correlaciones\n",
    "from eda import correlation_analysis,show_correlation_respect_to_feature\n",
    "\n",
    "print(X_train_clean.columns)\n",
    "\n",
    "#show_correlation_respect_to_feature(X_train_clean,'Churn')\n",
    "\n",
    "target_cols = ['SeniorCitizen', 'Partner', 'Dependents', 'tenure', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod', 'MonthlyCharges', 'TotalCharges']\n",
    "correlation_analysis(X_train,target_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 3.1 - Preprocesamiento de datos usando los modelos creados\n",
    "# # RECARGAR M√ìDULO MODELS CON CORRECCIONES (VERSI√ìN ACTUALIZADA)\n",
    "# import importlib\n",
    "# import sys\n",
    "\n",
    "# # Limpiar m√≥dulo del cache\n",
    "# if 'models' in sys.modules:\n",
    "#     del sys.modules['models']\n",
    "\n",
    "# # Importar y recargar\n",
    "# import models\n",
    "# importlib.reload(models)\n",
    "# from models import ChurnPredictor\n",
    "\n",
    "# print(\"üîÑ M√≥dulo models recargado con correcciones aplicadas\")\n",
    "# print(\"‚úÖ Correcciones incluidas:\")\n",
    "# print(\"   - Fix para transformer 'bin' no existente\")\n",
    "# print(\"   - Manejo robusto de columnas transformadas\")\n",
    "# print(\"   - Correcci√≥n en map_target para numpy arrays\")\n",
    "\n",
    "# # Reinicializar el predictor\n",
    "# predictor = ChurnPredictor(random_state=42)\n",
    "\n",
    "# # Crear el preprocesador\n",
    "# predictor.create_preprocessor(X_train_clean)\n",
    "\n",
    "# predictor.inspect_transformed_columns(\n",
    "#      X_original=X_train,\n",
    "#      columns=['Partner', 'Dependents', 'Contract', 'PaymentMethod']\n",
    "# )\n",
    "\n",
    "# # Crear los modelos (esto autom√°ticamente usa el preprocesador)\n",
    "# models_dict = predictor.create_models()\n",
    "\n",
    "# # ENTRENAR con los datos sincronizados\n",
    "# print(\"\\nüéØ Iniciando entrenamiento con datos sincronizados...\")\n",
    "# predictor.train_models(X_train_clean, y_train_sync)\n",
    "\n",
    "# print(\"\\nüéâ Entrenamiento completado para todos los modelos:\")\n",
    "# for model_name in models_dict.keys():\n",
    "#     print(f\"   ‚úÖ {model_name}\")\n",
    "\n",
    "# print(f\"\\nüìä Resumen del entrenamiento:\")\n",
    "# print(f\"   - Datos de entrenamiento: {X_train_clean.shape}\")\n",
    "# print(f\"   - Etiquetas: {y_train_sync.shape if hasattr(y_train_sync, 'shape') else len(y_train_sync)}\")\n",
    "# print(f\"   - Modelos entrenados: {len(models_dict)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 3.2 - Inspecci√≥n de transformaciones de columnas\n",
    "# # La funci√≥n inspect_transformed_columns es un M√âTODO de la clase ChurnPredictor\n",
    "# # Se debe llamar usando la instancia: predictor.inspect_transformed_columns()\n",
    "\n",
    "# print(\"üîç Inspeccionando transformaciones de columnas espec√≠ficas...\")\n",
    "\n",
    "# try:\n",
    "#     # Usar el M√âTODO de la instancia predictor (no como funci√≥n independiente)\n",
    "#     predictor.inspect_transformed_columns(\n",
    "#         X_original=X_train,\n",
    "#         columns=['Partner', 'Dependents', 'Contract', 'PaymentMethod'],\n",
    "#         fit=False  # False porque ya hicimos fit anteriormente\n",
    "#     )\n",
    "# except Exception as e:\n",
    "#     print(f\"‚ö†Ô∏è Error al inspeccionar columnas: {e}\")\n",
    "#     print(\"üí° Continuando con el an√°lisis b√°sico...\")\n",
    "    \n",
    "#     # An√°lisis alternativo si la funci√≥n falla\n",
    "#     print(\"\\nüìä An√°lisis b√°sico de columnas seleccionadas:\")\n",
    "#     selected_cols = ['Partner', 'Dependents', 'Contract', 'PaymentMethod']\n",
    "#     for col in selected_cols:\n",
    "#         if col in X_train.columns:\n",
    "#             print(f\"   - {col}: {X_train[col].unique()}\")\n",
    "#         else:\n",
    "#             print(f\"   - {col}: ‚ùå No encontrada en el dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 3.1.1 Crear preprocesador para LogisticRegression\n",
    "# # RECARGAR M√ìDULO MODELS CON CORRECCIONES (VERSI√ìN ACTUALIZADA)\n",
    "# import importlib\n",
    "# import sys\n",
    "\n",
    "# # Limpiar m√≥dulo del cache\n",
    "# if 'models' in sys.modules:\n",
    "#     del sys.modules['models']\n",
    "\n",
    "# # Importar y recargar\n",
    "# import models\n",
    "# importlib.reload(models)\n",
    "# from models import ChurnPredictor\n",
    "\n",
    "# print(\"üîÑ M√≥dulo models recargado con correcciones aplicadas\")\n",
    "# print(\"‚úÖ Correcciones incluidas:\")\n",
    "# print(\"   - Fix para transformer 'bin' no existente\")\n",
    "# print(\"   - Manejo robusto de columnas transformadas\")\n",
    "# print(\"   - Correcci√≥n en map_target para numpy arrays\")\n",
    "\n",
    "# # Reinicializar el predictor\n",
    "# predictor = ChurnPredictor(random_state=42)\n",
    "\n",
    "# # Crear el preprocesador\n",
    "# predictor.create_preprocessor(X_train_clean)\n",
    "\n",
    "# predictor.inspect_transformed_columns(\n",
    "#      X_original=X_train,\n",
    "#      columns=['Partner', 'Dependents', 'Contract', 'PaymentMethod']\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **4. Modelado y Evaluaci√≥n**\n",
    "\n",
    "## Ahora entrenaremos y evaluaremos los tres modelos requeridos:\n",
    "## Regresi√≥n Log√≠stica, k-NN y Naive Bayes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.1. Preparaci√≥n de datos de validaci√≥n de train y test.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda √∫nica para importar siempre la versi√≥n m√°s reciente de tu clase\n",
    "\n",
    "import sys\n",
    "import importlib\n",
    "\n",
    "# Ruta a tu m√≥dulo (ajust√° si es necesario)\n",
    "module_name = \"models\"\n",
    "\n",
    "# Eliminar del cach√© de m√≥dulos si ya estaba cargado\n",
    "if module_name in sys.modules:\n",
    "    del sys.modules[module_name]\n",
    "\n",
    "# Importar y recargar\n",
    "import models\n",
    "importlib.reload(models)\n",
    "\n",
    "# Instanciar la clase\n",
    "if predictor is None:\n",
    "    predictor = models.ChurnPredictor()\n",
    "    print('Clase predicto instanciada')\n",
    "\n",
    "print('Se actualizaron las instancias necesarias')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 Modelado \n",
    "print(\"ü§ñ Iniciando entrenamiento de modelos...\")\n",
    "\n",
    "# Preparar datos para la divisi√≥n train/validation\n",
    "print(\"üîß Preparando datos para divisi√≥n train/validation...\")\n",
    "\n",
    "# Cargar datos originales si es necesario\n",
    "try:\n",
    "    # Separar features y target\n",
    "    y = X_train[\"Churn\"]\n",
    "    print(f\"üìä Variable objetivo extra√≠da: {y.shape}\")\n",
    "    \n",
    "    # Extraer caracter√≠sticas (X) - remover Churn y customerID\n",
    "    columns_to_drop = ['Churn']\n",
    "    if 'customerID' in X_train.columns:\n",
    "        columns_to_drop.append('customerID')\n",
    "  \n",
    "    X = X_train.drop(columns_to_drop, axis=1)\n",
    "    print(f\"üìä Caracter√≠sticas extra√≠das: {X.shape}\")\n",
    "    print(f\"üìã Columnas removidas: {columns_to_drop}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error preparando datos: {e}\")\n",
    "    print(\"üí° Aseg√∫rate de que el dataset est√© cargado correctamente\")\n",
    "\n",
    "# Dividir datos en entrenamiento y validaci√≥n interna\n",
    "print(\"\\nüîÑ Dividiendo datos en train/validation interno...\")\n",
    "X_train_split, X_val, y_train_split, y_val = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.2 Crear los preprocesadores para datos de validaci√≥n**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2.1 Crear preprocesador para el modelo LogisticRegression\n",
    "\n",
    "#Preprocesador para LogisticRegression\n",
    "try:\n",
    "    preprocessor_logistic_regression = predictor.create_preprocessor_logistic_regression(X_train_split)\n",
    "    \n",
    "    print(\"‚úÖ Preprocesador para LogisticRegression configurado exitosamente\")\n",
    "    X_val_features = X_train_split.shape[1]\n",
    "    print(f\"üìä Caracter√≠sticas a procesar: {X_val_features}\")\n",
    "    \n",
    "    #Mostrar estado de columnas luego del preprocesamiento.\n",
    "    # predictor.inspect_transformed_columns(\n",
    "    #     X_original=X_train_split,\n",
    "    #     columns=['Partner', 'Dependents', 'Contract', 'PaymentMethod']\n",
    "    # )\n",
    "    \n",
    "    # Mostrar informaci√≥n del preprocesador\n",
    "    print(\"\\nüîß Configuraci√≥n del preprocesador:\")\n",
    "    numeric_features = X_train_split.select_dtypes(include=['int64', 'float64']).columns\n",
    "    categorical_features = X_train_split.select_dtypes(include=['object']).columns                      \n",
    "                                             \n",
    "    print(f\"   - Caracter√≠sticas num√©ricas: {len(numeric_features)} : {numeric_features}\")\n",
    "    print(f\"   - Caracter√≠sticas categ√≥ricas: {len(categorical_features)}: {categorical_features}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error preprocesando los datos: {e}\")\n",
    "    print(\"üí° Aseg√∫rate de que el dataset est√© cargado correctamente\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2.2 Crear preprocesador para el modelo KNN\n",
    "\n",
    "#Preprocesador para KNN\n",
    "try:\n",
    "    preprocessor_knn = predictor.create_preprocessor_knn(X_train_split)\n",
    "    \n",
    "    print(\"‚úÖ Preprocesador para KNN configurado exitosamente\")\n",
    "    X_val_features = X_train_split.shape[1]\n",
    "    print(f\"üìä Caracter√≠sticas a procesar: {X_val_features}\")\n",
    "    \n",
    "    #Mostrar estado de columnas luego del preprocesamiento.\n",
    "    # predictor.inspect_transformed_columns(\n",
    "    #     X_original=X_train_split,\n",
    "    #     columns=['Partner', 'Dependents', 'Contract', 'PaymentMethod']\n",
    "    # )\n",
    "    \n",
    "    # Mostrar informaci√≥n del preprocesador\n",
    "    print(\"\\nüîß Configuraci√≥n del preprocesador:\")\n",
    "    numeric_features = X_train_split.select_dtypes(include=['int64', 'float64']).columns\n",
    "    categorical_features = X_train_split.select_dtypes(include=['object']).columns                      \n",
    "                                             \n",
    "    print(f\"   - Caracter√≠sticas num√©ricas: {len(numeric_features)} : {numeric_features}\")\n",
    "    print(f\"   - Caracter√≠sticas categ√≥ricas: {len(categorical_features)}: {categorical_features}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error preprocesando los datos: {e}\")\n",
    "    print(\"üí° Aseg√∫rate de que el dataset est√© cargado correctamente\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2.3 Crear preprocesador para el modelo RandomForest\n",
    "\n",
    "#Preprocesador para RandomForest\n",
    "try:\n",
    "    preprocessor_random_forest = predictor.create_preprocessor_random_forest(X_train_split)\n",
    "    \n",
    "    print(\"‚úÖ Preprocesador para RandomForest configurado exitosamente\")\n",
    "    X_val_features = X_train_split.shape[1]\n",
    "    print(f\"üìä Caracter√≠sticas a procesar: {X_val_features}\")\n",
    "    \n",
    "    #Mostrar estado de columnas luego del preprocesamiento.\n",
    "    # predictor.inspect_transformed_columns(\n",
    "    #     X_original=X_val,\n",
    "    #     columns=['Partner', 'Dependents', 'Contract', 'PaymentMethod']\n",
    "    # )\n",
    "    \n",
    "    # Mostrar informaci√≥n del preprocesador\n",
    "    print(\"\\nüîß Configuraci√≥n del preprocesador:\")\n",
    "    numeric_features = X_train_split.select_dtypes(include=['int64', 'float64']).columns\n",
    "    categorical_features = X_train_split.select_dtypes(include=['object']).columns                      \n",
    "                                             \n",
    "    print(f\"   - Caracter√≠sticas num√©ricas: {len(numeric_features)} : {numeric_features}\")\n",
    "    print(f\"   - Caracter√≠sticas categ√≥ricas: {len(categorical_features)}: {categorical_features}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error preprocesando los datos: {e}\")\n",
    "    print(\"üí° Aseg√∫rate de que el dataset est√© cargado correctamente\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2.4 Crear preprocesador para el modelo Naives Bayes\n",
    "\n",
    "#Preprocesador para Naives Bayes\n",
    "try:\n",
    "    preprocessor_naive_bayes = predictor.create_preprocessor_naive_bayes(X_train_split)\n",
    "    \n",
    "    print(\"‚úÖ Preprocesador para Naives Bayes configurado exitosamente\")\n",
    "    X_val_features = X_train_split.shape[1]\n",
    "    print(f\"üìä Caracter√≠sticas a procesar: {X_val_features}\")\n",
    "    \n",
    "    #Mostrar estado de columnas luego del preprocesamiento.\n",
    "    # predictor.inspect_transformed_columns(\n",
    "    #     X_original=X_train_split,\n",
    "    #     columns=['Partner', 'Dependents', 'Contract', 'PaymentMethod']\n",
    "    # )\n",
    "    \n",
    "    # Mostrar informaci√≥n del preprocesador\n",
    "    print(\"\\nüîß Configuraci√≥n del preprocesador:\")\n",
    "    numeric_features = X_train_split.select_dtypes(include=['int64', 'float64']).columns\n",
    "    categorical_features = X_train_split.select_dtypes(include=['object']).columns                      \n",
    "                                             \n",
    "    print(f\"   - Caracter√≠sticas num√©ricas: {len(numeric_features)} : {numeric_features}\")\n",
    "    print(f\"   - Caracter√≠sticas categ√≥ricas: {len(categorical_features)}: {categorical_features}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error preprocesando los datos: {e}\")\n",
    "    print(\"üí° Aseg√∫rate de que el dataset est√© cargado correctamente\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2.4 Crear preprocesador para el modelo XGBoost\n",
    "\n",
    "#Preprocesador para XGBoost\n",
    "try:\n",
    "    preprocessor_xgboost = predictor.create_preprocessor_xgboost(X_train_split)\n",
    "    \n",
    "    print(\"‚úÖ Preprocesador para XGBoost configurado exitosamente\")\n",
    "    X_val_features = X_train_split.shape[1]\n",
    "    print(f\"üìä Caracter√≠sticas a procesar: {X_val_features}\")\n",
    "    \n",
    "    #Mostrar estado de columnas luego del preprocesamiento.\n",
    "    # predictor.inspect_transformed_columns(\n",
    "    #     X_original=X_train_split,\n",
    "    #     columns=['Partner', 'Dependents', 'Contract', 'PaymentMethod']\n",
    "    # )\n",
    "    \n",
    "    # Mostrar informaci√≥n del preprocesador\n",
    "    print(\"\\nüîß Configuraci√≥n del preprocesador:\")\n",
    "    numeric_features = X_train_split.select_dtypes(include=['int64', 'float64']).columns\n",
    "    categorical_features = X_train_split.select_dtypes(include=['object']).columns                      \n",
    "                                             \n",
    "    print(f\"   - Caracter√≠sticas num√©ricas: {len(numeric_features)} : {numeric_features}\")\n",
    "    print(f\"   - Caracter√≠sticas categ√≥ricas: {len(categorical_features)}: {categorical_features}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error preprocesando los datos: {e}\")\n",
    "    print(\"üí° Aseg√∫rate de que el dataset est√© cargado correctamente\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2.5 Crear preprocesador para el modelo SVM\n",
    "\n",
    "#Preprocesador para SVM\n",
    "try:\n",
    "    preprocessor_svm = predictor.create_preprocessor_svm(X_train_split)\n",
    "    \n",
    "    print(\"‚úÖ Preprocesador para SVM configurado exitosamente\")\n",
    "    X_val_features = X_train_split.shape[1]\n",
    "    print(f\"üìä Caracter√≠sticas a procesar: {X_val_features}\")\n",
    "    \n",
    "    #Mostrar estado de columnas luego del preprocesamiento.\n",
    "    predictor.inspect_transformed_columns(\n",
    "        X_original=X_train_split,\n",
    "        columns=['Partner', 'Dependents', 'Contract', 'PaymentMethod']\n",
    "    )\n",
    "    \n",
    "    # Mostrar informaci√≥n del preprocesador\n",
    "    print(\"\\nüîß Configuraci√≥n del preprocesador:\")\n",
    "    numeric_features = X_train_split.select_dtypes(include=['int64', 'float64']).columns\n",
    "    categorical_features = X_train_split.select_dtypes(include=['object']).columns                      \n",
    "                                             \n",
    "    print(f\"   - Caracter√≠sticas num√©ricas: {len(numeric_features)} : {numeric_features}\")\n",
    "    print(f\"   - Caracter√≠sticas categ√≥ricas: {len(categorical_features)}: {categorical_features}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error preprocesando los datos: {e}\")\n",
    "    print(\"üí° Aseg√∫rate de que el dataset est√© cargado correctamente\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3.1 - **Crear los modelos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.3.1 Crear los modelos\n",
    "\n",
    "# Datos a evaluar\n",
    "print(\"\\nüìä X_val : \")\n",
    "display(X_val.head(5))\n",
    "print(\"\\nüìä y_val original\")\n",
    "display(y_val.head(5))   \n",
    "\n",
    "# IMPORTANTE: Mapear TANTO y_train_split como y_val para consistencia de tipos\n",
    "print(\"\\nüìä Mapeando datos para entrenamiento y evaluaci√≥n...\")\n",
    "y_train_mapped = predictor.map_target(y_train_split)  # Mapear datos de entrenamiento\n",
    "y_val_mapped = predictor.map_target(y_val)           # Mapear datos de validaci√≥n\n",
    "\n",
    "print(\"\\nüìä y_train_mapped\")\n",
    "display(y_train_mapped.head(5))   \n",
    "print(\"\\nüìä y_val_mapped\")\n",
    "display(y_val_mapped.head(5))   \n",
    "\n",
    "models = predictor.create_models(X_train)\n",
    "\n",
    "\n",
    "# Actualizar y_val para evaluaci√≥n posterior\n",
    "y_val = y_val_mapped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3.2 - **Entrenar los modelos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar los modelos (esto autom√°ticamente usa el preprocesador optimizado para cada caso)\n",
    "#Recrear modelos con preprocessor fitted\n",
    "\n",
    "models = predictor.create_models(X_train_split)\n",
    "print(f\"‚úÖ Modelos recreados: {list(models.keys())}\")\n",
    "\n",
    "# Entrenar modelos\n",
    "print(\"\\nüéØ Entrenando modelos con datos limpios...\")\n",
    "try:\n",
    "    predictor.train_models(X_train_split, y_train_mapped)\n",
    "    print(\"‚úÖ Entrenamiento completado sin errores\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error en entrenamiento: {e}\")\n",
    "\n",
    "print(\"\\nüöÄ Paso 3: Verificar que los modelos est√°n listos para evaluaci√≥n\")\n",
    "\n",
    "# Verificar estado de los modelos\n",
    "for name, model in predictor.models.items():\n",
    "    print(f\"\\nüîç Verificando {name}:\")\n",
    "    \n",
    "    # Verificar que el pipeline tiene preprocessor fitted\n",
    "    if hasattr(model, 'named_steps') and 'preprocessor' in model.named_steps:\n",
    "        preprocessor = model.named_steps['preprocessor']\n",
    "        if hasattr(preprocessor, 'transformers_'):\n",
    "            print(f\"   ‚úÖ Preprocessor fitted correctamente\")\n",
    "        else:\n",
    "            print(f\"   ‚ùå Preprocessor no fitted\")\n",
    "    \n",
    "    # Test de predicci√≥n simple\n",
    "    try:\n",
    "        # Tomar una muestra peque√±a para test\n",
    "        sample_X = X_val.head(5)\n",
    "        sample_pred = model.predict(sample_X)\n",
    "        sample_proba = model.predict_proba(sample_X)\n",
    "        print(f\"   ‚úÖ Predicci√≥n test exitosa: {len(sample_pred)} predicciones\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error en test de predicci√≥n: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.4 - Evaluaci√≥n de modelos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.4.1 Evaluaci√≥n de modelos\n",
    "try:\n",
    "    from metrics import MetricsCalculator\n",
    "    \n",
    "    #üìä Evaluando modelos...\"\n",
    "    results = predictor.evaluate_models(X_val, y_val)\n",
    "    best_model_name, best_model = predictor.get_best_model('ROC_AUC',results)\n",
    "    predictor.generate_model_report(X_val, y_val)\n",
    "    calc = MetricsCalculator()\n",
    "    y_pred = best_model.predict(X_val)\n",
    "    y_pred_proba = best_model.predict_proba(X_val)[:, 1]\n",
    "    calc.compare_models(results)\n",
    "    detailed_report = calc.generate_detailed_report(y_val, y_pred, y_pred_proba, class_names=['No Churn', 'Churn'], model_name=best_model_name)\n",
    "    print(f\"\\nüèÜ Mejor modelo seleccionado: {best_model_name}\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è M√≥dulo metrics no disponible, evaluando modelos b√°sicamente...\")\n",
    "    results = predictor.evaluate_models(X_val, y_val)\n",
    "    best_model_name, best_model = predictor.get_best_model('ROC_AUC')\n",
    "    print(f\"\\nüèÜ Mejor modelo seleccionado: {best_model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  **5. Selecci√≥n de Modelo y Generaci√≥n de Submission para Kaggle**\n",
    "\n",
    "## Basado en tus resultados de validaci√≥n, elige el mejor modelo . Luego, re-entr√©nalo usando **todos los datos de `train.csv`** y √∫salo para hacer predicciones sobre `test.csv`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Funci√≥n para generar el archivo de submission**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicciones finales y creaci√≥n del archivo de submission\n",
    "from submission import create_submission_file\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"üìÑ Generando predicciones finales...\")\n",
    "\n",
    "# Generar timestamp unico para evitar sobrescribir archivos\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "submission_file = (\n",
    "    f\"submissions\\\\submission_grupoM_{timestamp}.csv\"\n",
    ")\n",
    "print(f\"[FILE] Archivo de submission: {submission_file}\")\n",
    "\n",
    "print(f\"üìä Datos para entrenamiento final: {len(X_train_clean):,} muestras\")\n",
    "print(f'Best Model: {best_model}')\n",
    "\n",
    "# Crear archivo de submission\n",
    "submission_df = create_submission_file(\n",
    "    final_model=best_model,\n",
    "    X_train_full=X_train_clean,  # Solo caracter√≠sticas\n",
    "    y_train_full=y_train_sync,   # Variable objetivo sincronizada.\n",
    "    X_test_full=X_test_clean, # Solo caracter√≠sticas de test\n",
    "    customer_ids=customer_ids,\n",
    "    filename=submission_file\n",
    ")\n",
    "\n",
    "# Mostrar primeras predicciones\n",
    "print(f\"\\nüìã Primeras 10 predicciones:\")\n",
    "print(submission_df.head(10))\n",
    "\n",
    "# Estad√≠sticas de las predicciones\n",
    "predictions = submission_df.iloc[:, 1].values\n",
    "print(f\"\\nüìä Estad√≠sticas de predicciones:\")\n",
    "print(f\"   - Predicciones de churn (>0.5): {np.sum(predictions > 0.5):,} ({np.mean(predictions > 0.5)*100:.1f}%)\")\n",
    "print(f\"   - Predicciones de no churn (‚â§0.5): {np.sum(predictions <= 0.5):,} ({np.mean(predictions <= 0.5)*100:.1f}%)\")\n",
    "print(f\"   - Rango: [{predictions.min():.4f}, {predictions.max():.4f}]\")\n",
    "\n",
    "print(f\"\\n‚úÖ Archivo de submission {submission_file} creado exitosamente\")\n",
    "print(f\"üéØ Listo para subir a Kaggle!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q5Q5ZRMqom4o"
   },
   "outputs": [],
   "source": [
    "# Optimizaci√≥n de hiperpar√°metros para el mejor modelo\n",
    "from models import hyperparameter_tuning\n",
    "\n",
    "print(f\"üîß Optimizando hiperpar√°metros para {best_model_name}...\")\n",
    "\n",
    "from scipy.stats import loguniform\n",
    "#Randomized (r√°pido):\n",
    "logit_dist = {\n",
    "    \"classifier__C\": loguniform(1e-3, 1e2),\n",
    "    \"classifier__solver\": [\"lbfgs\", \"liblinear\", \"saga\"],  # seg√∫n  preprocesamiento\n",
    "    \"classifier__penalty\": [\"l2\"],                         # (lbfgs/liblinear/saga)\n",
    "    \"classifier__class_weight\": [None, \"balanced\"],\n",
    "    \"classifier__max_iter\": [2000],\n",
    "}\n",
    "#Grid fino (alrededor del mejor C):\n",
    "logit_grid = {\n",
    "    \"classifier__C\": [0.05, 0.1, 0.2, 0.5, 1, 2, 5],\n",
    "    \"classifier__solver\": [\"lbfgs\", \"liblinear\"],\n",
    "    \"classifier__class_weight\": [None, \"balanced\"],\n",
    "}\n",
    "\n",
    "# Definir grillas de par√°metros seg√∫n el modelo\n",
    "if 'Logistic' in best_model_name:\n",
    "    print(f'Tuneando hiper parametros para {best_model_name}')\n",
    "    param_grid = logit_grid #or logit_dist\n",
    "elif 'KNN' in best_model_name:\n",
    "    print(f'Tuneando hiper parametros para {best_model_name}')\n",
    "    param_grid = {\n",
    "        'classifier__n_neighbors': [3, 5, 7, 9],\n",
    "        'classifier__weights': ['uniform', 'distance']\n",
    "    }\n",
    "elif 'Random' in best_model_name:\n",
    "    print(f'Tuneando hiper parametros para {best_model_name}')\n",
    "    param_grid = {\n",
    "        'classifier__n_estimators': [50, 100, 200],\n",
    "        'classifier__max_depth': [None, 10, 20],\n",
    "        'classifier__min_samples_split': [2, 5]\n",
    "    }\n",
    "else:\n",
    "    # Para Naive Bayes u otros\n",
    "    print(f'Tuneando hiper parametros para {best_model_name}')\n",
    "    param_grid = {\n",
    "        'classifier__var_smoothing': [1e-9, 1e-8, 1e-7]\n",
    "    }\n",
    "\n",
    "# Realizar b√∫squeda de hiperpar√°metros\n",
    "grid_search = hyperparameter_tuning(\n",
    "    best_model, param_grid, X_train, y_train,\n",
    "    cv=5, scoring='roc_auc'\n",
    ")\n",
    "\n",
    "print(f\"\\nüéØ Hiperpar√°metros optimizados:\")\n",
    "print(f\"   - Mejor score CV: {grid_search.best_score_:.4f}\")\n",
    "print(f\"   - Mejores par√°metros: {grid_search.best_params_}\")\n",
    "\n",
    "# Actualizar el mejor modelo con los par√°metros optimizados\n",
    "optimized_model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluar modelo optimizado\n",
    "y_pred_opt = optimized_model.predict(X_val)\n",
    "y_pred_proba_opt = optimized_model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# Calcular m√©tricas del modelo optimizado\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "opt_auc = roc_auc_score(y_val, y_pred_proba_opt)\n",
    "opt_acc = accuracy_score(y_val, y_pred_opt)\n",
    "\n",
    "print(f\"\\nüìà Mejora con optimizaci√≥n:\")\n",
    "print(f\"   - ROC AUC original: {results[best_model_name]['ROC_AUC']:.4f}\")\n",
    "print(f\"   - ROC AUC optimizado: {opt_auc:.4f}\")\n",
    "print(f\"   - Mejora: {opt_auc - results[best_model_name]['ROC_AUC']:.4f}\")\n",
    "\n",
    "# Guardar el modelo optimizado\n",
    "best_model = optimized_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  **6. Conclusiones (Opcional pero Recomendado)**\n",
    "\n",
    "## Escribe un breve resumen de tus hallazgos.\n",
    "* ## ¬øQu√© modelo funcion√≥ mejor y por qu√© crees que fue as√≠?\n",
    "* ## ¬øCu√°les fueron las caracter√≠sticas m√°s importantes o los descubrimientos m√°s interesantes del EDA?\n",
    "* ## ¬øQu√© desaf√≠os encontraron y c√≥mo los resolvieron?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß™ PRUEBA DE KERNEL - CELDA SIMPLE\n",
    "print(\"üöÄ ¬°Kernel funcionando correctamente!\")\n",
    "print(\"‚úÖ VS Code Jupyter Extension conectada\")\n",
    "\n",
    "# Test b√°sico de Python\n",
    "import sys\n",
    "print(f\"üìç Python version: {sys.version}\")\n",
    "print(f\"üìÇ Python executable: {sys.executable}\")\n",
    "\n",
    "# Test de matem√°ticas b√°sicas\n",
    "resultado = 2 + 2\n",
    "print(f\"üßÆ Test matem√°tico: 2 + 2 = {resultado}\")\n",
    "\n",
    "# Test de pandas\n",
    "try:\n",
    "    import pandas as pd\n",
    "    print(f\"üìä Pandas version: {pd.__version__}\")\n",
    "    print(\"‚úÖ Pandas importado correctamente\")\n",
    "except ImportError:\n",
    "    print(\"‚ùå Error importando pandas\")\n",
    "\n",
    "print(\"\\nüéâ ¬°Kernel listo para trabajar!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß DIAGN√ìSTICO Y CORRECCI√ìN COMPLETA DEL PROBLEMA DE COLUMNSTRANSFORMER\n",
    "# Problema identificado: Los transformers no tienen atributo 'transformers_' porque no han sido fitted\n",
    "\n",
    "print(\"üîç DIAGN√ìSTICO COMPLETO DEL PROBLEMA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. Verificar estado del predictor y modelos\n",
    "print(\"\\n1Ô∏è‚É£ Estado actual del predictor:\")\n",
    "print(f\"   - Predictor existe: {predictor is not None}\")\n",
    "if hasattr(predictor, 'models'):\n",
    "    print(f\"   - Modelos creados: {len(predictor.models) if predictor.models else 0}\")\n",
    "    if predictor.models:\n",
    "        print(f\"   - Nombres de modelos: {list(predictor.models.keys())}\")\n",
    "else:\n",
    "    print(\"   - No hay modelos en el predictor\")\n",
    "\n",
    "# 2. Verificar estado del preprocessor\n",
    "print(\"\\n2Ô∏è‚É£ Estado del preprocessor:\")\n",
    "if hasattr(predictor, 'preprocessor') and predictor.preprocessor is not None:\n",
    "    print(f\"   - Preprocessor existe: {type(predictor.preprocessor).__name__}\")\n",
    "    \n",
    "    # Verificar si est√° fitted\n",
    "    if hasattr(predictor.preprocessor, 'transformers_'):\n",
    "        print(\"   ‚úÖ Preprocessor est√° fitted (tiene transformers_)\")\n",
    "    else:\n",
    "        print(\"   ‚ùå Preprocessor NO est√° fitted (falta transformers_)\")\n",
    "        print(\"   üîß Esto causa el error en evaluate_models\")\n",
    "else:\n",
    "    print(\"   ‚ùå No hay preprocessor configurado\")\n",
    "\n",
    "# 3. Verificar datos de entrenamiento\n",
    "print(\"\\n3Ô∏è‚É£ Estado de los datos:\")\n",
    "print(f\"   - X_train_split shape: {X_train_split.shape}\")\n",
    "print(f\"   - y_train_mapped shape: {y_train_mapped.shape if hasattr(y_train_mapped, 'shape') else len(y_train_mapped)}\")\n",
    "print(f\"   - X_val shape: {X_val.shape}\")\n",
    "print(f\"   - y_val shape: {y_val.shape if hasattr(y_val, 'shape') else len(y_val)}\")\n",
    "\n",
    "print(\"\\nüîß APLICANDO CORRECCI√ìN...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# SOLUCI√ìN: Re-entrenar completamente con verificaci√≥n paso a paso\n",
    "print(\"\\nüöÄ Paso 1: Recrear preprocessor y verificar fitting\")\n",
    "\n",
    "# Crear preprocessor y fittear expl√≠citamente\n",
    "predictor.create_preprocessor(X_train_split)\n",
    "\n",
    "# Verificar que el preprocessor se cre√≥ correctamente\n",
    "if hasattr(predictor, 'preprocessor') and predictor.preprocessor is not None:\n",
    "    print(\"‚úÖ Preprocessor recreado exitosamente\")\n",
    "    \n",
    "    # Fit del preprocessor de manera expl√≠cita\n",
    "    print(\"üîß Fitting preprocessor...\")\n",
    "    predictor.preprocessor.fit(X_train_split)\n",
    "    \n",
    "    # Verificar que ahora tiene transformers_\n",
    "    if hasattr(predictor.preprocessor, 'transformers_'):\n",
    "        print(\"‚úÖ Preprocessor fitted correctamente (transformers_ disponible)\")\n",
    "    else:\n",
    "        print(\"‚ùå A√∫n hay problemas con el fitting del preprocessor\")\n",
    "else:\n",
    "    print(\"‚ùå Error recreando preprocessor\")\n",
    "\n",
    "print(\"\\nüöÄ Paso 2: Recrear y entrenar modelos\")\n",
    "\n",
    "# Recrear modelos con preprocessor fitted\n",
    "models = predictor.create_models(X_train_split)\n",
    "print(f\"‚úÖ Modelos recreados: {list(models.keys())}\")\n",
    "\n",
    "# Entrenar modelos\n",
    "print(\"\\nüéØ Entrenando modelos con datos limpios...\")\n",
    "try:\n",
    "    predictor.train_models(X_train_split, y_train_mapped)\n",
    "    print(\"‚úÖ Entrenamiento completado sin errores\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error en entrenamiento: {e}\")\n",
    "\n",
    "print(\"\\nüöÄ Paso 3: Verificar que los modelos est√°n listos para evaluaci√≥n\")\n",
    "\n",
    "# Verificar estado de los modelos\n",
    "for name, model in predictor.models.items():\n",
    "    print(f\"\\nüîç Verificando {name}:\")\n",
    "    \n",
    "    # Verificar que el pipeline tiene preprocessor fitted\n",
    "    if hasattr(model, 'named_steps') and 'preprocessor' in model.named_steps:\n",
    "        preprocessor = model.named_steps['preprocessor']\n",
    "        if hasattr(preprocessor, 'transformers_'):\n",
    "            print(f\"   ‚úÖ Preprocessor fitted correctamente\")\n",
    "        else:\n",
    "            print(f\"   ‚ùå Preprocessor no fitted\")\n",
    "    \n",
    "    # Test de predicci√≥n simple\n",
    "    try:\n",
    "        # Tomar una muestra peque√±a para test\n",
    "        sample_X = X_val.head(5)\n",
    "        sample_pred = model.predict(sample_X)\n",
    "        sample_proba = model.predict_proba(sample_X)\n",
    "        print(f\"   ‚úÖ Predicci√≥n test exitosa: {len(sample_pred)} predicciones\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error en test de predicci√≥n: {e}\")\n",
    "\n",
    "print(\"\\nüéØ CORRECCI√ìN COMPLETADA - LISTA PARA EVALUACI√ìN\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîç DEPURACI√ìN DETALLADA DEL ERROR \"A given column is not a column of the dataframe\"\n",
    "\n",
    "print(\"üîç DEPURANDO EL ERROR DE CREATE_MODELS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. Verificar las columnas disponibles\n",
    "print(\"1Ô∏è‚É£ Verificando columnas en X_train_split:\")\n",
    "print(f\"   - Shape: {X_train_split.shape}\")\n",
    "print(f\"   - Columnas: {list(X_train_split.columns)}\")\n",
    "print(f\"   - Contiene 'Churn': {'Churn' in X_train_split.columns}\")\n",
    "print(f\"   - Contiene 'customerID': {'customerID' in X_train_split.columns}\")\n",
    "\n",
    "# 2. Probar FeatureEngineer paso a paso\n",
    "print(\"\\n2Ô∏è‚É£ Probando FeatureEngineer...\")\n",
    "try:\n",
    "    from models import FeatureEngineer\n",
    "    \n",
    "    fe = FeatureEngineer()\n",
    "    print(\"   ‚úÖ FeatureEngineer instanciado\")\n",
    "    \n",
    "    # Aplicar transform\n",
    "    X_engineered = fe.transform(X_train_split)\n",
    "    print(f\"   ‚úÖ Transform exitoso - Nueva shape: {X_engineered.shape}\")\n",
    "    print(f\"   - Nuevas columnas: {set(X_engineered.columns) - set(X_train_split.columns)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Error en FeatureEngineer: {e}\")\n",
    "    print(f\"   Tipo de error: {type(e).__name__}\")\n",
    "\n",
    "# 3. Probar el m√©todo _normalize_service_values\n",
    "print(\"\\n3Ô∏è‚É£ Probando _normalize_service_values...\")\n",
    "try:\n",
    "    X_normalized = predictor._normalize_service_values(X_train_split)\n",
    "    print(f\"   ‚úÖ Normalizaci√≥n exitosa - Shape: {X_normalized.shape}\")\n",
    "    \n",
    "    # Verificar algunos cambios espec√≠ficos\n",
    "    for col in ['OnlineSecurity', 'MultipleLines']:\n",
    "        if col in X_train_split.columns:\n",
    "            original_values = set(X_train_split[col].unique())\n",
    "            normalized_values = set(X_normalized[col].unique())\n",
    "            if original_values != normalized_values:\n",
    "                print(f\"   - {col}: {original_values} ‚Üí {normalized_values}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Error en _normalize_service_values: {e}\")\n",
    "\n",
    "# 4. Probar _pick_preprocessor para cada modelo\n",
    "print(\"\\n4Ô∏è‚É£ Probando _pick_preprocessor para cada modelo...\")\n",
    "\n",
    "# Aplicar normalizaci√≥n y feature engineering primero\n",
    "try:\n",
    "    fe = FeatureEngineer()\n",
    "    X_for_schema = fe.transform(predictor._normalize_service_values(X_train_split))\n",
    "    print(f\"   ‚úÖ X_for_schema preparado - Shape: {X_for_schema.shape}\")\n",
    "    \n",
    "    models_to_test = [\"Logistic_Regression\", \"Random_Forest\", \"Naive_Bayes\", \"KNN\"]\n",
    "    \n",
    "    for model_name in models_to_test:\n",
    "        try:\n",
    "            preprocessor = predictor._pick_preprocessor(model_name, X_for_schema)\n",
    "            print(f\"   ‚úÖ {model_name}: Preprocessor creado exitosamente\")\n",
    "            \n",
    "            # Probar fit del preprocessor\n",
    "            preprocessor.fit(X_for_schema)\n",
    "            print(f\"   ‚úÖ {model_name}: Preprocessor fitted exitosamente\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå {model_name}: Error - {e}\")\n",
    "            print(f\"      Tipo de error: {type(e).__name__}\")\n",
    "            \n",
    "            # Informaci√≥n adicional para debugging\n",
    "            if \"column\" in str(e).lower():\n",
    "                print(f\"      Columnas disponibles en X_for_schema: {list(X_for_schema.columns)}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Error preparando X_for_schema: {e}\")\n",
    "\n",
    "print(\"\\nüéØ DIAGN√ìSTICO COMPLETADO\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
