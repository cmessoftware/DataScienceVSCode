{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image in a markdown cell](https://cursos.utnba.centrodeelearning.com/pluginfile.php/1/theme_space/customlogo/1738330016/Logo%20UTN%20Horizontal.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Diplomado de Ciencia de Datos y An√°lisis Avanzado**\n",
    "# **Unidad 5: Modelado Predictivo I**: Regresi√≥n y Clasificaci√≥n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Proyecto de Competencia Kaggle: Predicci√≥n de Abandono de Clientes**\n",
    "\n",
    "## **Curso:** Diplomado en Ciencia de Datos\n",
    "\n",
    "# **Nombres de los Miembros del Equipo:**\n",
    "### *   [Nombre Completo del Miembro 1]\n",
    "### *   [Nombre Completo del Miembro 2]\n",
    "### *   [Nombre Completo del Miembro 3]\n",
    "\n",
    "# **Objetivo:**\n",
    "## El objetivo de este proyecto es construir y evaluar varios modelos de clasificaci√≥n para predecir si un cliente de una compa√±√≠a de telecomunicaciones abandonar√° o no el servicio (churn). El rendimiento final del mejor modelo se medir√° en la competencia de Kaggle a trav√©s de la **m√©trica ROC AUC**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Enlace para unirse a la competencia**\n",
    "### **USE EL ENLACE PARA UNIRSE POR EQUIPO, NO DE MANERA INDIVIDUAL**\n",
    "\n",
    "https://www.kaggle.com/t/57b70c381e4d451b8ae38e164b91a2aa\n",
    "\n",
    "\n",
    "### **Por favor siga las indicaciones que se suministran en la plataforma**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. **Configuraci√≥n Inicial e Importaci√≥n de Librer√≠as**\n",
    "\n",
    "## En esta secci√≥n, importaremos todas las librer√≠as necesarias para el proyecto. Es una buena pr√°ctica tener todas las importaciones en la primera celda.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì¶ *** Importando librer√≠as...\n"
     ]
    }
   ],
   "source": [
    "    from print_utils import USE_EMOJIS, safe_print, supports_emojis\n",
    "\n",
    "    safe_print(\"\\nüì¶ *** Importando librer√≠as...\")\n",
    "\n",
    "    # Importaciones bsicas\n",
    "    import importlib\n",
    "    import pathlib\n",
    "    from datetime import datetime\n",
    "    from typing import Any, Dict, List, Tuple\n",
    "\n",
    "    import matplotlib.gridspec as gridspec\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import seaborn as sns\n",
    "    from sklearn.compose import ColumnTransformer\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.metrics import (\n",
    "        accuracy_score,\n",
    "        f1_score,\n",
    "        precision_score,\n",
    "        recall_score,\n",
    "        roc_auc_score,\n",
    "    )\n",
    "    from sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ  M√≥dulos del proyecto importados correctamente\n",
      "INFO: M√≥dulo models_stable disponible\n"
     ]
    }
   ],
   "source": [
    "# üîß Importar m√≥dulos del proyecto (si existen)\n",
    "try:\n",
    "    import data_loader\n",
    "    import dataset_splitter\n",
    "    import eda\n",
    "    import metrics\n",
    "    import models\n",
    "    import models_stable  # M√≥dulo estable sin emojis\n",
    "    import submission\n",
    "\n",
    "    print(\"‚úÖ  M√≥dulos del proyecto importados correctamente\")\n",
    "    print(\"INFO: M√≥dulo models_stable disponible\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ö†Ô∏è  Algunos m√≥dulos del proyecto no est√°n disponibles: {e}\")\n",
    "    print(\"üí°  Continuando con funcionalidad b√°sica...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (4029976911.py, line 4)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mprint(\"\\n‚öôÔ∏è  Configurando MLflow...\")\u001b[39m\n    ^\n\u001b[31mIndentationError\u001b[39m\u001b[31m:\u001b[39m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "# ========================================================================\n",
    "    # üìä CONFIGURACI√ìN DE MLFLOW\n",
    "    # ========================================================================\n",
    "    print(\"\\n‚öôÔ∏è  Configurando MLflow...\")\n",
    "\n",
    "    if MLFLOW_AVAILABLE:\n",
    "        # Configurar el directorio de tracking (Windows compatible)\n",
    "        mlflow_tracking_dir = os.path.join(os.getcwd(), \"mlruns\")\n",
    "        os.makedirs(mlflow_tracking_dir, exist_ok=True)\n",
    "\n",
    "        # Usar URI con scheme file:// para compatibilidad completa\n",
    "        tracking_uri = pathlib.Path(mlflow_tracking_dir).as_uri()\n",
    "\n",
    "        print(f\"[FOLDER] Configurando tracking URI: {tracking_uri}\")\n",
    "        mlflow.set_tracking_uri(tracking_uri)\n",
    "\n",
    "        # Configurar variables de entorno para evitar warnings del Model Registry\n",
    "        os.environ[\"MLFLOW_DISABLE_ENV_MANAGER_CONDA_WARNING\"] = \"TRUE\"\n",
    "\n",
    "        experiment_name = \"Churn_Prediction_TP5\"\n",
    "\n",
    "        # Crear o obtener el experimento con configuracin optimizada\n",
    "        try:\n",
    "            # Verificar si el experimento ya existe primero\n",
    "            existing_experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "            if existing_experiment is not None:\n",
    "                experiment_id = existing_experiment.experiment_id\n",
    "                print(\n",
    "                    f\"‚úÖ Experimento '{experiment_name}' encontrado con ID: {experiment_id}\"\n",
    "                )\n",
    "            else:\n",
    "                experiment_id = mlflow.create_experiment(experiment_name)\n",
    "                print(\n",
    "                    f\"‚úÖ Experimento '{experiment_name}' creado con ID: {experiment_id}\"\n",
    "                )\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Configuracin de experimento con limitaciones: {str(e)[:100]}...\")\n",
    "            print(\" Continuando con experimento por defecto\")\n",
    "            experiment_id = \"0\"  # Usar experimento por defecto\n",
    "\n",
    "        # Configurar experimento activo\n",
    "        try:\n",
    "            mlflow.set_experiment(experiment_name)\n",
    "            print(f\"üéØ Experimento activo: {experiment_name}\")\n",
    "            MLFLOW_TRACKING_ENABLED = True\n",
    "        except Exception as e:\n",
    "            print(f\" Usando experimento por defecto - tracking bsico disponible\")\n",
    "            MLFLOW_TRACKING_ENABLED = True  # Tracking sigue funcionando\n",
    "\n",
    "        print(\"‚úÖ MLflow configurado exitosamente\")\n",
    "\n",
    "        # Iniciar run principal para toda la ejecucion\n",
    "        main_run_name = (\n",
    "            f\"churn_prediction_run_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "        )\n",
    "        if MLFLOW_AVAILABLE and MLFLOW_TRACKING_ENABLED:\n",
    "            try:\n",
    "                mlflow.start_run(run_name=main_run_name)\n",
    "                print(\"‚úÖ Run principal iniciado: {main_run_name}\")\n",
    "                active_run = mlflow.active_run()\n",
    "                if active_run is not None:\n",
    "                    main_run_id = active_run.info.run_id\n",
    "                    print(f\" Run ID: {main_run_id}\")\n",
    "                else:\n",
    "                    print(\"‚ö†Ô∏è No se pudo obtener run activo\")\n",
    "                    main_run_id = \"fallback\"\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Error iniciando run principal: {e}\")\n",
    "                main_run_id = \"fallback\"\n",
    "\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è MLflow no disponible - usando configuracin fallback\")\n",
    "        MLFLOW_TRACKING_ENABLED = False\n",
    "        main_run_id = f\"fallback_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "        experiment_id = \"fallback\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  **1. Carga de Datos**\n",
    "Cargaremos los datasets proporcionados para la competencia: `train.csv`, `test.csv` y `sample_submission.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìÅ Carga de datos desde archivos CSV\n",
    "print(\"üìÅ 1. Cargando datos...\")\n",
    "\n",
    "try:\n",
    "    # Cargar los datasets\n",
    "    X_train = pd.read_csv('train.csv')\n",
    "    X_test = pd.read_csv('test.csv')\n",
    "    sample_submission_df = pd.read_csv('sample_submission.csv')\n",
    "    \n",
    "    print(\"‚úÖ Datos cargados exitosamente:\")\n",
    "    print(f\"üìä   - Dataset de entrenamiento: {X_train.shape}\")\n",
    "    print(f\"üß™   - Dataset de prueba: {X_test.shape}\")\n",
    "    print(f\"üìù   - Sample submission: {sample_submission_df.shape}\")\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "    print(\"üí° Aseg√∫rate de que los archivos .csv est√©n en el directorio correcto\")\n",
    "    print(\"üìÇ Archivos esperados: train.csv, test.csv, sample_submission.csv\")\n",
    "    \n",
    "    # Listar archivos CSV disponibles\n",
    "    import os\n",
    "    csv_files = [f for f in os.listdir('.') if f.endswith('.csv')]\n",
    "    print(f\"üìã Archivos CSV encontrados: {csv_files}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error inesperado: {e}\")\n",
    "\n",
    "# Mostrar informaci√≥n b√°sica de los datos\n",
    "if 'X_train' in locals():\n",
    "    print(\"\\nüìä Informaci√≥n b√°sica del dataset de entrenamiento:\")\n",
    "    print(f\"   - Forma: {X_train.shape}\")\n",
    "    print(f\"   - Columnas: {list(X_train.columns)}\")\n",
    "    \n",
    "    print(\"\\nüìã Primeras 5 filas del dataset de entrenamiento:\")\n",
    "    print(X_train.head())\n",
    "    \n",
    "    print(\"\\nüìã Primeras 5 filas del dataset de prueba:\")\n",
    "    print(X_test.head())\n",
    "    \n",
    "    print(\"\\nüìã Sample submission structure:\")\n",
    "    print(sample_submission_df.head())\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No se pudieron cargar los datos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2**. An√°lisis Exploratorio de Datos (EDA)**\n",
    "En esta fase, exploraremos el dataset de entrenamiento para entender mejor nuestros datos, encontrar patrones, identificar valores faltantes y visualizar relaciones entre las caracter√≠sticas y la variable objetivo (`Churn`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objetivo: conocer distribuci√≥n de datos, target, tipos de columnas.\n",
    "\n",
    "Variables como Contract, InternetService, PaymentMethod requieren OneHotEncoding o LabelEncoding. #TODO: Verificar.\n",
    "\n",
    "Target Churn: dataset m√°s desbalanceado (~20% churn). #Verificar el desbalanceo.\n",
    "\n",
    "## Descripci√≥n de par√°metros\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä 2. Realizando an√°lisis exploratorio...\n",
      "‚ùå Error: X_train no est√° definido. Ejecuta primero la celda de carga de datos.\n"
     ]
    }
   ],
   "source": [
    "# üìä 2. An√°lisis Exploratorio de Datos (EDA)\n",
    "print(\"üìä 2. Realizando an√°lisis exploratorio...\")\n",
    "\n",
    "# Verificar que los datos est√©n cargados\n",
    "if 'X_train' not in locals() or X_train is None:\n",
    "    print(\"‚ùå Error: X_train no est√° definido. Ejecuta primero la celda de carga de datos.\")\n",
    "else:\n",
    "    try:\n",
    "        # Reload the eda module to pick up any changes\n",
    "        import importlib\n",
    "        import sys\n",
    "        if 'eda' in sys.modules:\n",
    "            import eda\n",
    "            importlib.reload(eda)\n",
    "        else:\n",
    "            import eda\n",
    "        \n",
    "        print(\"‚ÑπÔ∏è INFORMACI√ìN GENERAL DEL DATASET\")\n",
    "        print(\"=\" * 70)\n",
    "        eda.basic_info(X_train)\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"‚ö†Ô∏è M√≥dulo 'eda' no disponible. Realizando an√°lisis b√°sico...\")\n",
    "        \n",
    "        # An√°lisis b√°sico sin el m√≥dulo eda\n",
    "        print(\"‚ÑπÔ∏è INFORMACI√ìN GENERAL DEL DATASET\")\n",
    "        print(\"=\" * 70)\n",
    "        print(f\"üìê Dimensiones: {X_train.shape}\")\n",
    "        print(f\"üìã Columnas: {list(X_train.columns)}\")\n",
    "        \n",
    "        # Verificar valores faltantes\n",
    "        missing_values = X_train.isnull().sum()\n",
    "        if missing_values.sum() == 0:\n",
    "            print(\"üîç VALORES FALTANTES:\")\n",
    "            print(\"‚úÖ No hay valores faltantes\")\n",
    "        else:\n",
    "            print(\"üîç VALORES FALTANTES:\")\n",
    "            for col, count in missing_values[missing_values > 0].items():\n",
    "                print(f\"   {col}: {count} ({count/len(X_train)*100:.1f}%)\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error en el m√≥dulo eda: {e}\")\n",
    "        print(\"Realizando an√°lisis b√°sico...\")\n",
    "        \n",
    "        # An√°lisis b√°sico como fallback\n",
    "        print(\"‚ÑπÔ∏è INFORMACI√ìN GENERAL DEL DATASET\")\n",
    "        print(\"=\" * 70)\n",
    "        print(f\"üìê Dimensiones: {X_train.shape}\")\n",
    "        print(f\"üìã Columnas: {list(X_train.columns)}\")\n",
    "        \n",
    "        # Verificar valores faltantes\n",
    "        missing_values = X_train.isnull().sum()\n",
    "        if missing_values.sum() == 0:\n",
    "            print(\"üîç VALORES FALTANTES:\")\n",
    "            print(\"‚úÖ No hay valores faltantes\")\n",
    "        else:\n",
    "            print(\"üîç VALORES FALTANTES:\")\n",
    "            for col, count in missing_values[missing_values > 0].items():\n",
    "                print(f\"   {col}: {count} ({count/len(X_train)*100:.1f}%)\")\n",
    "\n",
    "    # Distribuci√≥n de la variable objetivo\n",
    "    if 'Churn' in X_train.columns:\n",
    "        print(\"\\nüéØ üéØ DISTRIBUCI√ìN DE LA VARIABLE OBJETIVO (Churn):\")\n",
    "        y_train = X_train['Churn']\n",
    "        churn_counts = y_train.value_counts()\n",
    "        churn_pct = y_train.value_counts(normalize=True) * 100\n",
    "        \n",
    "        for value, count in churn_counts.items():\n",
    "            pct = churn_pct[value]\n",
    "            label = \"No Churn\" if value == \"No\" else \"Churn\"\n",
    "            print(f\"{label}: {count} ({pct:.1f}%)\")\n",
    "\n",
    "        # Visualizaci√≥n de la distribuci√≥n del target\n",
    "        plt.figure(figsize=(12, 4))\n",
    "\n",
    "        plt.subplot(1, 2, 1)\n",
    "        y_train.value_counts().plot(kind='bar', color=['lightblue', 'salmon'])\n",
    "        plt.title('Distribuci√≥n de Churn')\n",
    "        plt.xlabel('Churn')\n",
    "        plt.ylabel('Frecuencia')\n",
    "        plt.xticks(rotation=0)\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        y_train.value_counts(normalize=True).plot(kind='pie', autopct='%1.1f%%', colors=['lightblue', 'salmon'])\n",
    "        plt.title('Proporci√≥n de Churn')\n",
    "        plt.ylabel('')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('churn_distribution.png', dpi=150, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        print(\"‚úÖ Gr√°fico de distribuci√≥n guardado: churn_distribution.png\")\n",
    "\n",
    "        # Mostrar relaci√≥n de features num√©ricas con el target\n",
    "        print(\"\\nüîç An√°lisis de caracter√≠sticas vs Churn:\")\n",
    "        \n",
    "        # Seleccionar caracter√≠sticas para an√°lisis\n",
    "        numeric_features = ['tenure', 'MonthlyCharges', 'TotalCharges']\n",
    "        categorical_features = ['InternetService', 'Dependents', 'OnlineSecurity']\n",
    "        \n",
    "        # Verificar qu√© caracter√≠sticas est√°n disponibles\n",
    "        available_numeric = [col for col in numeric_features if col in X_train.columns]\n",
    "        available_categorical = [col for col in categorical_features if col in X_train.columns]\n",
    "        \n",
    "        if available_numeric or available_categorical:\n",
    "            features_to_plot = available_numeric + available_categorical\n",
    "            \n",
    "            if len(features_to_plot) > 0:\n",
    "                fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "                axes = axes.flatten()\n",
    "                \n",
    "                for i, col in enumerate(features_to_plot[:6]):  # M√°ximo 6 plots\n",
    "                    if i < len(axes):\n",
    "                        try:\n",
    "                            if col in available_numeric:\n",
    "                                sns.boxplot(x='Churn', y=col, data=X_train, ax=axes[i])\n",
    "                            else:\n",
    "                                # Para categ√≥ricas, usar countplot\n",
    "                                sns.countplot(x=col, hue='Churn', data=X_train, ax=axes[i])\n",
    "                                axes[i].tick_params(axis='x', rotation=45)\n",
    "                            \n",
    "                            axes[i].set_title(f'{col} vs Churn')\n",
    "                        except Exception as e:\n",
    "                            axes[i].text(0.5, 0.5, f\"Error: {col}\", \n",
    "                                       transform=axes[i].transAxes, ha='center')\n",
    "                \n",
    "                # Ocultar axes no utilizados\n",
    "                for i in range(len(features_to_plot), len(axes)):\n",
    "                    axes[i].axis('off')\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è No se encontraron caracter√≠sticas num√©ricas conocidas para an√°lisis\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Columna 'Churn' no encontrada en el dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3. Preprocesamiento de Datos**\n",
    "\n",
    "Prepararemos los datos para que puedan ser utilizados por los modelos de Machine Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 - Preprocesamiento de datos usando los modelos creados\n",
    "from models import ChurnPredictor\n",
    "\n",
    "# Inicializar el predictor\n",
    "predictor = ChurnPredictor(random_state=42)\n",
    "\n",
    "# Crear el preprocesador\n",
    "predictor.create_preprocessor(X_train)\n",
    "\n",
    "print(\"‚úÖ Preprocesador configurado exitosamente\")\n",
    "X_features = X_train.shape[1]\n",
    "print(f\"üìä Caracter√≠sticas a procesar: {X_features}\")\n",
    "\n",
    "#Mostrar estado de columnas luego del preprocesamiento.\n",
    "predictor.inspect_transformed_columns(\n",
    "    X_original=X_train,\n",
    "    columns=['Partner', 'Dependents', 'Contract', 'PaymentMethod']\n",
    ")\n",
    "\n",
    "# Mostrar informaci√≥n del preprocesador\n",
    "print(\"\\nüîß Configuraci√≥n del preprocesador:\")\n",
    "numeric_features = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_features = X_train.select_dtypes(include=['object']).columns                      \n",
    "                                         \n",
    "print(f\"   - Caracter√≠sticas num√©ricas: {len(numeric_features)} : {numeric_features}\")\n",
    "print(f\"   - Caracter√≠sticas categ√≥ricas: {len(categorical_features)}: {categorical_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **4. Modelado y Evaluaci√≥n**\n",
    "\n",
    "Ahora entrenaremos y evaluaremos los tres modelos requeridos:\n",
    "Regresi√≥n Log√≠stica , k-NN y Naive Bayes. Agrego RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para importar siempre la versi√≥n m√°s reciente de la clase ChurnPredictor\n",
    "\n",
    "import sys\n",
    "import importlib\n",
    "\n",
    "# Ruta a tu m√≥dulo (ajust√° si es necesario)\n",
    "module_name = \"models\"\n",
    "\n",
    "# Eliminar del cach√© de m√≥dulos si ya estaba cargado\n",
    "if module_name in sys.modules:\n",
    "    del sys.modules[module_name]\n",
    "\n",
    "# Importar y recargar\n",
    "import models\n",
    "importlib.reload(models)\n",
    "\n",
    "# Instanciar la clase\n",
    "predictor = models.ChurnPredictor()\n",
    "\n",
    "# 4.1 Modelado \n",
    "print(\"ü§ñ Iniciando entrenamiento de modelos...\")\n",
    "\n",
    "# Preparar datos para la divisi√≥n train/validation\n",
    "print(\"üîß Preparando datos para divisi√≥n train/validation...\")\n",
    "\n",
    "# Cargar datos originales si es necesario\n",
    "try:\n",
    "    # Separar features y target\n",
    "    y = X_train[\"Churn\"]\n",
    "    print(f\"üìä Variable objetivo extra√≠da: {y.shape}\")\n",
    "    \n",
    "    # Extraer caracter√≠sticas (X) - remover Churn y customerID\n",
    "    columns_to_drop = ['Churn']\n",
    "    if 'customerID' in X_train.columns:\n",
    "        columns_to_drop.append('customerID')\n",
    "  \n",
    "    X = X_train.drop(columns_to_drop, axis=1)\n",
    "    print(f\"üìä Caracter√≠sticas extra√≠das: {X.shape}\")\n",
    "    print(f\"üìã Columnas removidas: {columns_to_drop}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error preparando datos: {e}\")\n",
    "    print(\"üí° Aseg√∫rate de que el dataset est√© cargado correctamente\")\n",
    "\n",
    "# Dividir datos en entrenamiento y validaci√≥n interna\n",
    "print(\"\\nüîÑ Dividiendo datos en train/validation interno...\")\n",
    "X_train_split, X_val, y_train_split, y_val = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2 Crear el preprocesador para datos de validaci√≥n.\n",
    "try:\n",
    "    # Crear los modelos primero (incluyen preprocesador en pipeline)\n",
    "    models = predictor.create_models()\n",
    "    \n",
    "    print(\"‚úÖ Modelos con preprocesador configurados exitosamente\")\n",
    "    print(f\"üìä Modelos creados: {list(models.keys())}\")\n",
    "    \n",
    "    # Verificar estructura de uno de los modelos\n",
    "    sample_model = list(models.values())[0]\n",
    "    print(f\"üîß Estructura del pipeline: {list(sample_model.named_steps.keys())}\")\n",
    "    \n",
    "    X_val_features = X_val.shape[1]\n",
    "    print(f\"üìä Caracter√≠sticas a procesar: {X_val_features}\")\n",
    "    \n",
    "    # Mostrar informaci√≥n del preprocesador\n",
    "    print(\"\\nüîß Informaci√≥n de datos:\")\n",
    "    numeric_features = X_val.select_dtypes(include=['int64', 'float64']).columns\n",
    "    categorical_features = X_val.select_dtypes(include=['object']).columns                      \n",
    "                                             \n",
    "    print(f\"   - Caracter√≠sticas num√©ricas: {len(numeric_features)} : {list(numeric_features)}\")\n",
    "    print(f\"   - Caracter√≠sticas categ√≥ricas: {len(categorical_features)}: {list(categorical_features)}\")\n",
    "    \n",
    "    # Verificar que no hay columnas categ√≥ricas problem√°ticas\n",
    "    print(\"\\nüîç Verificando datos categ√≥ricos:\")\n",
    "    for col in categorical_features:\n",
    "        unique_vals = X_val[col].unique()\n",
    "        print(f\"   - {col}: {unique_vals[:5]}{'...' if len(unique_vals) > 5 else ''}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error configurando modelos: {e}\")\n",
    "    print(\"üí° Aseg√∫rate de que el dataset est√© cargado correctamente\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.3 Crear y entrenar los modelos\n",
    "print(\"üìä X_val : \")\n",
    "display(X_val.head())\n",
    "\n",
    "print(\"üìä y_val original\")\n",
    "print(y_val.head())\n",
    "\n",
    "# ‚úÖ FIXED: Map the correct split data, not the full training data\n",
    "print(\"üìä Mapeando datos para entrenamiento y evaluaci√≥n...\")\n",
    "label_mapping = {\"No\": 0, \"Yes\": 1}\n",
    "\n",
    "# Map the SPLIT training data (not the full y_train)\n",
    "print(f\"üîç y_train_split valores √∫nicos antes del mapeo: {y_train_split.unique()}\")\n",
    "y_train_split_mapped = y_train_split.map(label_mapping)\n",
    "print(f\"‚úÖ y_train_split_mapped valores √∫nicos despu√©s del mapeo: {y_train_split_mapped.unique()}\")\n",
    "\n",
    "print(f\"üîç y_val valores √∫nicos antes del mapeo: {y_val.unique()}\")\n",
    "y_val_mapped = y_val.map(label_mapping)\n",
    "print(f\"‚úÖ y_val_mapped valores √∫nicos despu√©s del mapeo: {y_val_mapped.unique()}\")\n",
    "\n",
    "print(f\"\\nüìä Verificando shapes:\")\n",
    "print(f\"   X_train_split: {X_train_split.shape}\")\n",
    "print(f\"   y_train_split_mapped: {y_train_split_mapped.shape}\")\n",
    "print(f\"   X_val: {X_val.shape}\")\n",
    "print(f\"   y_val_mapped: {y_val_mapped.shape}\")\n",
    "\n",
    "# ‚úÖ FIXED WORKFLOW: Create preprocessor BEFORE creating models\n",
    "print(f\"\\nüîß Setting up preprocessor first...\")\n",
    "if predictor.preprocessor is None:\n",
    "    preprocessor = predictor.create_preprocessor(X_train_split)\n",
    "    print(f\"‚úÖ Preprocessor created: {type(preprocessor)}\")\n",
    "\n",
    "# Create models with proper preprocessor\n",
    "print(f\"\\nüîß Creating models with preprocessor...\")\n",
    "models = predictor.create_models()\n",
    "print(f\"‚úÖ Models created with proper preprocessing pipeline\")\n",
    "\n",
    "# ‚úÖ FIXED: Use matching datasets - X_train_split with y_train_split_mapped\n",
    "print(f\"\\nüéØ Entrenando modelos con datos mapeados y shapes consistentes...\")\n",
    "predictor.train_models(X_train_split, y_train_split_mapped)\n",
    "\n",
    "print(\"\\nüéâ Entrenamiento completado para todos los modelos:\")\n",
    "for name in predictor.models.keys():\n",
    "    print(f\"   ‚úÖ {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß FIXING THE PREPROCESSING ISSUE\n",
    "# The problem is that create_models() is called before the preprocessor is set up\n",
    "# Let's check if we need to create the preprocessor first\n",
    "\n",
    "print(\"üîç Checking current predictor state:\")\n",
    "print(f\"   - Preprocessor: {type(predictor.preprocessor)}\")\n",
    "print(f\"   - Models created: {len(predictor.models)} models\")\n",
    "\n",
    "# If preprocessor is None, we need to create it first\n",
    "if predictor.preprocessor is None:\n",
    "    print(\"\\n‚ö†Ô∏è  Preprocessor is None - creating it first...\")\n",
    "    preprocessor = predictor.create_preprocessor(X_train_split)\n",
    "    print(f\"‚úÖ Preprocessor created: {type(preprocessor)}\")\n",
    "\n",
    "# Now create models with the proper preprocessor\n",
    "if len(predictor.models) == 0:\n",
    "    print(\"\\nüîß Creating models with preprocessor...\")\n",
    "    models = predictor.create_models()\n",
    "    print(f\"‚úÖ Models created: {list(models.keys())}\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ Models already exist\")\n",
    "\n",
    "# Final check that models have proper preprocessors\n",
    "sample_model = list(predictor.models.values())[0]\n",
    "print(f\"\\nüîç Sample model pipeline steps:\")\n",
    "for step_name, step_obj in sample_model.steps:\n",
    "    print(f\"   - {step_name}: {type(step_obj)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß RECREATE MODELS WITH PROPER PREPROCESSOR\n",
    "# Since the existing models were created with preprocessor=None, we need to recreate them\n",
    "\n",
    "print(\"üîß Recreating models with proper preprocessor...\")\n",
    "\n",
    "# Recreate the models now that preprocessor is set up\n",
    "models = predictor.create_models()\n",
    "print(f\"‚úÖ Models recreated: {list(models.keys())}\")\n",
    "\n",
    "# Verify the models now have proper preprocessors\n",
    "sample_model = list(predictor.models.values())[0]\n",
    "print(f\"\\nüîç Sample model pipeline steps (after recreation):\")\n",
    "for step_name, step_obj in sample_model.steps:\n",
    "    print(f\"   - {step_name}: {type(step_obj)}\")\n",
    "\n",
    "print(f\"\\nüéØ Ready to train models!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß™ TEST MODEL TRAINING WITH FIXED PREPROCESSOR\n",
    "print(\"üß™ Testing model training with fixed preprocessing pipeline...\")\n",
    "\n",
    "# Test with a small sample first to verify everything works\n",
    "print(f\"üìä Training data shape: {X_train_split.shape}\")\n",
    "print(f\"üìä Target data shape: {y_train_split.shape}\")\n",
    "\n",
    "# Try training the models\n",
    "try:\n",
    "    predictor.train_models(X_train_split, y_train_split)\n",
    "    print(\"\\nüéâ SUCCESS! Model training completed without errors.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Error during training: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîç DEBUG DATA SHAPE INCONSISTENCY \n",
    "print(\"üîç Debugging data shape inconsistency...\")\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_train_split shape: {X_train_split.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_train_split shape: {y_train_split.shape}\")\n",
    "print(f\"y_train_mapped shape: {y_train_split.shape}\")\n",
    "\n",
    "print(f\"\\nüîç The issue: X_train_split has {X_train_split.shape[0]} samples\")\n",
    "print(f\"              but y_train_mapped has {y_train_split.shape[0]} samples\")\n",
    "print(f\"              We need to use y_train_split instead!\")\n",
    "\n",
    "# Map the correct y_train_split data\n",
    "print(f\"\\nüîß Mapping y_train_split instead of y_train...\")\n",
    "label_mapping = {\"No\": 0, \"Yes\": 1}\n",
    "y_train_split_mapped = y_train_split.map(label_mapping)\n",
    "print(f\"‚úÖ y_train_split_mapped shape: {y_train_split_mapped.shape}\")\n",
    "\n",
    "print(f\"\\nNow the shapes match:\")\n",
    "print(f\"   X_train_split: {X_train_split.shape}\")\n",
    "print(f\"   y_train_split_mapped: {y_train_split_mapped.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîç DIAGN√ìSTICO CONCISO DEL PROBLEMA\n",
    "print(\"üîç DIAGN√ìSTICO R√ÅPIDO\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Verificar datos categ√≥ricos\n",
    "categorical_cols = X_train_split.select_dtypes(include=['object']).columns\n",
    "print(f\"üìä Columnas categ√≥ricas: {len(categorical_cols)}\")\n",
    "if len(categorical_cols) > 0:\n",
    "    print(f\"   Columnas: {list(categorical_cols)}\")\n",
    "    for col in categorical_cols[:3]:  # Solo las primeras 3\n",
    "        sample_vals = X_train_split[col].head(2).tolist()\n",
    "        print(f\"   {col}: {sample_vals}\")\n",
    "\n",
    "# Verificar estructura del modelo\n",
    "sample_model = list(models.values())[0]\n",
    "print(f\"\\nü§ñ Modelo tipo: {type(sample_model)}\")\n",
    "if hasattr(sample_model, 'named_steps'):\n",
    "    steps = list(sample_model.named_steps.keys())\n",
    "    print(f\"   Pipeline steps: {steps}\")\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è No es un pipeline!\")\n",
    "\n",
    "# Test r√°pido del preprocesador\n",
    "print(f\"\\nüß™ Test del pipeline completo:\")\n",
    "try:\n",
    "    X_sample = X_train_split.head(1)\n",
    "    y_sample = y_train_split.head(1) if hasattr(y_train_split, 'head') else [y_train_split[0]]\n",
    "    \n",
    "    # Intentar fit con una muestra peque√±a\n",
    "    sample_model.fit(X_sample, y_sample)\n",
    "    print(\"   ‚úÖ Pipeline funciona correctamente\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Error: {str(e)[:100]}...\")\n",
    "    \n",
    "    # Verificar si el problema est√° en el preprocesador\n",
    "    if hasattr(sample_model, 'named_steps') and 'preprocessor' in sample_model.named_steps:\n",
    "        try:\n",
    "            preprocessor = sample_model.named_steps['preprocessor']\n",
    "            X_transformed = preprocessor.fit_transform(X_sample)\n",
    "            print(f\"   üìä Preprocesador OK: {X_transformed.shape}\")\n",
    "            print(f\"   ‚ùå Error en el clasificador: {str(e)[:50]}...\")\n",
    "        except Exception as prep_error:\n",
    "            print(f\"   ‚ùå Error en preprocesador: {str(prep_error)[:50]}...\")\n",
    "\n",
    "print(\"=\"*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.4 Evaluaci√≥n de modelos\n",
    "try:\n",
    "    from metrics import MetricsCalculator\n",
    "    \n",
    "    # ‚úÖ FIXED: Use y_val_mapped instead of y_val for consistency\n",
    "    print(\"üìä Evaluando modelos con datos mapeados...\")\n",
    "    results = predictor.evaluate_models(X_val, y_val_mapped)\n",
    "    best_model_name, best_model = predictor.get_best_model('ROC_AUC', results)\n",
    "    \n",
    "    # ‚úÖ FIXED: Use y_val_mapped for model report generation too\n",
    "    print(\"üìà Generando reporte de modelos...\")\n",
    "    predictor.generate_model_report(X_val, y_val_mapped)\n",
    "    \n",
    "    # Calculate detailed metrics\n",
    "    calc = MetricsCalculator()\n",
    "    y_pred = best_model.predict(X_val)\n",
    "    y_pred_proba = best_model.predict_proba(X_val)[:, 1]\n",
    "    detailed_report = calc.generate_detailed_report(\n",
    "        y_val_mapped, y_pred, y_pred_proba, \n",
    "        class_names=['No Churn', 'Churn'], \n",
    "        model_name=best_model_name\n",
    "    )\n",
    "    print(f\"\\nüèÜ Mejor modelo seleccionado: {best_model_name}\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è M√≥dulo metrics no disponible, evaluando modelos b√°sicamente...\")\n",
    "    \n",
    "    # ‚úÖ FIXED: Use y_val_mapped for basic evaluation too\n",
    "    results = predictor.evaluate_models(X_val, y_val_mapped)\n",
    "    best_model_name, best_model = predictor.get_best_model('ROC_AUC', results)\n",
    "    print(f\"\\nüèÜ Mejor modelo seleccionado: {best_model_name}\")\n",
    "    \n",
    "    # Show basic results\n",
    "    print(f\"\\nüìä Resultados de evaluaci√≥n:\")\n",
    "    for model_name, metrics in results.items():\n",
    "        print(f\"   {model_name}:\")\n",
    "        for metric_name, value in metrics.items():\n",
    "            print(f\"      - {metric_name}: {value:.4f}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîç DIAGN√ìSTICO: Feature Mismatch Issue\n",
    "print(\"üîç Diagnosticando el problema de feature mismatch...\")\n",
    "\n",
    "# Check data dimensions\n",
    "print(f\"\\nüìä Dimensiones de datos:\")\n",
    "print(f\"   X_train_split (usado para entrenamiento): {X_train_split.shape}\")\n",
    "print(f\"   X_val (usado para evaluaci√≥n): {X_val.shape}\")\n",
    "\n",
    "# Check if they have the same columns\n",
    "print(f\"\\nüìã Comparaci√≥n de columnas:\")\n",
    "if set(X_train_split.columns) == set(X_val.columns):\n",
    "    print(\"   ‚úÖ X_train_split y X_val tienen las mismas columnas\")\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è PROBLEMA: X_train_split y X_val tienen columnas diferentes!\")\n",
    "    \n",
    "    train_cols = set(X_train_split.columns)\n",
    "    val_cols = set(X_val.columns)\n",
    "    \n",
    "    missing_in_val = train_cols - val_cols\n",
    "    extra_in_val = val_cols - train_cols\n",
    "    \n",
    "    if missing_in_val:\n",
    "        print(f\"   Columnas en train pero NO en val: {missing_in_val}\")\n",
    "    if extra_in_val:\n",
    "        print(f\"   Columnas en val pero NO en train: {extra_in_val}\")\n",
    "\n",
    "# Check what the model expects vs what we're giving it\n",
    "sample_model = list(predictor.models.values())[0]\n",
    "print(f\"\\nü§ñ Informaci√≥n del modelo:\")\n",
    "print(f\"   Tipo: {type(sample_model)}\")\n",
    "\n",
    "# Check if model has been fitted and what it expects\n",
    "if hasattr(sample_model, 'steps'):\n",
    "    classifier = sample_model.steps[-1][1]\n",
    "    if hasattr(classifier, 'n_features_in_'):\n",
    "        print(f\"   Modelo espera: {classifier.n_features_in_} features\")\n",
    "        print(f\"   X_val tiene: {X_val.shape[1]} features\")\n",
    "        print(f\"   Diferencia: {classifier.n_features_in_ - X_val.shape[1]} features faltantes\")\n",
    "\n",
    "# Check preprocessor output\n",
    "print(f\"\\nüîß Verificando preprocesador:\")\n",
    "if predictor.preprocessor is not None:\n",
    "    print(f\"   Preprocesador configurado: {type(predictor.preprocessor)}\")\n",
    "    \n",
    "    # Test preprocessor with a small sample\n",
    "    try:\n",
    "        X_val_sample = X_val.head(1)\n",
    "        X_transformed_sample = predictor.preprocessor.transform(X_val_sample)\n",
    "        print(f\"   X_val original: {X_val_sample.shape}\")\n",
    "        print(f\"   Despu√©s del preprocesador: {X_transformed_sample.shape}\")\n",
    "        \n",
    "        if X_transformed_sample.shape[1] != classifier.n_features_in_:\n",
    "            print(f\"   ‚ö†Ô∏è PROBLEMA: Preprocesador produce {X_transformed_sample.shape[1]} features\")\n",
    "            print(f\"               pero modelo espera {classifier.n_features_in_} features\")\n",
    "        else:\n",
    "            print(f\"   ‚úÖ Preprocesador produce el n√∫mero correcto de features\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error en preprocesador: {e}\")\n",
    "else:\n",
    "    print(\"   ‚ùå No hay preprocesador configurado!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß SOLUCI√ìN: Fix Preprocessor Mismatch\n",
    "print(\"üîß Solucionando el problema de preprocessor mismatch...\")\n",
    "\n",
    "# The issue is that the model pipeline has its own preprocessor\n",
    "# that was trained during the fit process, but we're trying to use\n",
    "# a separate preprocessor instance\n",
    "\n",
    "print(\"\\nüîç Analizando pipeline del modelo entrenado...\")\n",
    "sample_model = list(predictor.models.values())[0]\n",
    "\n",
    "# Extract the fitted preprocessor from the model pipeline\n",
    "if hasattr(sample_model, 'steps') and len(sample_model.steps) >= 2:\n",
    "    pipeline_preprocessor = sample_model.steps[0][1]  # First step should be preprocessor\n",
    "    pipeline_classifier = sample_model.steps[1][1]   # Second step should be classifier\n",
    "    \n",
    "    print(f\"   Pipeline step 0 (preprocessor): {type(pipeline_preprocessor)}\")\n",
    "    print(f\"   Pipeline step 1 (classifier): {type(pipeline_classifier)}\")\n",
    "    \n",
    "    # Test the pipeline preprocessor\n",
    "    try:\n",
    "        X_val_sample = X_val.head(1)\n",
    "        X_transformed_pipeline = pipeline_preprocessor.transform(X_val_sample)\n",
    "        print(f\"   ‚úÖ Pipeline preprocessor produce: {X_transformed_pipeline.shape[1]} features\")\n",
    "        \n",
    "        if X_transformed_pipeline.shape[1] == pipeline_classifier.n_features_in_:\n",
    "            print(f\"   ‚úÖ CORRECTO: Pipeline preprocessor match con classifier\")\n",
    "            print(f\"   üìä El problema era usar el preprocessor externo en lugar del pipeline interno\")\n",
    "        else:\n",
    "            print(f\"   ‚ö†Ô∏è A√∫n hay mismatch en el pipeline interno\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error con pipeline preprocessor: {e}\")\n",
    "\n",
    "# The solution is simple: the models already have the correct preprocessor built-in\n",
    "# We just need to call model.predict(X_val) directly without separate preprocessing\n",
    "print(f\"\\nüí° SOLUCI√ìN:\")\n",
    "print(f\"   - Los modelos de Pipeline ya incluyen el preprocessor correcto\")\n",
    "print(f\"   - NO debemos usar predictor.preprocessor por separado\")\n",
    "print(f\"   - Llamar directamente model.predict(X_val) y model.predict_proba(X_val)\")\n",
    "\n",
    "# Test the solution\n",
    "print(f\"\\nüß™ Probando la soluci√≥n:\")\n",
    "try:\n",
    "    test_model = list(predictor.models.values())[0]\n",
    "    y_pred_test = test_model.predict(X_val.head(1))\n",
    "    y_pred_proba_test = test_model.predict_proba(X_val.head(1))\n",
    "    \n",
    "    print(f\"   ‚úÖ SUCCESS: model.predict() funciona correctamente\")\n",
    "    print(f\"   Predicci√≥n: {y_pred_test}\")\n",
    "    print(f\"   Probabilidades: {y_pred_proba_test}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå ERROR: {e}\")\n",
    "\n",
    "print(\"\\nüéØ Ahora podemos proceder con la evaluaci√≥n correcta\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß SOLUCI√ìN DEFINITIVA: Retrain models with consistent preprocessing\n",
    "print(\"üîß El problema requiere reentrenamiento con preprocessing consistente...\")\n",
    "\n",
    "# The issue is that there was an inconsistency during training\n",
    "# Let's retrain the models properly with a fresh start\n",
    "\n",
    "print(\"\\nüîÑ Reinicializando el pipeline completo...\")\n",
    "\n",
    "# 1. Import the models module properly and create a fresh predictor instance\n",
    "import importlib\n",
    "import models as models_module\n",
    "importlib.reload(models_module)\n",
    "\n",
    "predictor_fixed = models_module.ChurnPredictor(random_state=42)\n",
    "\n",
    "# 2. Create preprocessor with training data\n",
    "print(\"üìä Creando preprocessor con datos de entrenamiento...\")\n",
    "preprocessor_fixed = predictor_fixed.create_preprocessor(X_train_split)\n",
    "\n",
    "# 3. Fit the preprocessor with training data\n",
    "print(\"üîß Ajustando preprocessor con datos de entrenamiento...\")\n",
    "preprocessor_fixed.fit(X_train_split)\n",
    "\n",
    "# 4. Verify preprocessor output shapes\n",
    "print(f\"\\nüîç Verificando preprocessor arreglado:\")\n",
    "X_train_sample = X_train_split.head(1)\n",
    "X_val_sample = X_val.head(1)\n",
    "\n",
    "X_train_transformed = preprocessor_fixed.transform(X_train_sample)\n",
    "X_val_transformed = preprocessor_fixed.transform(X_val_sample)\n",
    "\n",
    "print(f\"   X_train_split original: {X_train_sample.shape}\")\n",
    "print(f\"   X_train_split transformado: {X_train_transformed.shape}\")\n",
    "print(f\"   X_val original: {X_val_sample.shape}\")\n",
    "print(f\"   X_val transformado: {X_val_transformed.shape}\")\n",
    "\n",
    "if X_train_transformed.shape[1] == X_val_transformed.shape[1]:\n",
    "    print(f\"   ‚úÖ Shapes consistentes: {X_train_transformed.shape[1]} features\")\n",
    "    \n",
    "    # 5. Create models with the fixed preprocessor\n",
    "    print(f\"\\nü§ñ Creando modelos con preprocessor arreglado...\")\n",
    "    models_fixed = predictor_fixed.create_models()\n",
    "    \n",
    "    # 6. Retrain with the same data that was used before\n",
    "    print(f\"\\nüéØ Reentrenando modelos con datos consistentes...\")\n",
    "    predictor_fixed.train_models(X_train_split, y_train_split_mapped)\n",
    "    \n",
    "    # 7. Test the fixed models\n",
    "    print(f\"\\nüß™ Probando modelos arreglados...\")\n",
    "    test_model_fixed = list(predictor_fixed.models.values())[0]\n",
    "    \n",
    "    y_pred_test_fixed = test_model_fixed.predict(X_val.head(1))\n",
    "    y_pred_proba_test_fixed = test_model_fixed.predict_proba(X_val.head(1))\n",
    "    \n",
    "    print(f\"   ‚úÖ SUCCESS: Modelos reentrenados funcionan correctamente\")\n",
    "    print(f\"   Predicci√≥n: {y_pred_test_fixed}\")\n",
    "    print(f\"   Probabilidades shape: {y_pred_proba_test_fixed.shape}\")\n",
    "    \n",
    "    # 8. Replace the original predictor with the fixed one\n",
    "    print(f\"\\nüîÑ Reemplazando predictor original con versi√≥n arreglada...\")\n",
    "    predictor = predictor_fixed\n",
    "    models = predictor.models  # Update models dict as well\n",
    "    \n",
    "    print(f\"   ‚úÖ Predictor actualizado exitosamente\")\n",
    "    print(f\"   üìä Modelos disponibles: {list(models.keys())}\")\n",
    "    print(f\"\\nüéâ PROBLEMA RESUELTO: Ahora podemos proceder con la evaluaci√≥n\")\n",
    "    \n",
    "else:\n",
    "    print(f\"   ‚ùå A√∫n hay inconsistencia en shapes\")\n",
    "    print(f\"   Train: {X_train_transformed.shape[1]}, Val: {X_val_transformed.shape[1]}\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  **5. Selecci√≥n de Modelo y Generaci√≥n de Submission para Kaggle**\n",
    "\n",
    "## Basado en tus resultados de validaci√≥n, elige el mejor modelo . Luego, re-entr√©nalo usando **todos los datos de `train.csv`** y √∫salo para hacer predicciones sobre `test.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß CONFIGURACI√ìN Y ENTRENAMIENTO DEL CHURNPREDICTOR\n",
    "\n",
    "from models import ChurnPredictor\n",
    "\n",
    "print(\"üöÄ Inicializando ChurnPredictor...\")\n",
    "\n",
    "# Usar los datos ORIGINALES (antes del preprocesamiento) para crear el preprocesador\n",
    "predictor = ChurnPredictor(random_state=42)\n",
    "\n",
    "# Verificar y limpiar datos antes del preprocesamiento\n",
    "print(\"üîç Verificando datos antes del preprocesamiento...\")\n",
    "\n",
    "# Asumir que X_train son los datos originales (con customerID y sin procesar)\n",
    "# Necesitamos remover customerID de X_train si existe\n",
    "if 'customerID' in X_train.columns:\n",
    "    X_train_clean = X_train.drop(['customerID'], axis=1)\n",
    "else:\n",
    "    X_train_clean = X_train.copy()\n",
    "\n",
    "#customerIDs lo guardo para usarn en la generaci√≥n del archivo de submit.\n",
    "customer_ids = X_test['customerID']\n",
    "\n",
    "# Necesitamos remover customerID de X_test si existe\n",
    "if 'customerID' in X_test.columns:\n",
    "    X_test_clean = X_test.drop(['customerID'], axis=1)\n",
    "else:\n",
    "    X_test_clean = X_test.copy()\n",
    "\n",
    "\n",
    "# SINCRONIZACI√ìN FINAL: Asegurar que X e y tengan el mismo n√∫mero de muestras\n",
    "if X_train_clean.shape[0] != y_train.shape[0]:\n",
    "    print(f\"‚ö†Ô∏è Sincronizando datos finales:\")\n",
    "    print(f\"   - X_train_clean: {X_train_clean.shape[0]} ‚Üí \", end=\"\")\n",
    "    print(f\"   - y_train: {y_train.shape[0]} ‚Üí \", end=\"\")\n",
    "    \n",
    "    min_samples = min(X_train_clean.shape[0], y_train.shape[0])\n",
    "    X_train_clean = X_train_clean.iloc[:min_samples]\n",
    "    y_train_sync = y_train.iloc[:min_samples] if hasattr(y_train, 'iloc') else y_train[:min_samples]\n",
    "    \n",
    "    print(f\"Sincronizados a {min_samples} muestras\")\n",
    "else:\n",
    "    y_train_sync = y_train\n",
    "    print(\"‚úÖ Datos ya est√°n sincronizados\")\n",
    "\n",
    "# IMPORTANTE: Mapear y_train_sync para consistencia de tipos\n",
    "print(f\"\\nüîß Mapeando y_train_sync a formato num√©rico...\")\n",
    "y_train_sync = predictor.map_target(y_train_sync)\n",
    "\n",
    "# Crear el preprocesador con los datos originales\n",
    "preprocessor = predictor.create_preprocessor(X_train_clean)\n",
    "\n",
    "print(\"‚úÖ Preprocesador configurado exitosamente\")\n",
    "print(f\"üìä Caracter√≠sticas procesadas: {X_train_clean.shape[1]}\")\n",
    "print(f\"üìä Muestras para entrenamiento: {X_train_clean.shape[0]}\")\n",
    "\n",
    "# Crear los modelos (esto autom√°ticamente usa el preprocesador)\n",
    "models = predictor.create_models()\n",
    "\n",
    "# ENTRENAR con los datos sincronizados\n",
    "print(\"\\nüéØ Iniciando entrenamiento con datos sincronizados...\")\n",
    "predictor.train_models(X_train_clean, y_train_sync)\n",
    "\n",
    "print(\"\\nüéâ Entrenamiento completado para todos los modelos:\")\n",
    "for model_name in models.keys():\n",
    "    print(f\"   ‚úÖ {model_name}\")\n",
    "\n",
    "print(f\"\\nüìä Resumen del entrenamiento:\")\n",
    "print(f\"   - Datos de entrenamiento: {X_train_clean.shape}\")\n",
    "print(f\"   - Etiquetas: {y_train_sync.shape if hasattr(y_train_sync, 'shape') else len(y_train_sync)}\")\n",
    "print(f\"   - Modelos entrenados: {len(models)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Funci√≥n para generar el archivo de submission**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicciones finales y creaci√≥n del archivo de submission\n",
    "from submission import create_submission_file\n",
    "\n",
    "print(\"üìÑ Generando predicciones finales...\")\n",
    "\n",
    "\n",
    "print(f\"üìä Datos para entrenamiento final: {len(X_train_clean):,} muestras\")\n",
    "\n",
    "# Crear archivo de submission\n",
    "submission_df = create_submission_file(\n",
    "    final_model=best_model,\n",
    "    X_train_full=X_train_clean,  # Solo caracter√≠sticas\n",
    "    y_train_full=y_train_sync,   # Variable objetivo sincronizada.\n",
    "    X_test_full=X_test_clean, # Solo caracter√≠sticas de test\n",
    "    customer_ids=customer_ids,\n",
    "    filename=\"submission_grupoM_inicial.csv\"\n",
    ")\n",
    "\n",
    "# Mostrar primeras predicciones\n",
    "print(f\"\\nüìã Primeras 10 predicciones:\")\n",
    "print(submission_df.head(10))\n",
    "\n",
    "# Estad√≠sticas de las predicciones\n",
    "predictions = submission_df.iloc[:, 1].values\n",
    "print(f\"\\nüìä Estad√≠sticas de predicciones:\")\n",
    "print(f\"   - Predicciones de churn (>0.5): {np.sum(predictions > 0.5):,} ({np.mean(predictions > 0.5)*100:.1f}%)\")\n",
    "print(f\"   - Predicciones de no churn (‚â§0.5): {np.sum(predictions <= 0.5):,} ({np.mean(predictions <= 0.5)*100:.1f}%)\")\n",
    "print(f\"   - Rango: [{predictions.min():.4f}, {predictions.max():.4f}]\")\n",
    "\n",
    "print(f\"\\n‚úÖ Archivo de submission 'submission_grupoM.csv' creado exitosamente\")\n",
    "print(f\"üéØ Listo para subir a Kaggle!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß Optimizaci√≥n de hiperpar√°metros para el mejor modelo\n",
    "from models import hyperparameter_tuning\n",
    "\n",
    "print(f\"üîß Optimizando hiperpar√°metros para {best_model_name}...\")\n",
    "\n",
    "# Definir grillas de par√°metros seg√∫n el modelo\n",
    "if 'Logistic' in best_model_name:\n",
    "    param_grid = {\n",
    "        'classifier__C': [0.1, 1.0, 10.0],\n",
    "        'classifier__solver': ['liblinear', 'lbfgs']\n",
    "    }\n",
    "elif 'KNN' in best_model_name:\n",
    "    param_grid = {\n",
    "        'classifier__n_neighbors': [3, 5, 7, 9],\n",
    "        'classifier__weights': ['uniform', 'distance']\n",
    "    }\n",
    "elif 'Random' in best_model_name:\n",
    "    param_grid = {\n",
    "        'classifier__n_estimators': [50, 100, 200],\n",
    "        'classifier__max_depth': [None, 10, 20],\n",
    "        'classifier__min_samples_split': [2, 5]\n",
    "    }\n",
    "else:\n",
    "    # Para Naive Bayes u otros\n",
    "    param_grid = {\n",
    "        'classifier__var_smoothing': [1e-9, 1e-8, 1e-7]\n",
    "    }\n",
    "\n",
    "# Realizar b√∫squeda de hiperpar√°metros\n",
    "grid_search = hyperparameter_tuning(\n",
    "    best_model, param_grid, X_train_clean, y_train_sync,\n",
    "    cv=5, scoring='roc_auc'\n",
    ")\n",
    "\n",
    "print(f\"\\nüéØ Hiperpar√°metros optimizados:\")\n",
    "print(f\"   - Mejor score CV: {grid_search.best_score_:.4f}\")\n",
    "print(f\"   - Mejores par√°metros: {grid_search.best_params_}\")\n",
    "\n",
    "# Actualizar el mejor modelo con los par√°metros optimizados\n",
    "optimized_model = grid_search.best_estimator_\n",
    "\n",
    "print(\"\\nü§ñ Generando predicciones del modelo optimizado...\")\n",
    "\n",
    "# Obtener predicciones del modelo optimizado\n",
    "y_pred_opt = optimized_model.predict(X_val)\n",
    "y_pred_proba_opt = optimized_model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "print(\"‚úÖ Predicciones generadas correctamente\")\n",
    "print(\"üìã Las m√©tricas se calcular√°n en las siguientes celdas despu√©s de la correcci√≥n de tipos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß CORRECCI√ìN: Resolver inconsistencia de tipos entre y_val y y_pred_opt\n",
    "\n",
    "print(\"üîç Diagnosticando tipos de datos...\")\n",
    "print(f\"Tipo de y_val: {type(y_val)}\")\n",
    "print(f\"Valores √∫nicos en y_val: {np.unique(y_val)}\")\n",
    "print(f\"Tipo de y_pred_opt: {type(y_pred_opt)}\")\n",
    "print(f\"Valores √∫nicos en y_pred_opt: {np.unique(y_pred_opt)}\")\n",
    "\n",
    "# Verificar si las predicciones son de tipo string\n",
    "if y_pred_opt.dtype == 'object' or isinstance(y_pred_opt[0], str):\n",
    "    print(\"‚ö†Ô∏è Detectadas predicciones en formato texto, convirtiendo a num√©rico...\")\n",
    "    \n",
    "    # Mapear predicciones de texto a n√∫meros\n",
    "    y_pred_opt_numeric = np.where(y_pred_opt == 'Yes', 1, 0)\n",
    "    \n",
    "    print(f\"‚úÖ Predicciones convertidas:\")\n",
    "    print(f\"   - Antes: {np.unique(y_pred_opt)}\")\n",
    "    print(f\"   - Despu√©s: {np.unique(y_pred_opt_numeric)}\")\n",
    "    \n",
    "    # Reemplazar las predicciones\n",
    "    y_pred_opt = y_pred_opt_numeric\n",
    "else:\n",
    "    print(\"‚úÖ Predicciones ya est√°n en formato num√©rico\")\n",
    "\n",
    "# Verificar consistencia de y_val \n",
    "if y_val.dtype == 'object' or isinstance(y_val.iloc[0] if hasattr(y_val, 'iloc') else y_val[0], str):\n",
    "    print(\"‚ö†Ô∏è y_val tambi√©n est√° en formato texto, convirtiendo...\")\n",
    "    \n",
    "    # Convertir y_val si es necesario\n",
    "    if hasattr(y_val, 'map'):  # Si es pandas Series\n",
    "        y_val_numeric = y_val.map({'Yes': 1, 'No': 0})\n",
    "    else:  # Si es numpy array\n",
    "        y_val_numeric = np.where(y_val == 'Yes', 1, 0)\n",
    "    \n",
    "    y_val = y_val_numeric\n",
    "    print(\"‚úÖ y_val convertido a formato num√©rico\")\n",
    "\n",
    "print(f\"\\nüìä Estado final:\")\n",
    "print(f\"   - y_val: tipo {type(y_val)}, valores √∫nicos {np.unique(y_val)}\")\n",
    "print(f\"   - y_pred_opt: tipo {type(y_pred_opt)}, valores √∫nicos {np.unique(y_pred_opt)}\")\n",
    "print(\"‚úÖ Tipos de datos sincronizados correctamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìà EVALUACI√ìN FINAL: Calcular m√©tricas del modelo optimizado con MLflow\n",
    "\n",
    "print(\"üìä Calculando m√©tricas del modelo optimizado con tipos correctos...\")\n",
    "\n",
    "# Importar m√©tricas\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "try:\n",
    "    # Calcular m√©tricas del modelo optimizado\n",
    "    opt_auc = roc_auc_score(y_val, y_pred_proba_opt)\n",
    "    opt_acc = accuracy_score(y_val, y_pred_opt)\n",
    "    opt_precision = precision_score(y_val, y_pred_opt)\n",
    "    opt_recall = recall_score(y_val, y_pred_opt)\n",
    "    opt_f1 = f1_score(y_val, y_pred_opt)\n",
    "\n",
    "    print(f\"\\nüéØ M√©tricas del modelo optimizado:\")\n",
    "    print(f\"   - ROC AUC: {opt_auc:.4f}\")\n",
    "    print(f\"   - Accuracy: {opt_acc:.4f}\")\n",
    "    print(f\"   - Precision: {opt_precision:.4f}\")\n",
    "    print(f\"   - Recall: {opt_recall:.4f}\")\n",
    "    print(f\"   - F1-Score: {opt_f1:.4f}\")\n",
    "\n",
    "    # üéØ MLflow: Registrar modelo optimizado\n",
    "    print(\"\\nüîÑ Registrando modelo optimizado en MLflow...\")\n",
    "    \n",
    "    # Obtener hiperpar√°metros del modelo optimizado\n",
    "    if hasattr(optimized_model, 'named_steps'):\n",
    "        classifier = optimized_model.named_steps['classifier']\n",
    "        optimized_hyperparams = classifier.get_params()\n",
    "    else:\n",
    "        optimized_hyperparams = optimized_model.get_params()\n",
    "    \n",
    "    # Agregar informaci√≥n de optimizaci√≥n\n",
    "    optimized_hyperparams['optimization_method'] = 'GridSearchCV'\n",
    "    optimized_hyperparams['cv_folds'] = 5\n",
    "    optimized_hyperparams['cv_best_score'] = grid_search.best_score_\n",
    "    \n",
    "    # Registrar en MLflow\n",
    "    optimized_run_id = log_model_metrics(\n",
    "        model_name=f\"{best_model_name}_OPTIMIZED\",\n",
    "        y_true=y_val,\n",
    "        y_pred=y_pred_opt,\n",
    "        y_pred_proba=y_pred_proba_opt,\n",
    "        model=optimized_model,\n",
    "        X_test=X_val,\n",
    "        hyperparams=optimized_hyperparams\n",
    "    )\n",
    "    \n",
    "    print(f\"   ‚úÖ Modelo optimizado registrado - Run ID: {optimized_run_id[:8]}...\")\n",
    "\n",
    "    # üìä Comparar con el modelo original si est√° disponible\n",
    "    if 'results' in locals() or 'results' in globals():\n",
    "        original_auc = results[best_model_name]['ROC_AUC']\n",
    "        improvement = opt_auc - original_auc\n",
    "        \n",
    "        print(f\"\\nüìà Comparaci√≥n con modelo original:\")\n",
    "        print(f\"   - ROC AUC original: {original_auc:.4f}\")\n",
    "        print(f\"   - ROC AUC optimizado: {opt_auc:.4f}\")\n",
    "        print(f\"   - Mejora: {improvement:.4f} ({improvement*100:.2f}%)\")\n",
    "        \n",
    "        # üéØ MLflow: Registrar comparaci√≥n\n",
    "        with mlflow.start_run(run_name=f\"COMPARISON_{best_model_name}\"):\n",
    "            mlflow.log_metric(\"original_roc_auc\", original_auc)\n",
    "            mlflow.log_metric(\"optimized_roc_auc\", opt_auc)\n",
    "            mlflow.log_metric(\"improvement\", improvement)\n",
    "            mlflow.log_metric(\"improvement_percentage\", improvement * 100)\n",
    "            \n",
    "            mlflow.set_tag(\"comparison\", \"original_vs_optimized\")\n",
    "            mlflow.set_tag(\"best_model\", best_model_name)\n",
    "            \n",
    "            # Log de la decisi√≥n\n",
    "            if improvement > 0.001:  # Mejora significativa\n",
    "                mlflow.set_tag(\"decision\", \"use_optimized\")\n",
    "                print(\"üéâ ¬°Optimizaci√≥n exitosa! El modelo mejor√≥ significativamente\")\n",
    "                # Actualizar best_model si est√° disponible\n",
    "                if 'optimized_model' in locals() or 'optimized_model' in globals():\n",
    "                    best_model = optimized_model\n",
    "                    print(\"‚úÖ Mejor modelo actualizado con la versi√≥n optimizada\")\n",
    "            else:\n",
    "                mlflow.set_tag(\"decision\", \"use_original\")\n",
    "                print(\"üìù La optimizaci√≥n no produjo mejoras significativas\")\n",
    "                print(\"‚úÖ Manteniendo el modelo original\")\n",
    "                \n",
    "        print(f\"   ‚úÖ Comparaci√≥n registrada en MLflow\")\n",
    "    else:\n",
    "        print(\"üìù No hay resultados originales para comparar\")\n",
    "\n",
    "    print(\"\\n‚úÖ C√°lculo de m√©tricas completado exitosamente\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error calculando m√©tricas: {e}\")\n",
    "    print(\"üí° Verificando variables disponibles...\")\n",
    "    \n",
    "    available_vars = []\n",
    "    for var in ['y_val', 'y_pred_opt', 'y_pred_proba_opt', 'results', 'best_model_name']:\n",
    "        if var in locals() or var in globals():\n",
    "            available_vars.append(var)\n",
    "            \n",
    "    print(f\"Variables disponibles: {available_vars}\")\n",
    "    \n",
    "    # Log del error en MLflow\n",
    "    with mlflow.start_run(run_name=\"ERROR_OPTIMIZATION_EVALUATION\"):\n",
    "        mlflow.set_tag(\"status\", \"error\")\n",
    "        mlflow.set_tag(\"error_message\", str(e))\n",
    "        mlflow.log_param(\"available_variables\", str(available_vars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "filename = f\"submissions\\submission_grupoM_optimized_{optimized_run_id[:5]}.csv\"\n",
    "submission_df_optimized = create_submission_file(\n",
    "    final_model=optimized_model,\n",
    "    X_train_full=X_train_clean,\n",
    "    y_train_full=y_train_sync,\n",
    "    X_test_full=X_test_clean,\n",
    "    customer_ids=customer_ids,\n",
    "    filename=filename\n",
    ")\n",
    "\n",
    "print(f\"\\nüìã Primeras 10 predicciones del modelo optimizado:\")\n",
    "print(submission_df_optimized.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uAwfxIMfo-J4"
   },
   "source": [
    "#  **6. Conclusiones (Opcional pero Recomendado)**\n",
    "\n",
    "## Escribe un breve resumen de tus hallazgos.\n",
    "* ## ¬øQu√© modelo funcion√≥ mejor y por qu√© crees que fue as√≠?\n",
    "  El modelo que funcion√≥ mejor fue el de regresi√≥n logistica. Fue la que mejor se ajusto a los datos existentes.\n",
    "* ## ¬øCu√°les fueron las caracter√≠sticas m√°s importantes o los descubrimientos m√°s interesantes del EDA?\n",
    "  Pendiente\n",
    "* ## ¬øQu√© desaf√≠os encontraron y c√≥mo los resolvieron?\n",
    "  Los desafio fueron varios, los podriamos clasificar en \n",
    "  1. Desafios t√©cnicos de implemetaci√≥n dela ambiente.\n",
    "  2. Desafios en el desarrollod el pipeline de ML, principalmente en el preprocesamiento de datos, validaci√≥n interna usando test_split, conversi√≥n de parametros, validaci√≥n de Nan, problemas de casteo por diferencia de tipo de datos etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üéØ MLflow: Dashboard de Experimentos y Comparaci√≥n de Modelos\n",
    "\n",
    "print(\"üìä Generando dashboard de experimentos MLflow...\")\n",
    "\n",
    "# Obtener todos los runs del experimento actual\n",
    "experiment = mlflow.get_experiment_by_name(\"Churn_Prediction_TP5\")\n",
    "runs = mlflow.search_runs(experiment_ids=[experiment.experiment_id])\n",
    "\n",
    "print(f\"\\nüß™ Resumen del Experimento: {experiment.name}\")\n",
    "print(f\"   - Total de runs: {len(runs)}\")\n",
    "print(f\"   - Tracking URI: {mlflow.get_tracking_uri()}\")\n",
    "\n",
    "# Mostrar top 5 modelos por ROC AUC\n",
    "if len(runs) > 0 and 'metrics.roc_auc' in runs.columns:\n",
    "    print(\"\\nüèÜ Top 5 Modelos por ROC AUC:\")\n",
    "    top_models = runs.nlargest(5, 'metrics.roc_auc')[\n",
    "        ['tags.model_type', 'metrics.roc_auc', 'metrics.accuracy', 'metrics.f1_score']\n",
    "    ]\n",
    "    \n",
    "    for i, (idx, row) in enumerate(top_models.iterrows(), 1):\n",
    "        model_name = row.get('tags.model_type', 'Unknown')\n",
    "        roc_auc = row.get('metrics.roc_auc', 0)\n",
    "        accuracy = row.get('metrics.accuracy', 0)\n",
    "        f1_score = row.get('metrics.f1_score', 0)\n",
    "        \n",
    "        print(f\"   {i}. {model_name}\")\n",
    "        print(f\"      - ROC AUC: {roc_auc:.4f}\")\n",
    "        print(f\"      - Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"      - F1-Score: {f1_score:.4f}\")\n",
    "\n",
    "    # Visualizaci√≥n comparativa\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Subplot 1: ROC AUC Comparison\n",
    "    plt.subplot(1, 3, 1)\n",
    "    model_names = [name if len(name) <= 15 else name[:12] + '...' for name in top_models['tags.model_type']]\n",
    "    plt.bar(model_names, top_models['metrics.roc_auc'], color='skyblue')\n",
    "    plt.title('Comparaci√≥n ROC AUC')\n",
    "    plt.ylabel('ROC AUC Score')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Subplot 2: Accuracy Comparison\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.bar(model_names, top_models['metrics.accuracy'], color='lightgreen')\n",
    "    plt.title('Comparaci√≥n Accuracy')\n",
    "    plt.ylabel('Accuracy Score')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Subplot 3: F1-Score Comparison\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.bar(model_names, top_models['metrics.f1_score'], color='salmon')\n",
    "    plt.title('Comparaci√≥n F1-Score')\n",
    "    plt.ylabel('F1-Score')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Tabla comparativa detallada\n",
    "    print(\"\\nüìã Tabla Comparativa Detallada:\")\n",
    "    comparison_df = runs[[\n",
    "        'tags.model_type', 'metrics.roc_auc', 'metrics.accuracy', \n",
    "        'metrics.precision', 'metrics.recall', 'metrics.f1_score'\n",
    "    ]].round(4)\n",
    "    comparison_df.columns = ['Model', 'ROC_AUC', 'Accuracy', 'Precision', 'Recall', 'F1_Score']\n",
    "    display(comparison_df.sort_values('ROC_AUC', ascending=False))\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No se encontraron m√©tricas de ROC AUC en los runs\")\n",
    "    if len(runs) > 0:\n",
    "        print(\"üìã Columnas disponibles:\")\n",
    "        print(runs.columns.tolist())\n",
    "\n",
    "# Informaci√≥n para acceder a MLflow UI\n",
    "print(f\"\\nüåê Para acceder al dashboard completo de MLflow:\")\n",
    "print(f\"   1. Abre una terminal en la carpeta del proyecto\")\n",
    "print(f\"   2. Ejecuta: mlflow ui --backend-store-uri {mlflow.get_tracking_uri()}\")\n",
    "print(f\"   3. Abre tu navegador en: http://localhost:5000\")\n",
    "print(f\"   4. Busca el experimento: '{experiment.name}'\")\n",
    "\n",
    "print(\"\\n‚úÖ Dashboard de experimentos generado correctamente\")\n",
    "print(\"üéØ Ahora puedes comparar f√°cilmente todos tus modelos y experimentos!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ **Iniciar MLflow UI para Exploraci√≥n Interactiva**\n",
    "\n",
    "## **Pasos para acceder al dashboard web de MLflow:**\n",
    "\n",
    "### **1. Abrir Terminal**\n",
    "- En VS Code: presiona `Ctrl+Shift+` ` (backtick) o ve a Terminal > New Terminal\n",
    "\n",
    "### **2. Navegar al directorio del proyecto**\n",
    "```bash\n",
    "cd \"notebooks\\UTN-elearning-analisis-datos-avanzado\\Unidades\\Unidad5\\TP5\"\n",
    "```\n",
    "\n",
    "### **3. Iniciar el servidor MLflow**\n",
    "```bash\n",
    "mlflow ui\n",
    "```\n",
    "\n",
    "### **4. Acceder al dashboard**\n",
    "- Abre tu navegador web\n",
    "- Ve a: **http://localhost:5000**\n",
    "- Busca el experimento: **\"Churn_Prediction_TP5\"**\n",
    "\n",
    "## **Funcionalidades del MLflow UI:**\n",
    "\n",
    "### **üìä Comparaci√≥n de Experimentos**\n",
    "- **Tabla comparativa** de todos los modelos entrenados\n",
    "- **Filtros y ordenamiento** por cualquier m√©trica\n",
    "- **Gr√°ficos comparativos** autom√°ticos\n",
    "\n",
    "### **üìà M√©tricas Detalladas**\n",
    "- **ROC AUC, Accuracy, Precision, Recall, F1-Score**\n",
    "- **Hiperpar√°metros** de cada modelo\n",
    "- **Artifacts** (modelos guardados, gr√°ficos, etc.)\n",
    "\n",
    "### **üîç An√°lisis de Runs**\n",
    "- **Timeline** de experimentos\n",
    "- **Comparaci√≥n lado a lado** de modelos\n",
    "- **Download de modelos** entrenados\n",
    "\n",
    "### **üèÜ Mejores Pr√°cticas**\n",
    "- **Reproducibilidad** completa de experimentos\n",
    "- **Versionado** de modelos\n",
    "- **Colaboraci√≥n** en equipo\n",
    "\n",
    "---\n",
    "### üí° **Tip:** El servidor MLflow se ejecutar√° en background, puedes dejarlo corriendo mientras trabajas en el notebook."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
