{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41ced8fb",
   "metadata": {},
   "source": [
    "# MLflow Experiment Tracking\n",
    "# Seguimiento de Experimentos con MLflow\n",
    "\n",
    "Este notebook demuestra cÃ³mo usar MLflow para el seguimiento de experimentos de machine learning usando los datos del proyecto Practical Statistics for Data Scientists.\n",
    "\n",
    "## Objetivos:\n",
    "- Configurar MLflow para seguimiento de experimentos\n",
    "- Entrenar mÃºltiples modelos de clasificaciÃ³n\n",
    "- Registrar parÃ¡metros, mÃ©tricas y artefactos\n",
    "- Comparar diferentes modelos y configuraciones\n",
    "- Usar MLflow UI para visualizar resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18201a5b",
   "metadata": {},
   "source": [
    "## InstalaciÃ³n de MLflow\n",
    "\n",
    "Si no tienes MLflow instalado, descomenta y ejecuta la siguiente celda:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685cbc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install mlflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62373736",
   "metadata": {},
   "source": [
    "## Importar librerÃ­as necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e51c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# MLflow\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Configurar estilo de grÃ¡ficos\n",
    "plt.style.use('seaborn-v0_8')\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"ðŸ“š LibrerÃ­as importadas exitosamente\")\n",
    "print(f\"ðŸ”¬ MLflow versiÃ³n: {mlflow.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7d958c",
   "metadata": {},
   "source": [
    "## ConfiguraciÃ³n de MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe88094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar MLflow\n",
    "experiment_name = \"Loan Default Classification\"\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "# Obtener informaciÃ³n del experimento\n",
    "experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "print(f\"ðŸ”¬ Experimento: {experiment.name}\")\n",
    "print(f\"ðŸ“ Experiment ID: {experiment.experiment_id}\")\n",
    "print(f\"ðŸ“‚ Artifact Location: {experiment.artifact_location}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cff6ff1",
   "metadata": {},
   "source": [
    "## Carga y preparaciÃ³n de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017cafc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar datos\n",
    "try:\n",
    "    import common\n",
    "    DATA = common.dataDirectory()\n",
    "except ImportError:\n",
    "    DATA = Path().resolve().parent.parent / 'data'\n",
    "\n",
    "# Cargar dataset de prÃ©stamos\n",
    "loan_data = pd.read_csv(DATA / 'loan3000.csv')\n",
    "\n",
    "print(f\"ðŸ“Š Dataset cargado: {loan_data.shape[0]} filas, {loan_data.shape[1]} columnas\")\n",
    "print(f\"ðŸŽ¯ Variables objetivo: {loan_data['outcome'].value_counts().to_dict()}\")\n",
    "\n",
    "# Mostrar primeras filas\n",
    "loan_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b21bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ExploraciÃ³n rÃ¡pida de datos\n",
    "print(\"ðŸ“ˆ InformaciÃ³n del dataset:\")\n",
    "print(loan_data.info())\n",
    "print(\"\\nðŸ“Š EstadÃ­sticas descriptivas:\")\n",
    "print(loan_data.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b0aaba",
   "metadata": {},
   "source": [
    "## PreparaciÃ³n de datos para ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47966e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar datos para machine learning\n",
    "# Seleccionar caracterÃ­sticas numÃ©ricas\n",
    "features = ['borrower_score', 'payment_inc_ratio']\n",
    "target = 'outcome'\n",
    "\n",
    "X = loan_data[features]\n",
    "y = loan_data[target]\n",
    "\n",
    "# Codificar variable objetivo\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "\n",
    "# DivisiÃ³n train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
    ")\n",
    "\n",
    "# Escalado de caracterÃ­sticas\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"ðŸ“Š Datos de entrenamiento: {X_train.shape[0]} muestras\")\n",
    "print(f\"ðŸ“Š Datos de prueba: {X_test.shape[0]} muestras\")\n",
    "print(f\"ðŸ·ï¸ Clases: {le.classes_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96636827",
   "metadata": {},
   "source": [
    "## FunciÃ³n para entrenar y registrar modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11cc1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_log_model(model, model_name, X_train, X_test, y_train, y_test, params=None):\n",
    "    \"\"\"\n",
    "    Entrena un modelo y registra mÃ©tricas en MLflow\n",
    "    \"\"\"\n",
    "    with mlflow.start_run(run_name=model_name):\n",
    "        # Registrar parÃ¡metros\n",
    "        mlflow.log_param(\"model_type\", model_name)\n",
    "        mlflow.log_param(\"train_samples\", len(X_train))\n",
    "        mlflow.log_param(\"test_samples\", len(X_test))\n",
    "        mlflow.log_param(\"features\", list(X_train.columns) if hasattr(X_train, 'columns') else \"scaled_features\")\n",
    "        \n",
    "        if params:\n",
    "            for key, value in params.items():\n",
    "                mlflow.log_param(key, value)\n",
    "        \n",
    "        # Entrenar modelo\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Predicciones\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "        \n",
    "        # Calcular mÃ©tricas\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred, average='weighted')\n",
    "        recall = recall_score(y_test, y_pred, average='weighted')\n",
    "        f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "        \n",
    "        # Registrar mÃ©tricas\n",
    "        mlflow.log_metric(\"accuracy\", accuracy)\n",
    "        mlflow.log_metric(\"precision\", precision)\n",
    "        mlflow.log_metric(\"recall\", recall)\n",
    "        mlflow.log_metric(\"f1_score\", f1)\n",
    "        \n",
    "        # AUC si hay probabilidades\n",
    "        if y_pred_proba is not None:\n",
    "            auc = roc_auc_score(y_test, y_pred_proba)\n",
    "            mlflow.log_metric(\"auc\", auc)\n",
    "        \n",
    "        # Cross-validation score\n",
    "        cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')\n",
    "        mlflow.log_metric(\"cv_mean\", cv_scores.mean())\n",
    "        mlflow.log_metric(\"cv_std\", cv_scores.std())\n",
    "        \n",
    "        # Registrar modelo\n",
    "        mlflow.sklearn.log_model(model, \"model\")\n",
    "        \n",
    "        # Crear y guardar matriz de confusiÃ³n\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                   xticklabels=le.classes_, yticklabels=le.classes_)\n",
    "        plt.title(f'Matriz de ConfusiÃ³n - {model_name}')\n",
    "        plt.xlabel('PredicciÃ³n')\n",
    "        plt.ylabel('Actual')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Guardar grÃ¡fico como artefacto\n",
    "        plt.savefig(f\"confusion_matrix_{model_name.replace(' ', '_')}.png\")\n",
    "        mlflow.log_artifact(f\"confusion_matrix_{model_name.replace(' ', '_')}.png\")\n",
    "        plt.show()\n",
    "        \n",
    "        # Registrar reporte de clasificaciÃ³n\n",
    "        report = classification_report(y_test, y_pred, target_names=le.classes_, output_dict=True)\n",
    "        \n",
    "        # Crear archivo de texto con el reporte\n",
    "        with open(f\"classification_report_{model_name.replace(' ', '_')}.txt\", \"w\") as f:\n",
    "            f.write(classification_report(y_test, y_pred, target_names=le.classes_))\n",
    "        mlflow.log_artifact(f\"classification_report_{model_name.replace(' ', '_')}.txt\")\n",
    "        \n",
    "        print(f\"âœ… {model_name} entrenado y registrado en MLflow\")\n",
    "        print(f\"ðŸ“Š Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"ðŸ“Š F1-Score: {f1:.4f}\")\n",
    "        if y_pred_proba is not None:\n",
    "            print(f\"ðŸ“Š AUC: {auc:.4f}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a697d641",
   "metadata": {},
   "source": [
    "## Entrenar mÃºltiples modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5bf60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir modelos a probar\n",
    "models_to_test = [\n",
    "    (LogisticRegression(random_state=42), \"Logistic Regression\", {\"C\": 1.0, \"solver\": \"liblinear\"}),\n",
    "    (RandomForestClassifier(random_state=42), \"Random Forest\", {\"n_estimators\": 100, \"max_depth\": None}),\n",
    "    (SVC(random_state=42, probability=True), \"Support Vector Machine\", {\"C\": 1.0, \"kernel\": \"rbf\"}),\n",
    "    (GaussianNB(), \"Naive Bayes\", {}),\n",
    "    (DecisionTreeClassifier(random_state=42), \"Decision Tree\", {\"max_depth\": None, \"min_samples_split\": 2})\n",
    "]\n",
    "\n",
    "print(\"ðŸš€ Iniciando entrenamiento de modelos con MLflow...\\n\")\n",
    "\n",
    "trained_models = {}\n",
    "\n",
    "for model, name, params in models_to_test:\n",
    "    print(f\"ðŸ”„ Entrenando {name}...\")\n",
    "    trained_model = train_and_log_model(\n",
    "        model, name, \n",
    "        pd.DataFrame(X_train_scaled, columns=features), \n",
    "        pd.DataFrame(X_test_scaled, columns=features),\n",
    "        y_train, y_test, params\n",
    "    )\n",
    "    trained_models[name] = trained_model\n",
    "\n",
    "print(\"\\nðŸŽ‰ Â¡Todos los modelos han sido entrenados y registrados!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1845af50",
   "metadata": {},
   "source": [
    "## Experimento con hiperparÃ¡metros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48ae9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probar diferentes hiperparÃ¡metros para Random Forest\n",
    "print(\"ðŸ”¬ Experimentando con hiperparÃ¡metros de Random Forest...\\n\")\n",
    "\n",
    "rf_params = [\n",
    "    {\"n_estimators\": 50, \"max_depth\": 5},\n",
    "    {\"n_estimators\": 100, \"max_depth\": 10},\n",
    "    {\"n_estimators\": 200, \"max_depth\": 15},\n",
    "    {\"n_estimators\": 150, \"max_depth\": None}\n",
    "]\n",
    "\n",
    "for i, params in enumerate(rf_params):\n",
    "    model = RandomForestClassifier(random_state=42, **params)\n",
    "    model_name = f\"Random Forest - Config {i+1}\"\n",
    "    \n",
    "    print(f\"ðŸŒ³ Probando {model_name} con parÃ¡metros: {params}\")\n",
    "    train_and_log_model(\n",
    "        model, model_name,\n",
    "        pd.DataFrame(X_train_scaled, columns=features),\n",
    "        pd.DataFrame(X_test_scaled, columns=features),\n",
    "        y_train, y_test, params\n",
    "    )\n",
    "\n",
    "print(\"\\nâœ… Experimentos con hiperparÃ¡metros completados!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1ee473",
   "metadata": {},
   "source": [
    "## Consultar experimentos registrados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8714cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener todos los runs del experimento\n",
    "client = MlflowClient()\n",
    "experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "runs = client.search_runs(experiment.experiment_id)\n",
    "\n",
    "print(f\"ðŸ“Š Total de runs en el experimento: {len(runs)}\")\n",
    "print(\"\\nðŸ† Resumen de resultados:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Crear DataFrame con resultados\n",
    "results_data = []\n",
    "for run in runs:\n",
    "    metrics = run.data.metrics\n",
    "    params = run.data.params\n",
    "    \n",
    "    results_data.append({\n",
    "        'run_id': run.info.run_id[:8],\n",
    "        'model_type': params.get('model_type', 'Unknown'),\n",
    "        'accuracy': metrics.get('accuracy', 0),\n",
    "        'f1_score': metrics.get('f1_score', 0),\n",
    "        'auc': metrics.get('auc', 0),\n",
    "        'cv_mean': metrics.get('cv_mean', 0),\n",
    "        'start_time': run.info.start_time\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results_data)\n",
    "results_df = results_df.sort_values('accuracy', ascending=False)\n",
    "\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Mejor modelo\n",
    "best_model = results_df.iloc[0]\n",
    "print(f\"\\nðŸ¥‡ Mejor modelo: {best_model['model_type']}\")\n",
    "print(f\"ðŸ“Š Accuracy: {best_model['accuracy']:.4f}\")\n",
    "print(f\"ðŸ“Š F1-Score: {best_model['f1_score']:.4f}\")\n",
    "print(f\"ðŸ“Š AUC: {best_model['auc']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51e5f7a",
   "metadata": {},
   "source": [
    "## VisualizaciÃ³n de resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6b9d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GrÃ¡fico de comparaciÃ³n de modelos\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Subplot 1: Accuracy\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.barh(results_df['model_type'], results_df['accuracy'])\n",
    "plt.xlabel('Accuracy')\n",
    "plt.title('ComparaciÃ³n de Accuracy por Modelo')\n",
    "plt.xlim(0, 1)\n",
    "\n",
    "# Subplot 2: F1-Score\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.barh(results_df['model_type'], results_df['f1_score'])\n",
    "plt.xlabel('F1-Score')\n",
    "plt.title('ComparaciÃ³n de F1-Score por Modelo')\n",
    "plt.xlim(0, 1)\n",
    "\n",
    "# Subplot 3: AUC\n",
    "plt.subplot(2, 2, 3)\n",
    "auc_data = results_df[results_df['auc'] > 0]  # Solo modelos con AUC\n",
    "plt.barh(auc_data['model_type'], auc_data['auc'])\n",
    "plt.xlabel('AUC')\n",
    "plt.title('ComparaciÃ³n de AUC por Modelo')\n",
    "plt.xlim(0, 1)\n",
    "\n",
    "# Subplot 4: CV Mean\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.barh(results_df['model_type'], results_df['cv_mean'])\n",
    "plt.xlabel('CV Mean Accuracy')\n",
    "plt.title('ComparaciÃ³n de CV Mean por Modelo')\n",
    "plt.xlim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Guardar como artefacto en MLflow\n",
    "with mlflow.start_run(run_name=\"Model Comparison Summary\"):\n",
    "    mlflow.log_artifact('model_comparison.png')\n",
    "    mlflow.log_metric(\"total_models_tested\", len(results_df))\n",
    "    mlflow.log_metric(\"best_accuracy\", results_df['accuracy'].max())\n",
    "    mlflow.log_param(\"best_model_type\", best_model['model_type'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7d0926",
   "metadata": {},
   "source": [
    "## Instrucciones para usar MLflow UI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c40d493",
   "metadata": {},
   "source": [
    "## ðŸš€ CÃ³mo usar MLflow UI\n",
    "\n",
    "Para visualizar todos los experimentos en la interfaz web de MLflow:\n",
    "\n",
    "### 1. Abrir terminal/PowerShell\n",
    "```bash\n",
    "# En la carpeta del proyecto\n",
    "mlflow ui\n",
    "```\n",
    "\n",
    "### 2. Abrir navegador\n",
    "Ve a: http://localhost:5000\n",
    "\n",
    "### 3. Explorar experimentos\n",
    "- ðŸ“Š Ver todos los runs y mÃ©tricas\n",
    "- ðŸ“ˆ Comparar modelos lado a lado\n",
    "- ðŸ“ Descargar artefactos (grÃ¡ficos, modelos)\n",
    "- ðŸ” Filtrar y ordenar resultados\n",
    "- ðŸ“ Agregar notas y tags\n",
    "\n",
    "### 4. Funcionalidades Ãºtiles\n",
    "- **Compare runs**: Seleccionar mÃºltiples runs para comparar\n",
    "- **Parallel coordinates**: Visualizar relaciones entre parÃ¡metros y mÃ©tricas\n",
    "- **Scatter plots**: GrÃ¡ficos de dispersiÃ³n de mÃ©tricas\n",
    "- **Model registry**: Registrar los mejores modelos\n",
    "\n",
    "### 5. En Docker\n",
    "```powershell\n",
    "# Si usas Docker\n",
    ".\\docker-helper.ps1 shell\n",
    "# Dentro del contenedor:\n",
    "mlflow ui --host 0.0.0.0 --port 5000\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bd5b2a",
   "metadata": {},
   "source": [
    "## PrÃ³ximos pasos\n",
    "\n",
    "### ðŸŽ¯ Sugerencias para experimentar mÃ¡s:\n",
    "\n",
    "1. **MÃ¡s datasets**: Probar con `loan_data.csv` completo\n",
    "2. **Feature engineering**: Crear nuevas caracterÃ­sticas\n",
    "3. **MÃ¡s modelos**: XGBoost, LightGBM, Neural Networks\n",
    "4. **Hyperparameter tuning**: Grid search, Random search\n",
    "5. **Ensemble methods**: Voting, Stacking\n",
    "6. **Cross-validation**: MÃ¡s estrategias de validaciÃ³n\n",
    "7. **Model registry**: Registrar modelos en producciÃ³n\n",
    "8. **A/B testing**: Comparar modelos en producciÃ³n\n",
    "\n",
    "### ðŸ“š Recursos adicionales:\n",
    "- [MLflow Documentation](https://mlflow.org/docs/latest/index.html)\n",
    "- [MLflow Examples](https://github.com/mlflow/mlflow/tree/master/examples)\n",
    "- [MLflow Tutorials](https://mlflow.org/docs/latest/tutorials-and-examples/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ec40c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpiar archivos temporales\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Eliminar archivos de imÃ¡genes temporales\n",
    "for file in glob.glob(\"*.png\"):\n",
    "    try:\n",
    "        os.remove(file)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "for file in glob.glob(\"*.txt\"):\n",
    "    try:\n",
    "        os.remove(file)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print(\"ðŸ§¹ Archivos temporales limpiados\")\n",
    "print(\"\\nâœ… Â¡Notebook completado!\")\n",
    "print(\"ðŸš€ Ejecuta 'mlflow ui' en terminal para explorar los resultados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46ee3b3",
   "metadata": {},
   "source": [
    "# Ejemplo clÃ¡sico: Dataset Iris\n",
    "\n",
    "Ahora vamos a demostrar MLflow con el famoso dataset Iris, un ejemplo clÃ¡sico de clasificaciÃ³n multiclase en machine learning.\n",
    "\n",
    "## Sobre el dataset Iris:\n",
    "- **150 muestras** de flores iris\n",
    "- **4 caracterÃ­sticas**: longitud y ancho de sÃ©palo y pÃ©talo\n",
    "- **3 clases**: Iris-setosa, Iris-versicolor, Iris-virginica\n",
    "- **Problema**: ClasificaciÃ³n multiclase\n",
    "- **Objetivo**: Predecir la especie de iris basÃ¡ndose en las medidas florales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de0d06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar dataset Iris\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Cargar datos\n",
    "iris = load_iris()\n",
    "X_iris = iris.data\n",
    "y_iris = iris.target\n",
    "\n",
    "# Crear DataFrame para mejor visualizaciÃ³n\n",
    "iris_df = pd.DataFrame(X_iris, columns=iris.feature_names)\n",
    "iris_df['species'] = iris.target\n",
    "iris_df['species_name'] = iris_df['species'].map({0: 'setosa', 1: 'versicolor', 2: 'virginica'})\n",
    "\n",
    "print(\"ðŸŒ¸ Dataset Iris cargado exitosamente\")\n",
    "print(f\"ðŸ“Š Forma del dataset: {iris_df.shape}\")\n",
    "print(f\"ðŸ·ï¸ CaracterÃ­sticas: {list(iris.feature_names)}\")\n",
    "print(f\"ðŸŽ¯ Clases: {list(iris.target_names)}\")\n",
    "print(f\"ðŸ“ˆ DistribuciÃ³n de clases: {pd.Series(iris.target).value_counts().sort_index().to_dict()}\")\n",
    "\n",
    "# Mostrar primeras filas\n",
    "print(\"\\nðŸ“‹ Primeras 5 filas del dataset:\")\n",
    "iris_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ca79d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VisualizaciÃ³n exploratoria del dataset Iris\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('ðŸŒ¸ AnÃ¡lisis Exploratorio del Dataset Iris', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. DistribuciÃ³n de las caracterÃ­sticas\n",
    "iris_df_melted = iris_df.melt(id_vars=['species_name'], \n",
    "                              value_vars=iris.feature_names,\n",
    "                              var_name='feature', value_name='value')\n",
    "\n",
    "axes[0, 0].set_title('DistribuciÃ³n de CaracterÃ­sticas por Especie')\n",
    "sns.boxplot(data=iris_df_melted, x='feature', y='value', hue='species_name', ax=axes[0, 0])\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "axes[0, 0].legend(title='Especie')\n",
    "\n",
    "# 2. Matriz de correlaciÃ³n\n",
    "correlation_matrix = iris_df[iris.feature_names].corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, ax=axes[0, 1])\n",
    "axes[0, 1].set_title('Matriz de CorrelaciÃ³n de CaracterÃ­sticas')\n",
    "\n",
    "# 3. Scatter plot: Sepal vs Petal\n",
    "scatter = axes[1, 0].scatter(iris_df['sepal length (cm)'], iris_df['sepal width (cm)'], \n",
    "                           c=iris_df['species'], cmap='viridis', alpha=0.7)\n",
    "axes[1, 0].set_xlabel('Longitud del SÃ©palo (cm)')\n",
    "axes[1, 0].set_ylabel('Ancho del SÃ©palo (cm)')\n",
    "axes[1, 0].set_title('SÃ©palo: Longitud vs Ancho')\n",
    "plt.colorbar(scatter, ax=axes[1, 0])\n",
    "\n",
    "# 4. DistribuciÃ³n de clases\n",
    "species_counts = iris_df['species_name'].value_counts()\n",
    "axes[1, 1].pie(species_counts.values, labels=species_counts.index, autopct='%1.1f%%', startangle=90)\n",
    "axes[1, 1].set_title('DistribuciÃ³n de Especies')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# EstadÃ­sticas descriptivas por especie\n",
    "print(\"\\nðŸ“Š EstadÃ­sticas descriptivas por especie:\")\n",
    "print(iris_df.groupby('species_name')[iris.feature_names].describe().round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0cb0803",
   "metadata": {},
   "source": [
    "## PreparaciÃ³n de datos Iris para MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f533916c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar datos Iris para machine learning\n",
    "X_iris = iris.data\n",
    "y_iris = iris.target\n",
    "\n",
    "# DivisiÃ³n train/test\n",
    "X_train_iris, X_test_iris, y_train_iris, y_test_iris = train_test_split(\n",
    "    X_iris, y_iris, test_size=0.2, random_state=42, stratify=y_iris\n",
    ")\n",
    "\n",
    "# Escalado de caracterÃ­sticas\n",
    "scaler_iris = StandardScaler()\n",
    "X_train_iris_scaled = scaler_iris.fit_transform(X_train_iris)\n",
    "X_test_iris_scaled = scaler_iris.transform(X_test_iris)\n",
    "\n",
    "print(f\"ðŸŒ¸ Datos Iris preparados:\")\n",
    "print(f\"ðŸ“Š Entrenamiento: {X_train_iris.shape[0]} muestras\")\n",
    "print(f\"ðŸ“Š Prueba: {X_test_iris.shape[0]} muestras\") \n",
    "print(f\"ðŸ·ï¸ CaracterÃ­sticas: {iris.feature_names}\")\n",
    "print(f\"ðŸŽ¯ Clases: {iris.target_names}\")\n",
    "\n",
    "# Crear nuevo experimento para Iris\n",
    "iris_experiment_name = \"Iris Species Classification\"\n",
    "mlflow.set_experiment(iris_experiment_name)\n",
    "\n",
    "iris_experiment = mlflow.get_experiment_by_name(iris_experiment_name)\n",
    "print(f\"\\nðŸ”¬ Experimento Iris creado: {iris_experiment.name}\")\n",
    "print(f\"ðŸ“ Experiment ID: {iris_experiment.experiment_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f60deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_log_iris_model(model, model_name, X_train, X_test, y_train, y_test, params=None):\n",
    "    \"\"\"\n",
    "    Entrena un modelo multiclase para Iris y registra mÃ©tricas en MLflow\n",
    "    \"\"\"\n",
    "    with mlflow.start_run(run_name=f\"Iris - {model_name}\"):\n",
    "        # Registrar parÃ¡metros\n",
    "        mlflow.log_param(\"model_type\", model_name)\n",
    "        mlflow.log_param(\"dataset\", \"Iris\")\n",
    "        mlflow.log_param(\"problem_type\", \"multiclass_classification\")\n",
    "        mlflow.log_param(\"n_classes\", 3)\n",
    "        mlflow.log_param(\"train_samples\", len(X_train))\n",
    "        mlflow.log_param(\"test_samples\", len(X_test))\n",
    "        mlflow.log_param(\"n_features\", X_train.shape[1])\n",
    "        \n",
    "        if params:\n",
    "            for key, value in params.items():\n",
    "                mlflow.log_param(key, value)\n",
    "        \n",
    "        # Entrenar modelo\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Predicciones\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred_proba = model.predict_proba(X_test) if hasattr(model, 'predict_proba') else None\n",
    "        \n",
    "        # Calcular mÃ©tricas\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred, average='weighted')\n",
    "        recall = recall_score(y_test, y_pred, average='weighted')\n",
    "        f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "        \n",
    "        # MÃ©tricas por clase\n",
    "        precision_macro = precision_score(y_test, y_pred, average='macro')\n",
    "        recall_macro = recall_score(y_test, y_pred, average='macro')\n",
    "        f1_macro = f1_score(y_test, y_pred, average='macro')\n",
    "        \n",
    "        # Registrar mÃ©tricas\n",
    "        mlflow.log_metric(\"accuracy\", accuracy)\n",
    "        mlflow.log_metric(\"precision_weighted\", precision)\n",
    "        mlflow.log_metric(\"recall_weighted\", recall)\n",
    "        mlflow.log_metric(\"f1_weighted\", f1)\n",
    "        mlflow.log_metric(\"precision_macro\", precision_macro)\n",
    "        mlflow.log_metric(\"recall_macro\", recall_macro)\n",
    "        mlflow.log_metric(\"f1_macro\", f1_macro)\n",
    "        \n",
    "        # AUC multiclase (One-vs-Rest)\n",
    "        if y_pred_proba is not None:\n",
    "            auc_ovr = roc_auc_score(y_test, y_pred_proba, multi_class='ovr')\n",
    "            auc_ovo = roc_auc_score(y_test, y_pred_proba, multi_class='ovo')\n",
    "            mlflow.log_metric(\"auc_ovr\", auc_ovr)\n",
    "            mlflow.log_metric(\"auc_ovo\", auc_ovo)\n",
    "        \n",
    "        # Cross-validation\n",
    "        cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')\n",
    "        mlflow.log_metric(\"cv_mean\", cv_scores.mean())\n",
    "        mlflow.log_metric(\"cv_std\", cv_scores.std())\n",
    "        \n",
    "        # Registrar modelo\n",
    "        mlflow.sklearn.log_model(model, \"model\")\n",
    "        \n",
    "        # Matriz de confusiÃ³n\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                   xticklabels=iris.target_names, yticklabels=iris.target_names)\n",
    "        plt.title(f'Matriz de ConfusiÃ³n - {model_name} (Iris)')\n",
    "        plt.xlabel('PredicciÃ³n')\n",
    "        plt.ylabel('Actual')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Guardar grÃ¡fico\n",
    "        confusion_file = f\"iris_confusion_matrix_{model_name.replace(' ', '_')}.png\"\n",
    "        plt.savefig(confusion_file)\n",
    "        mlflow.log_artifact(confusion_file)\n",
    "        plt.show()\n",
    "        \n",
    "        # Reporte de clasificaciÃ³n\n",
    "        report = classification_report(y_test, y_pred, target_names=iris.target_names)\n",
    "        report_file = f\"iris_classification_report_{model_name.replace(' ', '_')}.txt\"\n",
    "        with open(report_file, \"w\") as f:\n",
    "            f.write(report)\n",
    "        mlflow.log_artifact(report_file)\n",
    "        \n",
    "        print(f\"ðŸŒ¸ {model_name} entrenado en dataset Iris\")\n",
    "        print(f\"ðŸ“Š Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"ðŸ“Š F1-Score (weighted): {f1:.4f}\")\n",
    "        print(f\"ðŸ“Š F1-Score (macro): {f1_macro:.4f}\")\n",
    "        if y_pred_proba is not None:\n",
    "            print(f\"ðŸ“Š AUC (OvR): {auc_ovr:.4f}\")\n",
    "            print(f\"ðŸ“Š AUC (OvO): {auc_ovo:.4f}\")\n",
    "        print(f\"ðŸ“Š CV Score: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e09c2c",
   "metadata": {},
   "source": [
    "## Entrenar mÃºltiples modelos con Iris\n",
    "\n",
    "Vamos a probar diferentes algoritmos de machine learning con el dataset Iris para comparar su rendimiento en clasificaciÃ³n multiclase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1458631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar modelos adicionales para Iris\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "\n",
    "# Definir modelos especÃ­ficos para Iris\n",
    "iris_models = [\n",
    "    (LogisticRegression(random_state=42, max_iter=1000), \"Logistic Regression\", \n",
    "     {\"C\": 1.0, \"solver\": \"liblinear\", \"multi_class\": \"ovr\"}),\n",
    "    \n",
    "    (RandomForestClassifier(random_state=42), \"Random Forest\", \n",
    "     {\"n_estimators\": 100, \"max_depth\": None, \"min_samples_split\": 2}),\n",
    "    \n",
    "    (SVC(random_state=42, probability=True), \"Support Vector Machine\", \n",
    "     {\"C\": 1.0, \"kernel\": \"rbf\", \"gamma\": \"scale\"}),\n",
    "    \n",
    "    (GaussianNB(), \"Naive Bayes\", {\"var_smoothing\": 1e-9}),\n",
    "    \n",
    "    (DecisionTreeClassifier(random_state=42), \"Decision Tree\", \n",
    "     {\"max_depth\": None, \"min_samples_split\": 2, \"criterion\": \"gini\"}),\n",
    "    \n",
    "    (KNeighborsClassifier(), \"K-Nearest Neighbors\", \n",
    "     {\"n_neighbors\": 5, \"weights\": \"uniform\", \"algorithm\": \"auto\"}),\n",
    "    \n",
    "    (GradientBoostingClassifier(random_state=42), \"Gradient Boosting\", \n",
    "     {\"n_estimators\": 100, \"learning_rate\": 0.1, \"max_depth\": 3}),\n",
    "    \n",
    "    (RidgeClassifier(random_state=42), \"Ridge Classifier\", \n",
    "     {\"alpha\": 1.0, \"solver\": \"auto\"})\n",
    "]\n",
    "\n",
    "print(\"ðŸš€ Iniciando entrenamiento de modelos con dataset Iris...\\n\")\n",
    "\n",
    "iris_trained_models = {}\n",
    "\n",
    "for model, name, params in iris_models:\n",
    "    print(f\"ðŸ”„ Entrenando {name} con Iris...\")\n",
    "    trained_model = train_and_log_iris_model(\n",
    "        model, name, \n",
    "        X_train_iris_scaled, X_test_iris_scaled,\n",
    "        y_train_iris, y_test_iris, params\n",
    "    )\n",
    "    iris_trained_models[name] = trained_model\n",
    "\n",
    "print(\"\\nðŸŽ‰ Â¡Todos los modelos Iris han sido entrenados y registrados!\")\n",
    "print(f\"ðŸ“Š Total de modelos entrenados: {len(iris_trained_models)}\")\n",
    "print(f\"ðŸ”¬ Experimento: {iris_experiment_name}\")\n",
    "print(f\"ðŸŒ Ver resultados en MLflow UI: http://localhost:5000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd1f13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AnÃ¡lisis de resultados Iris\n",
    "print(\"ðŸ” Analizando resultados del experimento Iris...\\n\")\n",
    "\n",
    "# Obtener runs del experimento Iris\n",
    "iris_client = MlflowClient()\n",
    "iris_runs = iris_client.search_runs(iris_experiment.experiment_id)\n",
    "\n",
    "print(f\"ðŸ“Š Total de runs en experimento Iris: {len(iris_runs)}\")\n",
    "print(\"\\nðŸ† Resumen de resultados Iris:\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# Crear DataFrame con resultados Iris\n",
    "iris_results_data = []\n",
    "for run in iris_runs:\n",
    "    metrics = run.data.metrics\n",
    "    params = run.data.params\n",
    "    \n",
    "    iris_results_data.append({\n",
    "        'run_id': run.info.run_id[:8],\n",
    "        'model_type': params.get('model_type', 'Unknown'),\n",
    "        'accuracy': metrics.get('accuracy', 0),\n",
    "        'f1_weighted': metrics.get('f1_weighted', 0),\n",
    "        'f1_macro': metrics.get('f1_macro', 0),\n",
    "        'auc_ovr': metrics.get('auc_ovr', 0),\n",
    "        'auc_ovo': metrics.get('auc_ovo', 0),\n",
    "        'cv_mean': metrics.get('cv_mean', 0),\n",
    "        'cv_std': metrics.get('cv_std', 0)\n",
    "    })\n",
    "\n",
    "iris_results_df = pd.DataFrame(iris_results_data)\n",
    "iris_results_df = iris_results_df.sort_values('accuracy', ascending=False)\n",
    "\n",
    "# Mostrar resultados\n",
    "print(iris_results_df.round(4).to_string(index=False))\n",
    "\n",
    "# Mejor modelo Iris\n",
    "best_iris_model = iris_results_df.iloc[0]\n",
    "print(f\"\\nðŸ¥‡ Mejor modelo Iris: {best_iris_model['model_type']}\")\n",
    "print(f\"ðŸ“Š Accuracy: {best_iris_model['accuracy']:.4f}\")\n",
    "print(f\"ðŸ“Š F1-Score (weighted): {best_iris_model['f1_weighted']:.4f}\")\n",
    "print(f\"ðŸ“Š F1-Score (macro): {best_iris_model['f1_macro']:.4f}\")\n",
    "print(f\"ðŸ“Š AUC (OvR): {best_iris_model['auc_ovr']:.4f}\")\n",
    "print(f\"ðŸ“Š CV Score: {best_iris_model['cv_mean']:.4f} (+/- {best_iris_model['cv_std'] * 2:.4f})\")\n",
    "\n",
    "# EstadÃ­sticas del experimento\n",
    "print(f\"\\nðŸ“ˆ EstadÃ­sticas del experimento Iris:\")\n",
    "print(f\"ðŸ“Š Accuracy promedio: {iris_results_df['accuracy'].mean():.4f}\")\n",
    "print(f\"ðŸ“Š Accuracy mÃ¡xima: {iris_results_df['accuracy'].max():.4f}\")\n",
    "print(f\"ðŸ“Š Accuracy mÃ­nima: {iris_results_df['accuracy'].min():.4f}\")\n",
    "print(f\"ðŸ“Š DesviaciÃ³n estÃ¡ndar: {iris_results_df['accuracy'].std():.4f}\")\n",
    "\n",
    "# Modelos con accuracy perfecta\n",
    "perfect_models = iris_results_df[iris_results_df['accuracy'] == 1.0]['model_type'].tolist()\n",
    "if perfect_models:\n",
    "    print(f\"\\nðŸŽ¯ Modelos con accuracy perfecta (100%): {', '.join(perfect_models)}\")\n",
    "else:\n",
    "    print(f\"\\nâš ï¸ NingÃºn modelo alcanzÃ³ accuracy perfecta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a37fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizaciones comparativas para Iris\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('ðŸŒ¸ ComparaciÃ³n de Modelos - Dataset Iris', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Accuracy\n",
    "axes[0, 0].barh(iris_results_df['model_type'], iris_results_df['accuracy'], color='skyblue')\n",
    "axes[0, 0].set_xlabel('Accuracy')\n",
    "axes[0, 0].set_title('Accuracy por Modelo')\n",
    "axes[0, 0].set_xlim(0, 1)\n",
    "axes[0, 0].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# 2. F1-Score Weighted\n",
    "axes[0, 1].barh(iris_results_df['model_type'], iris_results_df['f1_weighted'], color='lightgreen')\n",
    "axes[0, 1].set_xlabel('F1-Score (Weighted)')\n",
    "axes[0, 1].set_title('F1-Score Weighted por Modelo')\n",
    "axes[0, 1].set_xlim(0, 1)\n",
    "axes[0, 1].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# 3. F1-Score Macro\n",
    "axes[0, 2].barh(iris_results_df['model_type'], iris_results_df['f1_macro'], color='lightcoral')\n",
    "axes[0, 2].set_xlabel('F1-Score (Macro)')\n",
    "axes[0, 2].set_title('F1-Score Macro por Modelo')\n",
    "axes[0, 2].set_xlim(0, 1)\n",
    "axes[0, 2].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# 4. AUC OvR\n",
    "auc_ovr_data = iris_results_df[iris_results_df['auc_ovr'] > 0]\n",
    "if not auc_ovr_data.empty:\n",
    "    axes[1, 0].barh(auc_ovr_data['model_type'], auc_ovr_data['auc_ovr'], color='gold')\n",
    "    axes[1, 0].set_xlabel('AUC (One-vs-Rest)')\n",
    "    axes[1, 0].set_title('AUC OvR por Modelo')\n",
    "    axes[1, 0].set_xlim(0, 1)\n",
    "    axes[1, 0].grid(axis='x', alpha=0.3)\n",
    "else:\n",
    "    axes[1, 0].text(0.5, 0.5, 'No hay datos AUC', ha='center', va='center')\n",
    "    axes[1, 0].set_title('AUC OvR por Modelo')\n",
    "\n",
    "# 5. Cross-Validation Mean\n",
    "axes[1, 1].barh(iris_results_df['model_type'], iris_results_df['cv_mean'], color='mediumpurple')\n",
    "axes[1, 1].set_xlabel('CV Mean Accuracy')\n",
    "axes[1, 1].set_title('Cross-Validation Mean por Modelo')\n",
    "axes[1, 1].set_xlim(0, 1)\n",
    "axes[1, 1].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# 6. Scatter plot: Accuracy vs F1-Score\n",
    "scatter = axes[1, 2].scatter(iris_results_df['accuracy'], iris_results_df['f1_weighted'], \n",
    "                           s=100, alpha=0.7, c=range(len(iris_results_df)), cmap='viridis')\n",
    "axes[1, 2].set_xlabel('Accuracy')\n",
    "axes[1, 2].set_ylabel('F1-Score (Weighted)')\n",
    "axes[1, 2].set_title('Accuracy vs F1-Score')\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# AÃ±adir etiquetas a los puntos\n",
    "for i, model in enumerate(iris_results_df['model_type']):\n",
    "    axes[1, 2].annotate(model.split()[0][:3], \n",
    "                       (iris_results_df.iloc[i]['accuracy'], iris_results_df.iloc[i]['f1_weighted']),\n",
    "                       xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('iris_model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Registrar visualizaciÃ³n en MLflow\n",
    "with mlflow.start_run(run_name=\"Iris Model Comparison Summary\"):\n",
    "    mlflow.log_artifact('iris_model_comparison.png')\n",
    "    mlflow.log_metric(\"total_iris_models\", len(iris_results_df))\n",
    "    mlflow.log_metric(\"best_iris_accuracy\", iris_results_df['accuracy'].max())\n",
    "    mlflow.log_metric(\"avg_iris_accuracy\", iris_results_df['accuracy'].mean())\n",
    "    mlflow.log_param(\"best_iris_model\", best_iris_model['model_type'])\n",
    "    mlflow.log_param(\"dataset\", \"Iris\")\n",
    "    mlflow.log_param(\"problem_type\", \"multiclass_classification\")\n",
    "\n",
    "print(\"ðŸ“Š Visualizaciones guardadas en MLflow\")\n",
    "print(\"ðŸŽ¯ Resumen registrado como run separado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03859ae0",
   "metadata": {},
   "source": [
    "## Experimento con hiperparÃ¡metros - K-Nearest Neighbors\n",
    "\n",
    "KNN es particularmente interesante con Iris ya que las especies estÃ¡n bien separadas en el espacio de caracterÃ­sticas. Vamos a experimentar con diferentes valores de K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbc1084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimentar con diferentes configuraciones de KNN para Iris\n",
    "print(\"ðŸ”¬ Experimentando con K-Nearest Neighbors en Iris...\\n\")\n",
    "\n",
    "# Diferentes configuraciones de KNN\n",
    "knn_configs = [\n",
    "    {\"n_neighbors\": 1, \"weights\": \"uniform\"},\n",
    "    {\"n_neighbors\": 3, \"weights\": \"uniform\"},\n",
    "    {\"n_neighbors\": 5, \"weights\": \"uniform\"},\n",
    "    {\"n_neighbors\": 7, \"weights\": \"uniform\"},\n",
    "    {\"n_neighbors\": 9, \"weights\": \"uniform\"},\n",
    "    {\"n_neighbors\": 3, \"weights\": \"distance\"},\n",
    "    {\"n_neighbors\": 5, \"weights\": \"distance\"},\n",
    "    {\"n_neighbors\": 7, \"weights\": \"distance\"},\n",
    "    {\"n_neighbors\": 11, \"weights\": \"uniform\"},\n",
    "    {\"n_neighbors\": 15, \"weights\": \"uniform\"}\n",
    "]\n",
    "\n",
    "knn_results = []\n",
    "\n",
    "for i, config in enumerate(knn_configs):\n",
    "    model = KNeighborsClassifier(**config)\n",
    "    model_name = f\"KNN - K={config['n_neighbors']}, weights={config['weights']}\"\n",
    "    \n",
    "    print(f\"ðŸ”„ Probando {model_name}...\")\n",
    "    \n",
    "    # Entrenar y registrar\n",
    "    trained_model = train_and_log_iris_model(\n",
    "        model, model_name,\n",
    "        X_train_iris_scaled, X_test_iris_scaled,\n",
    "        y_train_iris, y_test_iris, config\n",
    "    )\n",
    "    \n",
    "    # Guardar resultados para anÃ¡lisis\n",
    "    accuracy = accuracy_score(y_test_iris, trained_model.predict(X_test_iris_scaled))\n",
    "    knn_results.append({\n",
    "        'k': config['n_neighbors'],\n",
    "        'weights': config['weights'],\n",
    "        'accuracy': accuracy,\n",
    "        'model_name': model_name\n",
    "    })\n",
    "\n",
    "print(\"\\nâœ… Experimentos KNN completados!\")\n",
    "\n",
    "# AnÃ¡lisis de resultados KNN\n",
    "knn_df = pd.DataFrame(knn_results)\n",
    "print(f\"\\nðŸ“Š Resultados de experimentos KNN:\")\n",
    "print(knn_df.sort_values('accuracy', ascending=False).to_string(index=False))\n",
    "\n",
    "# Encontrar mejor configuraciÃ³n\n",
    "best_knn = knn_df.loc[knn_df['accuracy'].idxmax()]\n",
    "print(f\"\\nðŸ¥‡ Mejor configuraciÃ³n KNN:\")\n",
    "print(f\"ðŸ“Š K = {best_knn['k']}, Weights = {best_knn['weights']}\")\n",
    "print(f\"ðŸ“Š Accuracy = {best_knn['accuracy']:.4f}\")\n",
    "\n",
    "# Visualizar resultados KNN\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Subplot 1: Accuracy vs K para different weights\n",
    "plt.subplot(2, 2, 1)\n",
    "uniform_data = knn_df[knn_df['weights'] == 'uniform']\n",
    "distance_data = knn_df[knn_df['weights'] == 'distance']\n",
    "\n",
    "plt.plot(uniform_data['k'], uniform_data['accuracy'], 'o-', label='uniform', linewidth=2, markersize=8)\n",
    "plt.plot(distance_data['k'], distance_data['accuracy'], 's-', label='distance', linewidth=2, markersize=8)\n",
    "plt.xlabel('K (nÃºmero de vecinos)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('KNN: Accuracy vs K')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Subplot 2: Bar plot de todas las configuraciones\n",
    "plt.subplot(2, 2, 2)\n",
    "colors = ['lightblue' if w == 'uniform' else 'lightcoral' for w in knn_df['weights']]\n",
    "bars = plt.bar(range(len(knn_df)), knn_df['accuracy'], color=colors)\n",
    "plt.xlabel('ConfiguraciÃ³n')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy por ConfiguraciÃ³n KNN')\n",
    "plt.xticks(range(len(knn_df)), [f\"K={row['k']}\\n{row['weights']}\" for _, row in knn_df.iterrows()], \n",
    "           rotation=45, ha='right')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# AÃ±adir leyenda de colores\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [Patch(facecolor='lightblue', label='uniform'),\n",
    "                  Patch(facecolor='lightcoral', label='distance')]\n",
    "plt.legend(handles=legend_elements, title='Weights')\n",
    "\n",
    "# Subplot 3: Heatmap de accuracy\n",
    "plt.subplot(2, 2, 3)\n",
    "pivot_data = knn_df.pivot(index='weights', columns='k', values='accuracy')\n",
    "sns.heatmap(pivot_data, annot=True, cmap='YlOrRd', fmt='.3f', cbar_kws={'label': 'Accuracy'})\n",
    "plt.title('Heatmap: Accuracy por K y Weights')\n",
    "plt.xlabel('K (nÃºmero de vecinos)')\n",
    "plt.ylabel('Weights')\n",
    "\n",
    "# Subplot 4: Box plot comparando weights\n",
    "plt.subplot(2, 2, 4)\n",
    "uniform_acc = knn_df[knn_df['weights'] == 'uniform']['accuracy']\n",
    "distance_acc = knn_df[knn_df['weights'] == 'distance']['accuracy']\n",
    "plt.boxplot([uniform_acc, distance_acc], labels=['uniform', 'distance'])\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('DistribuciÃ³n de Accuracy por Weights')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('iris_knn_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Registrar anÃ¡lisis KNN en MLflow\n",
    "with mlflow.start_run(run_name=\"Iris KNN Hyperparameter Analysis\"):\n",
    "    mlflow.log_artifact('iris_knn_analysis.png')\n",
    "    mlflow.log_metric(\"best_knn_accuracy\", best_knn['accuracy'])\n",
    "    mlflow.log_param(\"best_k\", best_knn['k'])\n",
    "    mlflow.log_param(\"best_weights\", best_knn['weights'])\n",
    "    mlflow.log_param(\"total_knn_configs\", len(knn_configs))\n",
    "    mlflow.log_metric(\"knn_accuracy_mean\", knn_df['accuracy'].mean())\n",
    "    mlflow.log_metric(\"knn_accuracy_std\", knn_df['accuracy'].std())\n",
    "\n",
    "print(\"ðŸ“Š AnÃ¡lisis KNN guardado en MLflow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251c401e",
   "metadata": {},
   "source": [
    "## ComparaciÃ³n final: PrÃ©stamos vs Iris\n",
    "\n",
    "Comparemos los resultados obtenidos en ambos datasets para entender las diferencias entre problemas de clasificaciÃ³n binaria y multiclase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a63c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ComparaciÃ³n de experimentos: PrÃ©stamos vs Iris\n",
    "print(\"ðŸ”„ Recopilando resultados de ambos experimentos...\")\n",
    "\n",
    "# Obtener runs del experimento de prÃ©stamos\n",
    "loan_experiment = mlflow.get_experiment_by_name(\"Loan Default Classification\")\n",
    "loan_runs = client.search_runs(loan_experiment.experiment_id)\n",
    "\n",
    "# Recopilar datos de prÃ©stamos (excluyendo runs de resumen)\n",
    "loan_data = []\n",
    "for run in loan_runs:\n",
    "    if \"Summary\" not in run.data.tags.get('mlflow.runName', ''):\n",
    "        metrics = run.data.metrics\n",
    "        params = run.data.params\n",
    "        \n",
    "        loan_data.append({\n",
    "            'dataset': 'PrÃ©stamos',\n",
    "            'model_type': params.get('model_type', 'Unknown'),\n",
    "            'accuracy': metrics.get('accuracy', 0),\n",
    "            'f1_score': metrics.get('f1_score', 0),\n",
    "            'problem_type': 'Binary Classification'\n",
    "        })\n",
    "\n",
    "# Recopilar datos de Iris (excluyendo runs de resumen y KNN especÃ­ficos)\n",
    "iris_data = []\n",
    "for run in iris_runs:\n",
    "    if \"Summary\" not in run.data.tags.get('mlflow.runName', '') and \"KNN -\" not in run.data.tags.get('mlflow.runName', ''):\n",
    "        metrics = run.data.metrics\n",
    "        params = run.data.params\n",
    "        \n",
    "        iris_data.append({\n",
    "            'dataset': 'Iris',\n",
    "            'model_type': params.get('model_type', 'Unknown'),\n",
    "            'accuracy': metrics.get('accuracy', 0),\n",
    "            'f1_score': metrics.get('f1_weighted', 0),  # Usar f1_weighted para Iris\n",
    "            'problem_type': 'Multiclass Classification'\n",
    "        })\n",
    "\n",
    "# Combinar datos\n",
    "all_data = loan_data + iris_data\n",
    "comparison_df = pd.DataFrame(all_data)\n",
    "\n",
    "print(f\"ðŸ“Š Datos recopilados:\")\n",
    "print(f\"   - PrÃ©stamos: {len(loan_data)} modelos\")\n",
    "print(f\"   - Iris: {len(iris_data)} modelos\")\n",
    "print(f\"   - Total: {len(all_data)} modelos\")\n",
    "\n",
    "# EstadÃ­sticas por dataset\n",
    "print(\"\\nðŸ“ˆ EstadÃ­sticas por dataset:\")\n",
    "stats_by_dataset = comparison_df.groupby('dataset').agg({\n",
    "    'accuracy': ['mean', 'std', 'min', 'max'],\n",
    "    'f1_score': ['mean', 'std', 'min', 'max']\n",
    "}).round(4)\n",
    "\n",
    "print(stats_by_dataset)\n",
    "\n",
    "# EstadÃ­sticas por tipo de modelo\n",
    "print(\"\\nðŸ¤– EstadÃ­sticas por tipo de modelo:\")\n",
    "stats_by_model = comparison_df.groupby('model_type').agg({\n",
    "    'accuracy': ['mean', 'std', 'count'],\n",
    "    'f1_score': ['mean', 'std']\n",
    "}).round(4)\n",
    "\n",
    "print(stats_by_model)\n",
    "\n",
    "# VisualizaciÃ³n comparativa\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('ðŸ“Š ComparaciÃ³n: PrÃ©stamos vs Iris', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Box plot de accuracy por dataset\n",
    "axes[0, 0].boxplot([comparison_df[comparison_df['dataset'] == 'PrÃ©stamos']['accuracy'],\n",
    "                   comparison_df[comparison_df['dataset'] == 'Iris']['accuracy']], \n",
    "                   labels=['PrÃ©stamos\\n(Binary)', 'Iris\\n(Multiclass)'])\n",
    "axes[0, 0].set_ylabel('Accuracy')\n",
    "axes[0, 0].set_title('DistribuciÃ³n de Accuracy por Dataset')\n",
    "axes[0, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 2. Box plot de F1-score por dataset\n",
    "axes[0, 1].boxplot([comparison_df[comparison_df['dataset'] == 'PrÃ©stamos']['f1_score'],\n",
    "                   comparison_df[comparison_df['dataset'] == 'Iris']['f1_score']], \n",
    "                   labels=['PrÃ©stamos\\n(Binary)', 'Iris\\n(Multiclass)'])\n",
    "axes[0, 1].set_ylabel('F1-Score')\n",
    "axes[0, 1].set_title('DistribuciÃ³n de F1-Score por Dataset')\n",
    "axes[0, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 3. Scatter plot: Accuracy vs F1-Score coloreado por dataset\n",
    "loan_subset = comparison_df[comparison_df['dataset'] == 'PrÃ©stamos']\n",
    "iris_subset = comparison_df[comparison_df['dataset'] == 'Iris']\n",
    "\n",
    "axes[1, 0].scatter(loan_subset['accuracy'], loan_subset['f1_score'], \n",
    "                  alpha=0.7, s=100, label='PrÃ©stamos', color='lightcoral')\n",
    "axes[1, 0].scatter(iris_subset['accuracy'], iris_subset['f1_score'], \n",
    "                  alpha=0.7, s=100, label='Iris', color='lightblue')\n",
    "axes[1, 0].set_xlabel('Accuracy')\n",
    "axes[1, 0].set_ylabel('F1-Score')\n",
    "axes[1, 0].set_title('Accuracy vs F1-Score')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Bar plot de accuracy promedio por modelo\n",
    "model_accuracy = comparison_df.groupby('model_type')['accuracy'].mean().sort_values(ascending=True)\n",
    "colors = ['lightcoral' if 'Logistic' in model else 'lightblue' if 'Random' in model \n",
    "          else 'lightgreen' if 'SVM' in model else 'gold' if 'Naive' in model \n",
    "          else 'mediumpurple' for model in model_accuracy.index]\n",
    "\n",
    "axes[1, 1].barh(model_accuracy.index, model_accuracy.values, color=colors)\n",
    "axes[1, 1].set_xlabel('Accuracy Promedio')\n",
    "axes[1, 1].set_title('Accuracy Promedio por Tipo de Modelo')\n",
    "axes[1, 1].grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('loans_vs_iris_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# AnÃ¡lisis de diferencias\n",
    "print(\"\\nðŸ” AnÃ¡lisis de diferencias:\")\n",
    "loan_avg_acc = comparison_df[comparison_df['dataset'] == 'PrÃ©stamos']['accuracy'].mean()\n",
    "iris_avg_acc = comparison_df[comparison_df['dataset'] == 'Iris']['accuracy'].mean()\n",
    "loan_avg_f1 = comparison_df[comparison_df['dataset'] == 'PrÃ©stamos']['f1_score'].mean()\n",
    "iris_avg_f1 = comparison_df[comparison_df['dataset'] == 'Iris']['f1_score'].mean()\n",
    "\n",
    "print(f\"ðŸ“Š Accuracy promedio - PrÃ©stamos: {loan_avg_acc:.4f}, Iris: {iris_avg_acc:.4f}\")\n",
    "print(f\"ðŸ“Š F1-Score promedio - PrÃ©stamos: {loan_avg_f1:.4f}, Iris: {iris_avg_f1:.4f}\")\n",
    "print(f\"ðŸ“Š Diferencia de Accuracy: {abs(iris_avg_acc - loan_avg_acc):.4f}\")\n",
    "print(f\"ðŸ“Š Diferencia de F1-Score: {abs(iris_avg_f1 - loan_avg_f1):.4f}\")\n",
    "\n",
    "if iris_avg_acc > loan_avg_acc:\n",
    "    print(\"ðŸŽ¯ Iris tiende a tener mayor accuracy (dataset mÃ¡s 'fÃ¡cil')\")\n",
    "else:\n",
    "    print(\"ðŸŽ¯ PrÃ©stamos tiende a tener mayor accuracy\")\n",
    "\n",
    "# Mejores modelos por dataset\n",
    "best_loan_model = comparison_df[comparison_df['dataset'] == 'PrÃ©stamos'].loc[\n",
    "    comparison_df[comparison_df['dataset'] == 'PrÃ©stamos']['accuracy'].idxmax()]\n",
    "best_iris_model = comparison_df[comparison_df['dataset'] == 'Iris'].loc[\n",
    "    comparison_df[comparison_df['dataset'] == 'Iris']['accuracy'].idxmax()]\n",
    "\n",
    "print(f\"\\nðŸ¥‡ Mejor modelo para PrÃ©stamos: {best_loan_model['model_type']} ({best_loan_model['accuracy']:.4f})\")\n",
    "print(f\"ðŸ¥‡ Mejor modelo para Iris: {best_iris_model['model_type']} ({best_iris_model['accuracy']:.4f})\")\n",
    "\n",
    "# Registrar comparaciÃ³n en MLflow\n",
    "with mlflow.start_run(run_name=\"Dataset Comparison: Loans vs Iris\"):\n",
    "    mlflow.log_artifact('loans_vs_iris_comparison.png')\n",
    "    mlflow.log_metric(\"loan_avg_accuracy\", loan_avg_acc)\n",
    "    mlflow.log_metric(\"iris_avg_accuracy\", iris_avg_acc)\n",
    "    mlflow.log_metric(\"loan_avg_f1\", loan_avg_f1)\n",
    "    mlflow.log_metric(\"iris_avg_f1\", iris_avg_f1)\n",
    "    mlflow.log_metric(\"accuracy_difference\", abs(iris_avg_acc - loan_avg_acc))\n",
    "    mlflow.log_param(\"total_models_compared\", len(comparison_df))\n",
    "    mlflow.log_param(\"best_loan_model\", best_loan_model['model_type'])\n",
    "    mlflow.log_param(\"best_iris_model\", best_iris_model['model_type'])\n",
    "\n",
    "print(\"\\nðŸ“Š ComparaciÃ³n guardada en MLflow\")\n",
    "print(\"ðŸŽ‰ Â¡AnÃ¡lisis completo finalizado!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
