   \documentclass[handout]{beamer}



   \mode<presentation>
   {
     \usetheme{PaloAlto}
   \setbeamertemplate{footline}[page number]

     \setbeamercolor{sidebar}{bg=white, fg=black}
     \setbeamercolor{frametitle}{bg=white, fg=black}
     % or ...
     \setbeamercolor{logo}{bg=white}
     \setbeamercolor{block body}{parent=normal text,bg=white}
     \setbeamercolor{author in sidebar}{fg=black}
     \setbeamercolor{title in sidebar}{fg=black}


     \setbeamercolor*{block title}{use=structure,fg=structure.fg,bg=structure.fg!20!bg}
     \setbeamercolor*{block title alerted}{use=alerted text,fg=alerted text.fg,bg=alerted text.fg!20!bg}
     \setbeamercolor*{block title example}{use=example text,fg=example text.fg,bg=example text.fg!20!bg}


     \setbeamercolor{block body}{parent=normal text,use=block title,bg=block title.bg!50!bg}
     \setbeamercolor{block body alerted}{parent=normal text,use=block title alerted,bg=block title alerted.bg!50!bg}
     \setbeamercolor{block body example}{parent=normal text,use=block title example,bg=block title example.bg!50!bg}

     % or ...

     \setbeamercovered{transparent}
     % or whatever (possibly just delete it)
     \logo{\resizebox{!}{1.5cm}{\href{\basename{R}}{\includegraphics{image}}}}
   }

   \mode<handout>
   {
     \usetheme{PaloAlto}
     \usecolortheme{default}
     \setbeamercolor{sidebar}{bg=white, fg=black}
     \setbeamercolor{frametitle}{bg=white, fg=black}
     % or ...
     \setbeamercolor{logo}{bg=white}
     \setbeamercolor{block body}{parent=normal text,bg=white}
     \setbeamercolor{author in sidebar}{fg=black}
     \setbeamercolor{title in sidebar}{fg=black}
     \setbeamercovered{transparent}
     % or whatever (possibly just delete it)
     \logo{}
   }

   \usepackage{epsdice,listings,epic}
   \usepackage[latin1]{inputenc}
   \usepackage{graphicx}
   \usepackage{amsmath,eepic,epic,algorithm}

   \newcommand{\figslide}[3]{
   \begin{frame}
   \frametitle{#1}
     \begin{center}
     \resizebox{!}{2.7in}{\includegraphics{#2}}    
     \end{center}
   {#3}
   \end{frame}
   }

   \newcommand{\fighslide}[4]{
   \begin{frame}
   \frametitle{#1}
     \begin{center}
     \resizebox{!}{#4}{\includegraphics{#2}}    
     \end{center}
   {#3}
   \end{frame}
   }

   \newcommand{\figwref}[1]{
   \href{#1}{\tiny \tt #1}}

   \newcommand{\unsupervised}[1]{{\color{red} #1}}
   \newcommand{\supervised}[1]{{\color{green} #1}}
   \newcommand{\argmax}{\mathop{\mathrm{argmax}}}
   \newcommand{\argmin}{\mathop{\mathrm{argmin}}}
   \newcommand{\minimize}{\mathop{\mathrm{minimize}}}
   \newcommand{\maximize}{\mathop{\mathrm{maximize}}}

   \newcommand{\B}[1]{\beta_{#1}}
   \newcommand{\Bh}[1]{\widehat{\beta}_{#1}}
   \newcommand{\V}{\text{Var}}
   \newcommand{\Cov}{\text{Cov}}
   \newcommand{\Vh}{\widehat{\V}}
   \newcommand{\s}{\sigma}
   \newcommand{\sh}{\widehat{\sigma}}

   \newcommand{\argmax}[1]{\mathop{\text{argmax}}_{#1}}
   \newcommand{\argmin}[1]{\mathop{\text{argmin}}_{#1}}
   \newcommand{\Ee}{\mathbb{E}}
   \newcommand{\Pp}{\mathbb{P}}
   \newcommand{\real}{\mathbb{R}}
   \newcommand{\Ybar}{\overline{Y}}
   \newcommand{\Yh}{\widehat{Y}}
   \newcommand{\Xbar}{\overline{X}}
   \newcommand{\Tr}{\text{Tr}}


   \newcommand{\model}{{\cal M}}

   \newcommand{\figvskip}{-0.7in}
   \newcommand{\fighskip}{-0.3in}
   \newcommand{\figheight}{3.5in}

   \newcommand{\Rcode}[1]{{\bf \tt #1 }}
   \newcommand{\Rtcode}[1]{{\tiny \bf \tt #1 }}
   \newcommand{\Rscode}[1]{{\small \bf \tt #1 }}

   \newcommand{\RR}{{\tt R} \;}
   \newcommand{\basename}[1]{http://stats191.stanford.edu/#1}
   \newcommand{\dataname}[1]{\basename{data/#1}}
   \newcommand{\Rname}[1]{\basename{R/#1}}

   \newcommand{\mycolor}[1]{{\color{blue} #1}}
   \newcommand{\basehref}[2]{\href{\basename{#1}}{\mycolor{#2}}}
   \newcommand{\Rhref}[2]{\href{\basename{R/#1}}{\mycolor{#2}}}
   \newcommand{\datahref}[2]{\href{\dataname{#1}}{\mycolor{#2}}}
   \newcommand{\X}{\pmb{X}}
   \newcommand{\Y}{\pmb{Y}}
   \newcommand{\be}{\pmb{varepsilon}}
   \newcommand{\logit}{\text{logit}}


   \title{Statistics 191: Introduction to Applied Statistics}
   \subtitle{Correlated Errors, Whitening  (Ch. 8, RABE)}
   \author{\copyright Jonathan Taylor \\
   }
   %}


   \begin{document}

   \begin{frame}
   \titlepage
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Correlated Erorrs}

   \begin{block}
   {Topics}
   \begin{itemize}

   \item Autocorrelation.


   \item Whitening.
   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Correlated errors}

   \begin{block}
   {Autocorrelation                     }

   \begin{itemize}

   \item In the random effects model, outcomes within groups
   were correlated.

   \item Other regression applications also have correlated outcomes (i.e. errors).

   \item Common examples: time series data.

   \item Why worry? Can lead to underestimates of SE $\rightarrow$ inflated $t$'s $\rightarrow$ false positives.

   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Correlated errors}

   \begin{block}
   {Autocorrelation                     }
   \begin{itemize}

   \item Suppose we plot Palo Alto's daily average temperature --
   clearly we would see a pattern in the data.
   \item Sometimes, this pattern can be attributed to a deterministic
   phenomenon (i.e. predictable seasonal fluctuations).

   \item Other times, ``patterns'' are due to correlations in the noise, maybe small time fluctuations in the stock market, economy, etc.



   \item Example: financial time series, monthly bond return.

   \item Example: residuals regressing consumer expenditure on money stock.
   \end{itemize}
   \end{block}
   \end{frame}

   %CODE
       % PA.temp <- read.table('http://www-stat.stanford.edu/~jtaylo/courses/stats191/data/paloaltoT.table', header=F, skip=2)
   % plot(PA.temp[,3], xlab='Day', ylab='Average Max Temp (F)', pch=23, bg='orange')


   \begin{frame}
   \frametitle{Average Max Temp in Palo Alto}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/4a013ee86e.png}}    
   \end{center}

   \end{frame}

   %CODE
       % url = 'http://stats191.stanford.edu/data/nasdaq_2011.csv'
   % nasdaq.data = read.table(url, header=TRUE, sep=',')
   % 
   % plot(nasdaq.data$Date, nasdaq.data$Close, xlab='Date', ylab='NASDAQ close',
   %      pch=23, bg='red', cex=2)


   \begin{frame}
   \frametitle{NASDAQ daily close 2011}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/40a1d58d2b.png}}    
   \end{center}
   \href{http://stats191.stanford.edu/correlated_errors.html#nasdag}{R code}
   \end{frame}

   %CODE
       % acf(nasdaq.data$Close)


   \begin{frame}
   \frametitle{NASDAQ daily close 2011, ACF}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/b8050fb529.png}}    
   \end{center}
   \href{http://stats191.stanford.edu/correlated_errors.html#nasdaq}{R code}
   \end{frame}

   %CODE
       % url = 'http://www-stat.stanford.edu/~jtaylo/courses/stats191/data/expenditure.table'
   % expenditure.table =read.table(url, header=T)
   % attach(expenditure.table)
   % 
   % plot(Stock, Expenditure, pch=23, bg='orange', cex=2)


   \begin{frame}
   \frametitle{Expenditure vs. stock}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/7f6229d84c.png}}    
   \end{center}
   \href{http://stats191.stanford.edu/correlated_errors.html#consumer-expenditure}{R code}
   \end{frame}

   %CODE
       % exp.lm = lm(Expenditure ~ Stock)
   % plot(resid(exp.lm), type='l', lwd=2, col='red')


   \begin{frame}
   \frametitle{Expenditure vs. stock: residuals}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/56669140a8.png}}    
   \end{center}
   \href{http://stats191.stanford.edu/correlated_errors.html#consumer-expenditure}{R code}
   \end{frame}

   %CODE
       % acf(resid(exp.lm))


   \begin{frame}
   \frametitle{ACF of residuals}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/25b26dc356.png}}    
   \end{center}
   \href{http://stats191.stanford.edu/correlated_errors.html#consumer-expenditure}{R code}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Correlated errors}

   \begin{block}
   {AR(1) noise}
   \begin{itemize}

   \item Suppose that, instead of being independent, the errors in our model were
   $$
   \varepsilon_t = \rho \cdot \varepsilon_{t-1} + \omega_t, \qquad -1 < \rho < 1$$
   with $\omega_t \sim N(0,\sigma^2)$ independent.
   \item If $\rho$ is close to 1, then errors are very correlated, $\rho=0$ is independence.
   \item This is ``Auto-Regressive Order (1)'' noise (AR(1)). Many other
   models of correlation exist: ARMA, ARIMA, ARCH, GARCH, etc.
   \end{itemize}

   \end{block}
   \end{frame}

   %CODE
       % ntrial = 500
   % nsample = 100
   % rho = 0.9
   % mu = 1.0
   % 
   % # This little function simulates our time series
   % 
   % get.sample = function() {
   %   return(arima.sim(list(ar=rho), nsample) + mu)
   % }
   % 
   % Z = get.sample()
   % plot(Z, lwd=2, col='red')


   \begin{frame}
   \frametitle{AR(1) noise, $\rho=0.9$}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/07d4c33fd2.png}}    
   \end{center}
   \href{http://stats191.stanford.edu/correlated_errors.html#simulating-time-series}{R code}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Correlated errors}

   \begin{block}
   {Autocorrelation function                     }
   \begin{itemize}
   \item       For a ``stationary'' time series $(Z_t)_{1 \leq t \leq \infty}$
   define
   $$
   ACF(t) = \text{ Cor}(Z_s, Z_{s+t}).$$

   \item Stationary means that correlation above does not depend on $s$.

   \item For AR(1) model,
   $$
   ACF(t) = \rho^t.$$
       \item For a sample $(Z_1, \dots, Z_n)$ from a stationary time series
   $$
   \widehat{ACF}(t) = \frac{\sum_{j=1}^{n-t} (Z_j - \overline{Z})(Z_{t+j} - \overline{Z})}{\sum_{j=1}^n(Z_j - \overline{Z})^2}.$$

   \end{itemize}

   \end{block}
   \end{frame}

   %CODE
       % acf(Z)


   \begin{frame}
   \frametitle{ACF of AR(1) noise, $\rho=0.9$}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/6201fdd299.png}}    
   \end{center}
   \href{http://stats191.stanford.edu/correlated_errors.html#simulating-time-series}{R code}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Correlated errors}

   \begin{block}
   {Effects on inference}
   \begin{itemize}
   \item So far, we have just mentioned that things {\em may} be correlated, but not thought about how it affects inference.

   \item Suppose we are in the ``one sample problem'' setting and we observe
   $$W_i  = Z_i + \mu, \qquad 1 \leq i \leq n$$
   with the $Z_i$'s from an $AR(1)$ time series.
   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Correlated errors}

   \begin{block}
   {Effects on inference}
   \begin{itemize}
   \item It is easy to see that
   $$
   E(\overline{W}) = \mu$$
   {\em BUT}, generally
   $$
   \text{Var}(\overline{W}) >  \frac{\text{Var}(Z_1)}{n}$$
   how much bigger depends on $\rho.$
   \end{itemize}
   \end{block}
   \end{frame}

   %CODE
       % sample.mean = numeric(ntrial)
   % sample.var = numeric(ntrial)
   % 
   % for (i in 1:ntrial) {
   %   cur.sample = get.sample()
   %   sample.mean[i] = mean(cur.sample)
   %   sample.var[i] = var(cur.sample)
   % }
   % 
   % data.frame(mean=mean(sample.mean), sd=sqrt(mean(sample.var)))
   % 
   % xval = seq(-5,5,0.05)
   % Y = c(density(sample.mean)$y, dnorm(xval, mean=mean(sample.mean),
   %                   sd=sqrt(mean(sample.var) / nsample)))
   % X = c(density(sample.mean)$x, xval)
   % 
   % plot(X, Y, type='n', main='Actual and "expected" density of sample mean')
   % lines(xval, dnorm(xval, mean=mean(sample.mean),
   %                   sd=sqrt(mean(sample.var) / nsample)), lwd=2, col='blue')
   % lines(density(sample.mean), lwd=2, col='red')


   \begin{frame}
   \frametitle{Misleading inference ignoring autocorrelation}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/0c676d9a91.png}}    
   \end{center}
   \href{http://stats191.stanford.edu/correlated_errors.html#simulating-time-series}{R code}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Regression with auto-correlated errors}

   \begin{block}
   {Model}
   \begin{itemize}
   \item Observations:
   $$
   Y_t = \beta_0 + \sum_{j=1}^p X_{tj} \beta_j + \varepsilon_t, \qquad 1 \leq t \leq n$$

   \item Errors:
   $$
   \varepsilon_t = \rho \cdot \varepsilon_{t-1} + \omega_t, \qquad -1 < \rho < 1$$

   \item Question: how do we determine if autocorrelation is present?

   \item Question: what do we do to correct for autocorrelation?
   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Regression with auto-correlated errors}

   \begin{block}
   {Graphical checks for autocorrelation }
   \begin{itemize}

   \item A plot of residuals vs. time is helpful.

   \item Residuals clustered above and below 0 line can indicate
   autocorrelation.

   \item Example: regressing consumer expenditure on money stock.

   \end{itemize}
   \end{block}
   \end{frame}

   %CODE
       % exp.lm = lm(Expenditure ~ Stock)
   % plot(resid(exp.lm), type='l', lwd=2, col='red')


   \begin{frame}
   \frametitle{Expenditure vs. stock: residuals}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/56669140a8.png}}    
   \end{center}
   \href{http://stats191.stanford.edu/correlated_errors.html#consumer-expenditure}{R code}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Regression with auto-correlated errors}

   \begin{block}
   {Durbin-Watson   test }

   \begin{itemize}

   \item In regression setting, if noise is AR(1), a simple estimate of $\rho$ is obtained
   by (essentially) regressing $e_t$ onto $e_{t-1}$
   $$
   \widehat{\rho} = \frac{\sum_{t=2}^n \left(e_t e_{t-1}\right)}{\sum_{t=1}^n e_t^2}.$$

   \item To formally test $H_0:\rho=0$ (i.e. whether residuals are independent vs. they are AR(1)), use Durbin-Watson test, based on
   $$
   d \approx 2(1 - \widehat{\rho}).$$
   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Regression with auto-correlated errors}

   \begin{block}
   {Correcting for AR(1)                      }
   \begin{itemize}
   \item Suppose we know $\rho$, if we ``whiten'' the data and regressors
   $$
   \begin{aligned}
   \tilde{Y}_{t+1} &= Y_{t+1} - \rho Y_t, t > 1   \\
   \tilde{X}_{(t+1)j} &= X_{(t+1)j} - \rho X_{tj}, i > 1
   \end{aligned}
   $$
   for $1 \leq t \leq n-1$.
   This model satisfies ``usual'' assumptions, i.e. the errors
   $$
   \tilde{\varepsilon}_t = \omega_{t+1} = \varepsilon_{t+1} - \rho \cdot \varepsilon_t$$
   are independent $N(0,\sigma^2)$.

   \item For coefficients in new model $\tilde{\beta}$, $\beta_0 = \tilde{\beta}_0 / (1 - \rho)$, $\beta_j = \tilde{\beta}_j.$

   \item Problem: in general, we don't know $\rho$.
   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Regression with auto-correlated errors}

   \begin{block}
   {Two-stage regression}
   \begin{itemize}
   \item Step 1: Fit linear model to unwhitened data (OLS: ordinary least squares, i.e. no pre-whitening).

   \item Step 2: Estimate $\rho$ with $\widehat{\rho}$.


   \item Step 3: Pre-whiten data using $\widehat{\rho}$ -- refit the model.
   \end{itemize}
   \end{block}
   \begin{block}
   {Other models of covariance}
   \begin{itemize}
   \item Suppose we model covariance of $\varepsilon$'s differently, i.e.
   $ARMA(p,q)$.

   \item As long as we can estimate parameters of covariance, from residuals
   of the OLS fit, we can use this two-stage procedure.

   \item This is very similar to weighted least squares.
   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Regression with auto-correlated errors}

   \begin{block}
   {Interpreting results of two-stage fit}
   \begin{itemize}
   \item Basically, interpretation is unchanged, but the exact
   degrees of freedom in the error is not exactly clear.

   \item Common argument: ``this works for large degrees of freedom, so we better hope we have enough degrees of freedom so this point is not important.''


   \item Can treat $t$-statistics as $Z$-statistics, $F$'s as $\chi^2$, appealing to asymptotics:
   \begin{itemize}
   \item $t_{\nu}$, with $\nu$ large is like $N(0,1)$;

   \item $F_{j,\nu}$, with $\nu$ large is like $\chi^2_j/j.$
   \end{itemize}
   \end{itemize}
   \end{block}
   \end{frame}

   %CODE
       % library(car) # durbin.watson is in the "car" package
   % durbin.watson(exp.lm)
   % 
   % rho = durbin.watson(exp.lm)$r
   % 
   % # whiten the data "by hand"
   % 
   % wExp = numeric(length(Expenditure) - 1)
   % wStock = numeric(length(Expenditure) - 1)
   % 
   % for (i in 2:length(Expenditure)) {
   %   wExp[i-1] = Expenditure[i] - rho * Expenditure[i-1]
   %   wStock[i-1] = Stock[i] - rho * Stock[i-1]
   % }
   % 
   % 
   % 
   % exp.whitened.lm = lm(wExp ~ wStock)
   % summary(exp.whitened.lm)
   % summary(exp.lm)
   % 
   % 
   % plot(resid(exp.whitened.lm), type='l', lwd=2, col='red')


   \begin{frame}
   \frametitle{Expenditure vs. stock: whitened residuals}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/1970cc2a75.png}}    
   \end{center}
   \href{http://stats191.stanford.edu/correlated_errors.html#consumer-expenditure}{R code}
   \end{frame}

   %CODE
       % acf(resid(exp.whitened.lm))


   \begin{frame}
   \frametitle{Expenditure vs. stock: ACF of whitened residuals}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/234922b033.png}}    
   \end{center}
   \href{http://stats191.stanford.edu/correlated_errors.html#consumer-expenditure}{R code}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} 

   \end{frame}

   \end{document}
