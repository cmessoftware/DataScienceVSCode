{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load_ext rmagic"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Final review"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "Overview\n",
      "\n",
      "* Simple linear regression.\n",
      "* Diagnostics for simple linear regression.\n",
      "* Multiple linear regression.\n",
      "* Diagnostics.\n",
      "* Interactions and ANOVA.\n",
      "* Weighted Least Squares.\n",
      "* Autocorrelation.\n",
      "* Model selection.\n",
      "* Logistic regression.\n",
      "* Poisson regression."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Simple linear regression"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from matplotlib import rc\n",
      "import pylab, numpy as np, sys\n",
      "np.random.seed(0);import random; random.seed(0)\n",
      "sys.path.append('/private/var/folders/dq/4_9bwd013ln6vvf_q110mwrh0000gn/T/tmpw7WwaI')\n",
      "f=pylab.gcf(); f.set_size_inches(8.0,6.0)\n",
      "datadir ='/private/var/folders/dq/4_9bwd013ln6vvf_q110mwrh0000gn/T/tmpw7WwaI/data'\n",
      "import matplotlib.mlab as ML\n",
      "H = ML.csv2rec('%s/pearson_lee.csv' % datadir)\n",
      "M = H['mother']\n",
      "D = H['daughter']\n",
      "pylab.scatter(M, D, c='red')\n",
      "x = 62\n",
      "xf, yf = pylab.poly_between([x-.5,x+.5], [50,50], [75, 75])\n",
      "g = (M < x+.5) * (M >= x-.5)\n",
      "pylab.fill(xf, yf, facecolor='blue', alpha=0.2, hatch='/', label='_nolegend_')\n",
      "pylab.gca().set_xlabel(\"Mother's height (inches)\")\n",
      "pylab.gca().set_ylabel(\"Daughter's height (inches)\")\n",
      "s = pylab.scatter([x],D[g].mean(), s=130, c='yellow', marker='^')\n",
      "s.set_label('Average at %d' % int(x))\n",
      "Dbar = D.mean(); Dsd = np.sqrt(((D - Dbar)**2).mean())\n",
      "Mbar = M.mean(); Msd = np.sqrt(((M - Mbar)**2).mean())\n",
      "r = np.corrcoef([M, D])[0,1]\n",
      "pylab.plot([Mbar-3.5*Msd,Mbar,Mbar+3.5*Msd],\n",
      "           [Dbar-r*3.5*Dsd,Dbar,Dbar+r*3.5*Dsd], '-', linewidth=3, label='D on M', color='black')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Simple linear regression"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "Least squares\n",
      "\n",
      "* We will be using \"least squares\" regression. This measures the goodness of fit of a line by the sum of squared errors, $SSE$.\n",
      "* Least squares regression chooses the line that minimizes $SSE(\\beta_0, \\beta_1) = \\sum_{i=1}^n (Y_i - \\beta_0 - \\beta_1 \\cdot X_i)^2.$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Geometry of Least Squares: Simple Linear Model"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Simple linear regression"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "What is a $t$-statistic?\n",
      "\n",
      "* Start with $Z \\sim N(0,1)$ is standard normal and $S^2 \\sim \\chi^2_{\\nu}$, independent of $Z$.\n",
      "* Compute $T = \\frac{Z}{\\sqrt{\\frac{S^2}{\\nu}}}.$\n",
      "* Then, $T \\sim t_{\\nu}$ has a $t$-distribution with $\\nu$ degrees of freedom.\n",
      "* Generally, a $t$-statistic has the form $$ T = $$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Simple linear regression"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "Interval for $\\beta_1$\n",
      " A $(1-\\alpha) \\cdot 100 \\%$ confidence interval: $\\widehat{\\beta}_1 \\pm SE(\\widehat{\\beta}_1) \\cdot t_{n-2, 1-\\alpha/2}.$\n",
      "Interval for regression line $\\beta_0 + \\beta_1 \\cdot X$\n",
      "\n",
      "* $(1-\\alpha) \\cdot 100 \\%$ confidence interval: $\\widehat{\\beta}_0 + \\widehat{\\beta}_1 X \\pm SE(\\widehat{\\beta}_0 + \\widehat{\\beta}_1 X) \\cdot t_{n-2, 1-\\alpha/2}$ where $SE(a_0\\widehat{\\beta}_0 + a_1\\widehat{\\beta}_1) = \\widehat{\\sigma} \\sqrt{\\frac{a_0^2}{n} + \\frac{(a_0\\overline{X} - a_1)^2}{\\sum_{i=1}^n \\left(X_i-\\overline{X}\\right)^2}}$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Simple linear regression"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "Predicting a new observation\n",
      "\n",
      "* $SE(\\widehat{\\beta}_0 + \\widehat{\\beta}_1 X_{\\text{new}} + \\varepsilon_{\\text{new}}) = \\widehat{\\sigma} \\sqrt{1 + \\frac{1}{n} + \\frac{(\\overline{X} - X_{\\text{new}})^2}{\\sum_{i=1}^n \\left(X_i-\\overline{X}\\right)^2}}.$\n",
      "* Prediction interval is $\\widehat{\\beta}_0 +  \\widehat{\\beta}_1 X_{\\text{new}} \\pm t_{n-2, 1-\\alpha/2} \\cdot SE(\\widehat{\\beta}_0 + \\widehat{\\beta}_1 X_{\\text{new}} + \\varepsilon_{\\text{new}})$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Simple linear diagnostics"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "Sums of squares\n",
      " $\\begin{aligned}\n",
      "   SSE &= \\sum_{i=1}^n(Y_i - \\widehat{Y}_i)^2 = \\sum_{i=1}^n (Y_i - \\widehat{\\beta}_0 - \\widehat{\\beta}_1 X_i)^2 \\\\\n",
      "   SSR &= \\sum_{i=1}^n(\\overline{Y} - \\widehat{Y}_i)^2 = \\sum_{i=1}^n (\\overline{Y} - \\widehat{\\beta}_0 - \\widehat{\\beta}_1 X_i)^2 \\\\\n",
      "   SST &= \\sum_{i=1}^n(Y_i - \\overline{Y})^2 = SSE + SSR \\\\\n",
      "   R^2 &= \\frac{SSR}{SST} = 1 - \\frac{SSE}{SST} = \\widehat{Cor}(\\pmb{X},\\pmb{Y})^2.\n",
      "   \\end{aligned}$\n",
      "Basic idea: if $R^2$ is large: a lot of the variability in $\\pmb{Y}$ is explained by $\\pmb{X}$."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Simple linear diagnostics"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "$F$-test in simple linear regression\n",
      "\n",
      "* *Full (bigger) model :*\n",
      "   $FM: \\qquad Y_i = \\beta_0 + \\beta_1 X_i + \\varepsilon_i$\n",
      "* *Reduced (smaller) model:*\n",
      "   $RM: \\qquad Y_i = \\beta_0  + \\varepsilon_i$\n",
      "* The $F$-statistic has the form $F=\\frac{(SSE(RM) - SSE(FM)) / (df_{RM} - df_{FM})}{SSE(FM) / df_{FM}}.$\n",
      "* Reject $H_0: RM$ is correct, if $F > F_{1-\\alpha, 1, n-2}$."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Simple linear diagnostics"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "What are the assumptions\n",
      "\n",
      "* $Y_i = \\beta_0 + \\beta_1 X_{i} + \\varepsilon_i$\n",
      "* Errors $\\varepsilon_i$ are assumed independent $N(0,\\sigma^2)$."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Residuals from linear model"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%R\n",
      "\n",
      "plot(anscombe$X2, resid(simple.lm), ylab='Residual', xlab='X',\n",
      "     pch=23, bg='orange', cex=2)\n",
      "\n",
      "## put a horizontal line through 0\n",
      "abline(h=0, lwd=2, col='red', lty=2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Quadratic model"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%R\n",
      "\n",
      "# Improve model by adding quadratic term\n",
      "# Instead of attaching, can use the \"data\" argument to lm\n",
      "\n",
      "quadratic.lm <- lm(Y2 ~ poly(X2, 2), data=anscombe)\n",
      "\n",
      "# Replot data, adding fitted quadratic\n",
      "Xsort <- sort(anscombe$X2)\n",
      "plot(anscombe$X2, anscombe$Y2, pch=23, bg='orange', cex=2, ylab='Y', xlab='X')\n",
      "lines(Xsort, predict(quadratic.lm, list(X2=Xsort)), col='red', lty=2, lwd=2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Residuals from quadratic model"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%R\n",
      "\n",
      "plot(anscombe$X2, resid(quadratic.lm), ylab='Residual',\n",
      "     xlab='X', pch=23, bg='orange', cex=2)\n",
      "## put a horizontal line through 0\n",
      "abline(h=0, lwd=2, col='red', lty=2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "QQplot of residuals from quadratic model"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%R\n",
      "qqnorm(resid(quadratic.lm), pch=23, bg='orange', cex=2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Simple linear diagnostics"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%R\n",
      "plot(GSS[good], resid(viral.lm), pch=23,\n",
      "bg='orange', cex=2, xlab='GSS', ylab='Residual')\n",
      "abline(h=0, lwd=2, col='red', lty=2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Multiple linear regression model"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "Specifying the model\n",
      "\n",
      "* Rather than one predictor, we have $p=6$ predictors.\n",
      "* $Y_i = \\beta_0 + \\beta_1 X_{i1} + \\dots + \\beta_p X_{ip} + \\varepsilon_i$\n",
      "* Errors $\\varepsilon$ are assumed independent $N(0,\\sigma^2)$, as in simple linear regression.\n",
      "* Coefficients are called (partial) regression coefficients because they \"allow\" for the effect of other variables."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Geometry of Least Squares: Multiple Regression"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Multiple linear regression"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "$F$-test\n",
      "\n",
      "* *Full (bigger) model :*\n",
      "   $Y_i = \\beta_0 + \\beta_1 X_{i1} + \\dots \\beta_p X_{ip} + \\varepsilon_i$\n",
      "* *Reduced (smaller) model:*\n",
      "   $Y_i = \\beta_0  + \\varepsilon_i$\n",
      "* The $F$-statistic has the form $F=\\frac{(SSE(R) - SSE(F)) / (df_R - df_F)}{SSE(F) / df_F}.$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Multiple linear regression"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "Matrix formulation\n",
      " ${\\pmb Y}_{n \\times 1} = \\pmb{X}_{n \\times (p + 1)} \\pmb{\\beta}_{(p+1) \\times 1} + \\pmb{\\varepsilon}_{n \\times 1}$\n",
      "* $\\pmb{X}$ is called the *design matrix*\n",
      "   of the model\n",
      "* $\\pmb{\\varepsilon} \\sim N(0, \\sigma^2 I_{n \\times n})$ is multivariate normal\n",
      "$SSE$ in matrix form\n",
      " $SSE(\\beta) = (\\pmb{Y} - \\pmb{X} \\pmb{\\beta})'(\\pmb{Y} - \\pmb{X} \\pmb{\\beta})$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Multiple linear regression"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "Least squares solution\n",
      "\n",
      "* Normal equations $\\frac{\\partial}{\\partial \\beta_j} SSE \\biggl|_{{\\widehat{\\beta}_{}}} = -2 \\left(\\pmb{Y} - \\pmb{X} {\\widehat{\\beta}_{}} \\right)^t \\pmb{X}_j = 0, \\qquad 0 \\leq j \\leq p.$\n",
      "* Equivalent to $\\begin{aligned}\n",
      "     (\\pmb{Y} - \\pmb{X}\\pmb{{\\widehat{\\beta}_{}}})^t\\pmb{X} &= 0 \\\\\n",
      "     \\pmb{{\\widehat{\\beta}_{}}} &= (\\pmb{X}^t\\pmb{X})^{-1}\\pmb{X}^t\\pmb{Y}\n",
      "     \\end{aligned}$\n",
      "* Properties: $$ ~N(, <sup>2</sup> (<sup>t</sup> )<sup>-1</sup> ), $$\n",
      "* [R code](http://stats191.stanford.edu/multiple.html)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Multiple linear regression"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "Confidence interval for $\\sum_{j=0}^p a_j \\beta_j$\n",
      "\n",
      "* Suppose we want a $(1-\\alpha)\\cdot 100\\%$ CI for $\\sum_{j=0}^p a_j\\beta_j$.\n",
      "* Just as in simple linear regression:\n",
      "* $\\sum_{j=0}^p a_j \\widehat{\\beta}_j \\pm t_{1-\\alpha/2, n-p-1} \\cdot SE\\left(\\sum_{j=0}^p a_j\\widehat{\\beta}_j\\right).$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Multiple linear regression"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      " General $F$-tests\n",
      "\n",
      "* Given two models $R \\subset F$ (i.e. $R$ is a subspace of $F$), we can consider testing $$ H<sup>0</sup>: $vs.$ H<sup>a</sup>: .$\\item The test statistic is$ F = $$\n",
      "* If $H_0$ is true, $F \\sim F_{df_R-df_F, df_F}$ so we reject $H_0$ at level $\\alpha$ if $F > F_{df_R-df_F, df_F, 1-\\alpha}$."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Diagnostics"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "What can go wrong?\n",
      "\n",
      "* Regression function can be wrong: maybe regression function should be quadratic (see [ R code\n",
      "  \n",
      "  ]({http://stats191.stanford.edu/R/simple_diagnostics/anscombeout.html})\n",
      "  ).\n",
      "* Model for the errors may be incorrect:\n",
      "*     * may not be normally distributed.\n",
      "      * may not be independent.\n",
      "      * may not have the same variance.\n",
      "* Detecting problems is more *art*\n",
      "   then *science*\n",
      "  , i.e. we cannot *test*\n",
      "   for all possible problems in a regression model.\n",
      "* Basic idea of diagnostic measures: if model is correct then residuals $e_i = Y_i -\\widehat{Y}_i, 1 \\leq i \\leq n$ should look like a sample of (not quite independent) $N(0, \\sigma^2)$ random variables."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Diagnostics"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%R\n",
      "library(car)\n",
      "\n",
      "url = 'http://stats191.stanford.edu/data/scottish_races.table'\n",
      "races.table = read.table(url, header=T)\n",
      "attach(races.table)\n",
      "races.lm = lm(Time ~ Distance + Climb)\n",
      "par(mfrow=c(2,2))\n",
      "plot(races.lm, pch=23 ,bg='orange',cex=2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Diagnostics"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "$DFFITS$\n",
      "\n",
      "* $DFFITS_i = \\frac{\\widehat{Y}_i - \\widehat{Y}_{i(i)}}{\\widehat{\\sigma}_{(i)} \\sqrt{H_{ii}}}$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Diagnostics"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "Cook\u2019s distance\n",
      "\n",
      "* $D_i = \\frac{\\sum_{j=1}^n(\\widehat{Y}_j - \\widehat{Y}_{j(i)})^2}{(p+1) \\, \\widehat{\\sigma}^2}$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Influence of an observation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "$DFBETAS$\n",
      "\n",
      "* $DFBETAS_{j(i)} = \\frac{\\widehat{\\beta}_j - \\widehat{\\beta}_{j(i)}}{\\sqrt{\\widehat{\\sigma}^2_{(i)} (X^TX)^{-1}_{jj}}}.$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Diagnostics"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "Outliers\n",
      "\n",
      "* Observations $(Y, X_1, \\dots, X_p)$ that do not follow the model, while most other observations seem to follow the model.\n",
      "* One solution: Bonferroni correction, threshold at $t_{1 - \\alpha/(2*n), n-p-2}$.\n",
      "* Bonferroni: if we are doing many $t$ (or other) tests, say $m >>1$ we can control overall false positive rate at $\\alpha$ by testing each one at level $\\alpha/m$."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "No difference"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%R\n",
      "url = 'http://stats191.stanford.edu/data/minority.table'\n",
      "\n",
      "minority.table <- read.table(url, header=T)\n",
      "minority.table$ETHN <- factor(minority.table$ETHN)\n",
      "attach(minority.table)\n",
      "\n",
      "plot(TEST, JPERF, type='n')\n",
      "points(TEST[(ETHN == 0)], JPERF[(ETHN == 0)], pch=21, cex=2, bg='purple')\n",
      "points(TEST[(ETHN == 1)], JPERF[(ETHN == 1)], pch=25, cex=2, bg='green')\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "minority.lm1 <- lm(JPERF ~ TEST)\n",
      "summary(minority.lm1)\n",
      "\n",
      "plot(TEST, JPERF, type='n')\n",
      "points(TEST[(ETHN == 0)], JPERF[(ETHN == 0)], pch=21, cex=2, bg='purple')\n",
      "points(TEST[(ETHN == 1)], JPERF[(ETHN == 1)], pch=25, cex=2, bg='green')\n",
      "abline(minority.lm1$coef, lwd=3, col='blue')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Different slopes, same intercept"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%R\n",
      "minority.lm2 = lm(JPERF ~ TEST + TEST:ETHN)\n",
      "summary(minority.lm2)\n",
      "\n",
      "plot(TEST, JPERF, type='n')\n",
      "points(TEST[(ETHN == 0)], JPERF[(ETHN == 0)], pch=21, cex=2, bg='purple')\n",
      "points(TEST[(ETHN == 1)], JPERF[(ETHN == 1)], pch=25, cex=2, bg='green')\n",
      "\n",
      "abline(minority.lm2$coef['(Intercept)'], minority.lm2$coef['TEST'], lwd=3, col='purple')\n",
      "abline(minority.lm2$coef['(Intercept)'], minority.lm2$coef['TEST'] + minority.lm2$coef['TEST:ETHN1'], lwd=3, col='green')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Different intercepts, same slope"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%R\n",
      "minority.lm3 = lm(JPERF ~ TEST + ETHN)\n",
      "summary(minority.lm3)\n",
      "\n",
      "plot(TEST, JPERF, type='n')\n",
      "points(TEST[(ETHN == 0)], JPERF[(ETHN == 0)], pch=21, cex=2, bg='purple')\n",
      "points(TEST[(ETHN == 1)], JPERF[(ETHN == 1)], pch=25, cex=2, bg='green')\n",
      "abline(minority.lm3$coef['(Intercept)'], minority.lm3$coef['TEST'], lwd=3, col='purple')\n",
      "abline(minority.lm3$coef['(Intercept)'] + minority.lm3$coef['ETHN1'], minority.lm3$coef['TEST'], lwd=3, col='green')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Different intercepts, different slopes"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%R\n",
      "minority.lm4 = lm(JPERF ~ TEST * ETHN)\n",
      "summary(minority.lm4)\n",
      "\n",
      "plot(TEST, JPERF, type='n')\n",
      "points(TEST[(ETHN == 0)], JPERF[(ETHN == 0)], pch=21, cex=2, bg='purple')\n",
      "points(TEST[(ETHN == 1)], JPERF[(ETHN == 1)], pch=25, cex=2, bg='green')\n",
      "abline(minority.lm4$coef['(Intercept)'], minority.lm4$coef['TEST'], lwd=3, col='purple')\n",
      "abline(minority.lm4$coef['(Intercept)'] + minority.lm4$coef['ETHN1'],\n",
      "       minority.lm4$coef['TEST'] + minority.lm4$coef['TEST:ETHN1'], lwd=3, col='green')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "ANOVA models"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "ANOVA table: One-way\n",
      "\n",
      "* \n",
      "  Source | $SS$ | $df$ | $E(MS)$\n",
      "  --- | --- | --- | ---\n",
      "  Treatments | $SSTR = \\sum_{i=1}^r n_i \\left(\\overline{Y}_{i\\cdot} - \\overline{Y}_{\\cdot\\cdot}\\right)^2$ | $r-1$ | $\\sigma^2 + \\frac{\\sum_{i=1}^r n_i \\alpha_i^2}{r-1}$\n",
      "  Error | $SSE = \\sum_{i=1}^r \\sum_{j=1}^{n_i}(Y_{ij} - \\overline{Y}_{i\\cdot})^2$ | $\\sum_{i=1}^r n_i - r$ | $\\sigma^2$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "ANOVA models"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "ANOVA table: Two-way (assuming $n_{ij}=n$)\n",
      "\n",
      "\n",
      "Term | $SS$\n",
      "--- | ---\n",
      "$A$ | $SSA = nm\\sum_{i=1}^r  \\left(\\overline{Y}_{i\\cdot\\cdot} - \\overline{Y}_{\\cdot\\cdot\\cdot}\\right)^2$\n",
      "$B$ | $SSB = nr\\sum_{j=1}^m  \\left(\\overline{Y}_{\\cdot j\\cdot} - \\overline{Y}_{\\cdot\\cdot\\cdot}\\right)^2$\n",
      "$AB$ | $SSAB = n\\sum_{i=1}^r \\sum_{j=1}^m  \\left(\\overline{Y}_{ij\\cdot} - \\overline{Y}_{i\\cdot\\cdot} - \\overline{Y}_{\\cdot j\\cdot} + \\overline{Y}_{\\cdot\\cdot\\cdot}\\right)^2$\n",
      "Error | $SSE = \\sum_{i=1}^r \\sum_{j=1}^m \\sum_{k=1}^{n}(Y_{ijk} - \\overline{Y}_{ij\\cdot})^2$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "ANOVA models"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "ANOVA table: Two-way (assuming $n_{ij}=n$)\n",
      "\n",
      "* \n",
      "  $SS$ | $df$ | $E(MS)$\n",
      "  --- | --- | ---\n",
      "  $SSA$ | $r-1$ | $\\sigma^2 + nm\\frac{\\sum_{i=1}^r \\alpha_i^2}{r-1}$\n",
      "  $SSB$ | $m-1$ | $\\sigma^2 + nr\\frac{\\sum_{j=1}^m \\beta_j^2}{m-1}$\n",
      "  $SSAB$ | $(m-1)(r-1)$ | $\\sigma^2 + n\\frac{\\sum_{i=1}^r\\sum_{j=1}^m (\\alpha\\beta)_{ij}^2}{(r-1)(m-1)}$\n",
      "  $SSE$ | $(n-1)mr$ | $\\sigma^2$\n",
      "* Also talked briefly about random effects."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Weighted least squares"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "Weighted Least Squares\n",
      "\n",
      "* Weighted Least Squares $SSE(\\beta, w) = \\sum_{i=1}^n w_i \\left(Y_i - \\beta_0 - \\beta_1 X_i\\right)^2.$\n",
      "* In general, weights should be like: $w_i = \\frac{1}{\\text{Var}(\\varepsilon_i)}.$\n",
      "* Briefly talked about efficiency of estimators."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "NASDAQ daily close 2011"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%R\n",
      "url = 'http://stats191.stanford.edu/data/nasdaq_2011.csv'\n",
      "nasdaq.data = read.table(url, header=TRUE, sep=',')\n",
      "\n",
      "plot(nasdaq.data$Date, nasdaq.data$Close, xlab='Date', ylab='NASDAQ close',\n",
      "     pch=23, bg='red', cex=2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Correlated errors"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "AR(1) noise\n",
      "\n",
      "* Suppose that, instead of being independent, the errors in our model were $\\varepsilon_t = \\rho \\cdot \\varepsilon_{t-1} + \\omega_t, \\qquad -1 < \\rho < 1$ with $\\omega_t \\sim N(0,\\sigma^2)$ independent.\n",
      "* If $\\rho$ is close to 1, then errors are very correlated, $\\rho=0$ is independence.\n",
      "* This is \"Auto-Regressive Order (1)\" noise (AR(1)). Many other models of correlation exist: ARMA, ARIMA, ARCH, GARCH, etc."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Correlated errors"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "Correcting for AR(1) \n",
      "\n",
      "* Suppose we know $\\rho$, if we \"whiten\" the data and regressors $\\begin{aligned}\n",
      "     \\tilde{Y}_{t+1} &= Y_{t+1} - \\rho Y_t, t > 1   \\\\\n",
      "     \\tilde{X}_{(t+1)j} &= X_{(t+1)j} - \\rho X_{tj}, i > 1\n",
      "     \\end{aligned}$ for $1 \\leq t \\leq n-1$. This model satisfies \"usual\" assumptions, i.e. the errors $\\tilde{\\varepsilon}_t = \\omega_{t+1} = \\varepsilon_{t+1} - \\rho \\cdot \\varepsilon_t$ are independent $N(0,\\sigma^2)$.\n",
      "* For coefficients in new model $\\tilde{\\beta}$, $\\beta_0 = \\tilde{\\beta}_0 / (1 - \\rho)$, $\\beta_j = \\tilde{\\beta}_j.$\n",
      "* Problem: in general, we don\u2019t know $\\rho$."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Model selection"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "Criteria\n",
      "\n",
      "* $C_p({\\cal M}) = \\frac{SSE({\\cal M})}{\\widehat{\\sigma}^2} + 2 \\cdot p({\\cal M}) - n.$\n",
      "* Akaike (AIC) defined as $AIC({\\cal M}) = - 2 \\log L({\\cal M}) + 2 p({\\cal M})$ where $L({\\cal M})$ is the maximized likelihood of the model.\n",
      "* Bayes (BIC) defined as $BIC({\\cal M}) = - 2 \\log L({\\cal M}) + \\log n \\cdot p({\\cal M})$\n",
      "* Adjusted $R^2$\n",
      "* Stepwise (step\n",
      "  ) vs. best subsets (leaps\n",
      "  )."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Shrinking an estimator"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%R\n",
      "nsample = 100\n",
      "ntrial = 50\n",
      "mu = 5 * c(1:nsample) / nsample\n",
      "mu = mu - mean(mu)\n",
      "\n",
      "get.sample = function() {\n",
      "  return(rnorm(nsample) + mu)\n",
      "}\n",
      "\n",
      "MSE = function(mu.hat, mu) {\n",
      "  return(sum((mu.hat - mu)^2) / length(mu))\n",
      "}\n",
      "\n",
      "alpha = seq(0.0,1,length=20)\n",
      "\n",
      "mse = numeric(length(alpha))\n",
      "\n",
      "for (i in 1:ntrial) {\n",
      "  Z = get.sample()\n",
      "  for (j in 1:length(alpha)) {\n",
      "    mse[j] = mse[j] + MSE(alpha[j] * Z, mu) / ntrial\n",
      "  }\n",
      "}\n",
      "\n",
      "plot(alpha, mse, type='l', lwd=2, col='red', ylim=c(0, max(mse)),\n",
      "     xlab='Shrinkage parameter, alpha', ylab='MSE(alpha)')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Model selection"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "$K$-fold cross-validation \n",
      "\n",
      "* Fix a model ${\\cal M}$. Break data set into $K$ approximately equal sized groups $(G_1, \\dots, G_K)$.\n",
      "* for (i in 1:K)\n",
      "   Use all groups except $G_i$ to fit model, predict outcome in group $G_i$ based on this model $\\widehat{Y}_{j,{\\cal M}, G_i}, j \\in G_i$.\n",
      "* Estimate $CV({\\cal M}) = \\frac{1}{n}\\sum_{i=1}^K \\sum_{j \\in G_i} (Y_j - \\widehat{Y}_{j,{\\cal M},G_i})^2.$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Logistic regression"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "Logistic model\n",
      "\n",
      "* Logistic model $\\pi(X_1,X_2) = \\frac{\\exp(\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2)}{1 + \\exp(\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2)}$\n",
      "* This automatically fixes $0 \\leq E(Y) = \\pi(X_1,X_2) \\leq 1$.\n",
      "* **Note:**\n",
      "   $\\text{logit}(\\pi(X_1, X_2)) = \\log\\left(\\frac{\\pi(X_1, X_2)}{1 - \\pi(X_1,X_2)}\\right) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Logistic regression"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "Odds Ratios\n",
      "\n",
      "* One reason logistic models are popular is that the parameters have simple interpretations in terms of **odds**\n",
      "   $ODDS(A) = \\frac{P(A)}{1-P(A)}.$\n",
      "* Logistic model: $OR_{X_j} = \\frac{ODDS(\\dots, X_j=x_j+1, \\dots)}{ODDS(\\dots, X_j=x_j, \\dots)} = e^{\\beta_j}$\n",
      "* If $X_j \\in {0, 1}$ is dichotomous, then odds for group with $X_j = 1$ are $e^{\\beta_j}$ higher, other parameters being equal."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Logistic regression"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "Deviance\n",
      "\n",
      "* For a model ${\\cal M}$, $DEV({\\cal M})$ replaces $SSE({\\cal M})$.\n",
      "* In least squares regression, we use $SSE({\\cal M}_R) - SSE({\\cal M}_F) \\sim \\sigma^2 \\chi^2_{df_R-df_F}$\n",
      "* This is replaced with $DEV({\\cal M}_R) - DEV({\\cal M}_F) \\overset{n \\rightarrow \\infty}{\\sim} \\chi^2_{df_R-df_F}$\n",
      "* An example of a *generalized linear model*\n",
      "  ."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Poisson regression"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "Poisson (log-linear) regression model\n",
      "\n",
      "* Given observations and covariates $Y_i , X_{ij} , 1 \\leq i  \\leq n, 1 \\leq j  \\leq p$.\n",
      "* **Model:**\n",
      "   $Y_{i} \\sim Poisson \\left(\\exp\\left(\\beta_0 + \\sum_{j=1}^p \\beta_j X_{ij} \\right)\\right)$\n",
      "* Poisson assumption implies the variance function is $V (\\mu) = \\mu.$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Poisson regression"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "Interpretation of coefficients\n",
      "\n",
      "* The log-linear model means covariates have *multiplicative*\n",
      "   effect.\n",
      "* Log-linear model model: $\\frac{E(Y|\\dots, X_j=x_j+1, \\dots)}{E(Y|\\dots, X_j=x_j, \\dots)} = e^{\\beta_j}$\n",
      "* So, one unit increase in variable $j$ results in $e^{\\beta_j}$ (multiplicative) increase the expected count, all other parameters being equal."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}