   \documentclass[handout]{beamer}



   \mode<presentation>
   {
     \usetheme{PaloAlto}
   \setbeamertemplate{footline}[page number]

     \setbeamercolor{sidebar}{bg=white, fg=black}
     \setbeamercolor{frametitle}{bg=white, fg=black}
     % or ...
     \setbeamercolor{logo}{bg=white}
     \setbeamercolor{block body}{parent=normal text,bg=white}
     \setbeamercolor{author in sidebar}{fg=black}
     \setbeamercolor{title in sidebar}{fg=black}


     \setbeamercolor*{block title}{use=structure,fg=structure.fg,bg=structure.fg!20!bg}
     \setbeamercolor*{block title alerted}{use=alerted text,fg=alerted text.fg,bg=alerted text.fg!20!bg}
     \setbeamercolor*{block title example}{use=example text,fg=example text.fg,bg=example text.fg!20!bg}


     \setbeamercolor{block body}{parent=normal text,use=block title,bg=block title.bg!50!bg}
     \setbeamercolor{block body alerted}{parent=normal text,use=block title alerted,bg=block title alerted.bg!50!bg}
     \setbeamercolor{block body example}{parent=normal text,use=block title example,bg=block title example.bg!50!bg}

     % or ...

     \setbeamercovered{transparent}
     % or whatever (possibly just delete it)
     \logo{\resizebox{!}{1.5cm}{\href{\basename{R}}{\includegraphics{image}}}}
   }

   \mode<handout>
   {
     \usetheme{PaloAlto}
     \usecolortheme{default}
     \setbeamercolor{sidebar}{bg=white, fg=black}
     \setbeamercolor{frametitle}{bg=white, fg=black}
     % or ...
     \setbeamercolor{logo}{bg=white}
     \setbeamercolor{block body}{parent=normal text,bg=white}
     \setbeamercolor{author in sidebar}{fg=black}
     \setbeamercolor{title in sidebar}{fg=black}
     \setbeamercovered{transparent}
     % or whatever (possibly just delete it)
     \logo{}
   }

   \usepackage{epsdice,listings,epic}
   \usepackage[latin1]{inputenc}
   \usepackage{graphicx}
   \usepackage{amsmath,eepic,epic,algorithm}

   \newcommand{\figslide}[3]{
   \begin{frame}
   \frametitle{#1}
     \begin{center}
     \resizebox{!}{2.7in}{\includegraphics{#2}}    
     \end{center}
   {#3}
   \end{frame}
   }

   \newcommand{\fighslide}[4]{
   \begin{frame}
   \frametitle{#1}
     \begin{center}
     \resizebox{!}{#4}{\includegraphics{#2}}    
     \end{center}
   {#3}
   \end{frame}
   }

   \newcommand{\figwref}[1]{
   \href{#1}{\tiny \tt #1}}

   \newcommand{\unsupervised}[1]{{\color{red} #1}}
   \newcommand{\supervised}[1]{{\color{green} #1}}
   \newcommand{\argmax}{\mathop{\mathrm{argmax}}}
   \newcommand{\argmin}{\mathop{\mathrm{argmin}}}
   \newcommand{\minimize}{\mathop{\mathrm{minimize}}}
   \newcommand{\maximize}{\mathop{\mathrm{maximize}}}

   \newcommand{\B}[1]{\beta_{#1}}
   \newcommand{\Bh}[1]{\widehat{\beta}_{#1}}
   \newcommand{\V}{\text{Var}}
   \newcommand{\Cov}{\text{Cov}}
   \newcommand{\Vh}{\widehat{\V}}
   \newcommand{\s}{\sigma}
   \newcommand{\sh}{\widehat{\sigma}}

   \newcommand{\argmax}[1]{\mathop{\text{argmax}}_{#1}}
   \newcommand{\argmin}[1]{\mathop{\text{argmin}}_{#1}}
   \newcommand{\Ee}{\mathbb{E}}
   \newcommand{\Pp}{\mathbb{P}}
   \newcommand{\real}{\mathbb{R}}
   \newcommand{\Ybar}{\overline{Y}}
   \newcommand{\Yh}{\widehat{Y}}
   \newcommand{\Xbar}{\overline{X}}
   \newcommand{\Tr}{\text{Tr}}


   \newcommand{\model}{{\cal M}}

   \newcommand{\figvskip}{-0.7in}
   \newcommand{\fighskip}{-0.3in}
   \newcommand{\figheight}{3.5in}

   \newcommand{\Rcode}[1]{{\bf \tt #1 }}
   \newcommand{\Rtcode}[1]{{\tiny \bf \tt #1 }}
   \newcommand{\Rscode}[1]{{\small \bf \tt #1 }}

   \newcommand{\RR}{{\tt R} \;}
   \newcommand{\basename}[1]{http://stats191.stanford.edu/#1}
   \newcommand{\dataname}[1]{\basename{data/#1}}
   \newcommand{\Rname}[1]{\basename{R/#1}}

   \newcommand{\mycolor}[1]{{\color{blue} #1}}
   \newcommand{\basehref}[2]{\href{\basename{#1}}{\mycolor{#2}}}
   \newcommand{\Rhref}[2]{\href{\basename{R/#1}}{\mycolor{#2}}}
   \newcommand{\datahref}[2]{\href{\dataname{#1}}{\mycolor{#2}}}
   \newcommand{\X}{\pmb{X}}
   \newcommand{\Y}{\pmb{Y}}
   \newcommand{\be}{\pmb{varepsilon}}
   \newcommand{\logit}{\text{logit}}


   \title{Statistics 191: Introduction to Applied Statistics}
   \subtitle{Further topics in regression}
   \author{\copyright Jonathan Taylor \\
   }
   %}


   \begin{document}

   \begin{frame}
   \titlepage
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Outline}

   \begin{block} {Topics}
     \begin{itemize}

     \item Nonparametric regression.
   \item Bootstrap.
     \item Time series.

   \end{itemize}

   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Further topics}

   \begin{block}
   {Nonparametric regression}
       \begin{itemize}
       \item We've focused on  linear regression:
       $$
       \minimize_{\beta} \frac{1}{2} \|Y-X\beta^2\|^2_2.
       $$
       \item Equivalent to
       $$
       \minimize_{f_j} \frac{1}{2} \|Y-\sum_{j=1}^p f_j(X)\|^2_2.
       $$
       where
       $$
       f_j \in \left\{(x^Te_j) \cdot \beta_j: \beta_j \in \real\right\} = {\cal F}_j
       $$
       \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Further topics}

   \begin{block}
   {Nonparametric regression}
       \begin{itemize}
       \item The class ${\cal F}_j$ can be changed.
       \item For instance, we might take
       $$
       {\cal F}_j = \left\{f = g_j(x_j) \right\}
       $$
       so our problem is
       $$
       \minimize_{\beta} \frac{1}{2} \|Y-\sum_{j=1}^p f_j(X_j)\|^2_2.
       $$
       \item This called an {\em additive} model.
       \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Further topics}

   \begin{block}
   {Nonparametric regression}
       \begin{itemize}
       \item The class ${\cal F}_j$ is too big -- we need
       to penalize it.
       \item Ideally, penalty should make it smoother.
       \item One example.
       $$
       {\cal P}(f) = \int_{-\infty}^{\infty} (f''(x))^2 \; dx
       $$
       \item This leads to
       $$
       \minimize_{f_1, \dots, f_p} \frac{1}{2} \|Y-\sum_{j=1}^p f_j(X_j)\|^2_2 +
       \lambda \sum_{j=1}^p \int_{-\infty}^{\infty} (f_j''(x))^2 \; dx.
       $$
       \item This called an {\em additive} model.
       \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Further topics}

   \begin{block}
   {Generalized additive model}
       \begin{itemize}
       \item The squared error part can be replaced
       by logistic or Poisson deviance.
       \item Leads to a Generalized Additive Model (GAM)
       $$
       \minimize_{f_1, \dots, f_p} DEV(Y;\sum_{j=1}^p f_j(X_j)) +
       \lambda \sum_{j=1}^p \int_{-\infty}^{\infty} (f_j''(x))^2 \; dx.
       $$
       \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Further topics}

   \begin{block}
   {Kernel trick}
       \begin{itemize}
       \item The quadratic penalty on the derivative
       can be replaced by what is known as a kernel penalty
       \item Leads to a Generalized Additive Model (GAM)
       $$
       \minimize_{f_1, \dots, f_p} DEV(Y;\sum_{j=1}^p f_j(X_j)) +
       \lambda \sum_{j=1}^p \|f_j\|^2_{{\cal H}_j}.
       $$
       \item Known as {\em kernelizing} or using the {\em kernel trick}.
       \item STATS315A would be a good course to see more of this.
       \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Further topics}

   \begin{block}
   {Nonparametric Inference}

   \begin{enumerate}
   \item Much of the distributions we've been using all quarter
   rely heavily on a Gaussian assumption.
   \item Of course, this may not be true in practice.
   \item If we're willing to assume independence of the
   pairs $(X_i,Y_i)$ then there are some non-parametric ways to
   perform inference.
   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Further topics}

   \begin{block}
   {Bootstrap}

   \begin{enumerate}
   \item Suppose we want to find a confidence interval
   for $\beta_j$.
   \item Do the following:
   \begin{enumerate}
   \item For $1 \leq b \leq B$: sample
   $n$ pairs $(X_i,Y_i)$ with replacement from
   the original sample. Call this bootstrap sample $X^b, Y^b$
   \item Form
   $$
   \hat{\beta}^b = ((X^b)^TX^b)^{-1} (X^b)^TY^b
   $$
   and store $\hat{\beta}^b_j$.
   \item Take the empirical 2.5\% and 97.5\% empirical quantiles of
   $$
   \left\{\hat{\beta}^b_j: 1 \leq b \leq B\right\}.
   $$
   \end{enumerate}
   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Further topics}

   \begin{block}
   {Tests of correlation}

   \begin{itemize}
   \item Suppose $p=1$ and we just want to
   know if $X$ and $Y$ are correlated but we don't believe
   the Gaussian assumption.
   \item Then, Spearman's correlation
   coefficient converts each $X$ and $Y$ to ranks
   $R_X, R_Y$.
   \item Test statistic is the $\sqrt{R^2}$ from regressing
   $R_Y$ onto $R_X$.
   \item Its null distribution does not depend heavily
   on the marginal distributions of $X$ and $Y$.
   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Further topics}

   \begin{block}
   {Two sample tests}

   \begin{itemize}
   \item Suppose we are looking at the two group problem
   and want to test whether there is a difference in means
   between the two groups.
   \item Do the following:
   \begin{enumerate}
   \item Compute the usual $T$ statistic to test the difference
   between the two.
   \item For $1 \leq b \leq B$: randomize
   the assignment of the $Y_i$'s to groups, call this
   newly shuffled response $Y^b$.
   \item Form the usual $T$ statistic based on response
   $Y^b$ and store it as $T^b$
   \item To test at level $5\%$, reject $H_0: \text{difference is 0}$ if
   $$
   |T| > \text{$95\%$ quantile of $\left\{|T^b|: 1 \leq b \leq B \right\}$}.
   $$
   \end{enumerate}
   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Further topics}

   \begin{block}
   {Nonparametric inference}

   \begin{itemize}
   \item STATS208 is a course on the bootstrap.

   \item Likely covers some other nonparametric inference as well \dots

   \item STATS205 is a course on nonparametric inference (not offered this
   year).
   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Further topics}

   \begin{block}
   {Time series / spatial statistics}
   \begin{itemize}
   \item We just scratched the surface on
   time series model in our $AR(1)$ example.
   \item STATS207 is an entire course on time series.
   \item STATS252 is an entire course on spatial covariance models
   (not offered this year).
   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} 

   \end{frame}

   \end{document}
