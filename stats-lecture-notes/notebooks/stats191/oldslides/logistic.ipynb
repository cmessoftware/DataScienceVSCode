{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load_ext rmagic"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Topics"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "Today\u2019s class\n",
      "\n",
      "* Binary outcomes.\n",
      "* Logistic regression.\n",
      "* Generalized linear models.\n",
      "* Deviance."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Logistic regression"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "Binary outcomes\n",
      "\n",
      "* Most models so far have had response $Y$ as continuous.\n",
      "* Many responses in practice fall into the $YES/NO$ framework.\n",
      "* Examples:\n",
      "*     1. medical: presence or absence of cancer\n",
      "      1. financial: bankrupt or solvent\n",
      "      1. industrial: passes a quality control test or not"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Logistic regression"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "Modelling probabilities\n",
      "\n",
      "* For $0-1$ responses we need to model $\\pi(x_1, \\dots, x_p) = P(Y=1|X_1=x_1,\\dots, X_p=x_p)$\n",
      "* That is, $Y$ is Bernoulli with a probability that depends on covariates $\\{X_1, \\dots, X_p\\}$.\n",
      "* **Note:**\n",
      "   $\\text{Var}(Y) = \\pi ( 1 - \\pi) = E(Y) \\cdot ( 1-  E(Y))$\n",
      "* **Or,**\n",
      "   the binary nature forces a relation between mean and variance of $Y$."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Logistic regression"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "Flu shot example\n",
      "\n",
      "* A local health clinic sent fliers to its clients to encourage everyone, but especially older persons at high risk of complications, to get a flu shot in time for protection against an expected flu epidemic.\n",
      "* In a pilot follow-up study, 50 clients were randomly selected and asked whether they actually received a flu shot. $Y={\\tt Shot}$\n",
      "* In addition, data were collected on their age $X_1={\\tt Age}$ and their health awareness $X_2={\\tt Health.Aware}$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Logistic regression"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "Model for probabilities\n",
      "\n",
      "* Simplest model $\\pi(X_1,X_2) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2$\n",
      "* Problems:\n",
      "*     * We must have $0 \\leq E(Y) = \\pi(X_1,X_2) \\leq 1$\n",
      "      * Ordinary least squares will not work because of relation between mean and variance."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Logistic regression"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "Logistic model\n",
      "\n",
      "* Logistic model $\\pi(X_1,X_2) = \\frac{\\exp(\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2)}{1 + \\exp(\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2)}$\n",
      "* This automatically fixes $0 \\leq E(Y) = \\pi(X_1,X_2) \\leq 1$.\n",
      "* **Note:**\n",
      "   $\\text{logit}(\\pi(X_1, X_2)) = \\log\\left(\\frac{\\pi(X_1, X_2)}{1 - \\pi(X_1,X_2)}\\right) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Logistic curve"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%R\n",
      "g <- function(x) {\n",
      "  return(log(x / (1 - x)))\n",
      "}\n",
      "\n",
      "g.inv <- function(y) {\n",
      "  return(exp(y) / (1 + exp(y)))\n",
      "}\n",
      "\n",
      "p = seq(g(0.01), g(0.99), length=200)\n",
      "plot(p, g.inv(p), lwd=2, type='l', col='red')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Logistic transform"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%R\n",
      "x = seq(0.01,0.99,length=200)\n",
      "plot(x, g(x), lwd=2, type='l', col='red')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Logistic regression"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "Binary regression models\n",
      "\n",
      "* Models $E(Y)$ as some increasing function of $\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2$.\n",
      "* The logistic model uses the function $f(x)=e^x/(1+e^x)$.\n",
      "* Can be fit using Maximum Likelihood / Iteratively Reweighted Least Squares.\n",
      "* Coefficients have nice interpretation in terms of **odds ratios**\n",
      "  \n",
      "* Inference (?)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Logistic regression"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "Odds Ratios\n",
      "\n",
      "* One reason logistic models are popular is that the parameters have simple interpretations in terms of **odds**\n",
      "   $ODDS(A) = \\frac{P(A)}{1-P(A)}.$\n",
      "* Logistic model: $OR_{X_j} = \\frac{ODDS(\\dots, X_j=x_j+1, \\dots)}{ODDS(\\dots, X_j=x_j, \\dots)} = e^{\\beta_j}$\n",
      "* If $X_j \\in {0, 1}$ is dichotomous, then odds for group with $X_j = 1$ are $e^{\\beta_j}$ higher, other parameters being equal."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Logistic regression"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "Rare disease hypothesis\n",
      "\n",
      "* When incidence is rare, $P(Y=0)\\approxeq 1$ no matter what the covariates $X_j$\u2019s are.\n",
      "* In this case, odds ratios are almost ratios of probabilities: $OR_{X_j} \\approxeq \\frac{{\\mathbb{P}}(Y=1|\\dots, X_j=x_j+1, \\dots)}{{\\mathbb{P}}(Y=1|\\dots, X_j=x_j, \\dots)}$\n",
      "* Hypothetical example: in a lung cancer study, if $X_j$ is an indicator of smoking or not, a $\\beta_j$ of 5 means for smoking vs. non-smoking means smokers are $e^5 \\approx 150$ times more likely to develop lung cancer\n",
      "* In flu example, the odds for a 45 year old with health awareness 50 compared to a 35 year old with the same health awareness are $e^{2.2178}=9.18$, but ratio of probs is $0.1932/0.0254 \\approx 7.61$."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Binary regression"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "Deviance\n",
      "\n",
      "* $DEV(\\mu| Y) = -2 \\log L(\\mu| Y) + 2 \\log L(Y| Y)$ where $\\mu$ is a location estimator for $Y$.\n",
      "* If $Y$ is Gaussian with independent $N(\\mu_i,\\sigma^2)$ entries $DEV(\\mu| Y) = \\frac{1}{\\sigma^2}\\sum_{i=1}^n(Y_i - \\mu_i)^2$\n",
      "* If $Y$ is a binary vector, with mean vector $\\pi$ $\\begin{aligned}\n",
      "     DEV(\\pi| Y) &= -2 \\sum_{i=1}^n \\left( Y_i \\log(\\pi_i) + (1-Y_i) \\log(1-\\pi_i) \\right) \\\\\n",
      "     \\end{aligned}$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Binary regression"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "Deviance\n",
      "\n",
      "* In the logistic model, $\\begin{aligned}\n",
      "     DEV(\\beta| Y) &=  -2 \\sum_{i=1}^n \\left( Y_i {\\text{logit}}(\\pi_i(\\beta)) + \\log(1-\\pi_i(\\beta)) \\right) \\\\\n",
      "     &= -2 \\sum_{i=1}^n \\left(Y_i \\left(\\beta_0 + \\sum_{j=1}^p \\beta_j X_{ij} \\right) + \\log(1 - \\pi_i(\\beta)) \\right)\n",
      "     \\end{aligned}$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Logistic regression"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "Fitting the model ($g(\\pi) = \\text{logit}(\\pi)$)\n",
      "\n",
      "1. Initialize $\\widehat{\\pi}_i = \\bar{Y}, 1 \\leq i \\leq n$\n",
      "1. Define $Z_i = g(\\widehat{\\pi}_i) + g'(\\widehat{\\pi}_i) (Y_i - \\widehat{\\pi_i})$\n",
      "1. Fit weighted least squares model $Z_i = \\beta_0 + \\sum_{j=1}^p \\beta_j X_{ij}, \\qquad w_i = \\widehat{\\pi_i} (1 - \\widehat{\\pi}_i)$\n",
      "1. Set $\\widehat{\\pi}_i = \\text{logit}^{-1} \\left(\\widehat{\\beta}_0 + \\sum_{j=1}^p \\widehat{\\beta}_j X_{ij}\\right)$.\n",
      "1. Repeat steps 2-4 until convergence.\n",
      "This is *basically*\n",
      " Newton-Raphson to minimize deviance."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Logistic regression"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "Inference\n",
      "\n",
      "* The IRLS procedure suggests using approximation $\\widehat{\\beta} \\approx N(\\beta, (X'WX)^{-1})$\n",
      "* This allows us to construct CIs, test linear hypotheses, etc.\n",
      "* What about comparing ${\\cal M}_F$ and ${\\cal M}_R$?"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Logistic regression"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "Deviance\n",
      "\n",
      "* For a model ${\\cal M}$, $DEV({\\cal M})$ replaces $SSE({\\cal M})$.\n",
      "* In least squares regression, we use $SSE({\\cal M}_R) - SSE({\\cal M}_F) \\sim \\chi^2_{df_R-df_F}$\n",
      "* This is replaced with $DEV({\\cal M}_R) - DEV({\\cal M}_F) \\overset{n \\rightarrow \\infty}{\\sim} \\chi^2_{df_R-df_F}$\n",
      "* Resulting tests **do not**\n",
      "   agree with those coming from IRLS (Wald tests). Both are often used."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Logistic regression"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "Other points\n",
      "\n",
      "* Diagnostics: similar to least square regression, only residuals used are *deviance residuals*\n",
      "   $r_i = \\text{sign}(Y_i-\\widehat{\\pi}_i) \\sqrt{DEV(\\widehat{\\pi}_i|Y_i)}.$\n",
      "* Model selection: because it is fit based on likelihood, stepwise selection can be used easily \u2026"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Diagnostics for logistic model"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%R\n",
      "flu.table <- read.table('http://www-stat.stanford.edu/~jtaylo/courses/stats191/data/flu.table', header=T)\n",
      "\n",
      "\n",
      "flu.glm = glm(Shot ~ Age + Health.Aware, data=flu.table, family=binomial())\n",
      "print(summary(flu.glm))\n",
      "\n",
      "\n",
      "par(mfrow=c(2,2))\n",
      "plot(flu.glm)\n",
      "par(mfrow=c(1,1))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Binary regression"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "Probit transform\n",
      "\n",
      "* Probit regression model: $\\Phi^{-1}({\\mathbb{E}}(Y_i))= \\beta_0 + \\sum_{j=1}^{p-1} \\beta_j X_{ij}$ where $\\Phi$ is CDF of $N(0,1)$, i.e. $\\Phi(t) = {\\tt pnorm(t)}$.\n",
      "* Complementary log-log model (cloglog): $-log(-log({\\mathbb{E}}(Y_i)) = \\beta_0 + \\sum_{j=1}^{p-1} \\beta_j X_{ij}.$\n",
      "* In logit, probit and cloglog ${\\text{Var}}(Y_i)=\\pi_i(1-\\pi_i)$ but the model for the mean is different."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Generalized linear models"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "Link & variance functions\n",
      "\n",
      "Given a dataset $(Y_i, X_{i1}, \\dots, X_{ip}), 1 \\leq i \\leq n$ we consider a model for the distribution of $Y|X_1, \\dots, X_p$.\n",
      "* If $\\eta_i=g({\\mathbb{E}}(Y_i)) = g(\\mu_i) = \\beta_0 + \\sum_{j=1}^k \\beta_j X_{ij}$ then $g$ is called the *link*\n",
      "   function for the model.\n",
      "* If ${\\text{Var}}(Y_i) = \\phi \\cdot V({\\mathbb{E}}(Y_i)) = \\phi \\cdot V(\\mu_i)$ for $\\phi > 0$ and some function $V$, then $V$ is the called *variance*\n",
      "   function for the model.\n",
      "* Canonical reference: *Generalized Linear Models*\n",
      "  , McCullagh and Nelder."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Binary regression as GLM"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "Binary (again)\n",
      "\n",
      "* For a logistic model, $g(\\mu)={\\text{logit}}(\\mu), \\qquad V(\\mu)=\\mu(1-\\mu).$\n",
      "* For a probit model, $g(\\mu)=\\Phi^{-1}(\\mu), \\qquad V(\\mu)=\\mu(1-\\mu).$\n",
      "* For a cloglog model, $g(\\mu)=-\\log(-\\log(\\mu)), \\qquad V(\\mu)=\\mu(1-\\mu).$"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}