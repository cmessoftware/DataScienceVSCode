   \documentclass[handout]{beamer}



   \mode<presentation>
   {
     \usetheme{PaloAlto}
   \setbeamertemplate{footline}[page number]

     \setbeamercolor{sidebar}{bg=white, fg=black}
     \setbeamercolor{frametitle}{bg=white, fg=black}
     % or ...
     \setbeamercolor{logo}{bg=white}
     \setbeamercolor{block body}{parent=normal text,bg=white}
     \setbeamercolor{author in sidebar}{fg=black}
     \setbeamercolor{title in sidebar}{fg=black}


     \setbeamercolor*{block title}{use=structure,fg=structure.fg,bg=structure.fg!20!bg}
     \setbeamercolor*{block title alerted}{use=alerted text,fg=alerted text.fg,bg=alerted text.fg!20!bg}
     \setbeamercolor*{block title example}{use=example text,fg=example text.fg,bg=example text.fg!20!bg}


     \setbeamercolor{block body}{parent=normal text,use=block title,bg=block title.bg!50!bg}
     \setbeamercolor{block body alerted}{parent=normal text,use=block title alerted,bg=block title alerted.bg!50!bg}
     \setbeamercolor{block body example}{parent=normal text,use=block title example,bg=block title example.bg!50!bg}

     % or ...

     \setbeamercovered{transparent}
     % or whatever (possibly just delete it)
     \logo{\resizebox{!}{1.5cm}{\href{\basename{R}}{\includegraphics{image}}}}
   }

   \mode<handout>
   {
     \usetheme{PaloAlto}
     \usecolortheme{default}
     \setbeamercolor{sidebar}{bg=white, fg=black}
     \setbeamercolor{frametitle}{bg=white, fg=black}
     % or ...
     \setbeamercolor{logo}{bg=white}
     \setbeamercolor{block body}{parent=normal text,bg=white}
     \setbeamercolor{author in sidebar}{fg=black}
     \setbeamercolor{title in sidebar}{fg=black}
     \setbeamercovered{transparent}
     % or whatever (possibly just delete it)
     \logo{}
   }

   \usepackage{epsdice,listings,epic}
   \usepackage[latin1]{inputenc}
   \usepackage{graphicx}
   \usepackage{amsmath,eepic,epic,algorithm}

   \newcommand{\figslide}[3]{
   \begin{frame}
   \frametitle{#1}
     \begin{center}
     \resizebox{!}{2.7in}{\includegraphics{#2}}    
     \end{center}
   {#3}
   \end{frame}
   }

   \newcommand{\fighslide}[4]{
   \begin{frame}
   \frametitle{#1}
     \begin{center}
     \resizebox{!}{#4}{\includegraphics{#2}}    
     \end{center}
   {#3}
   \end{frame}
   }

   \newcommand{\figwref}[1]{
   \href{#1}{\tiny \tt #1}}

   \newcommand{\unsupervised}[1]{{\color{red} #1}}
   \newcommand{\supervised}[1]{{\color{green} #1}}
   \newcommand{\argmax}{\mathop{\mathrm{argmax}}}
   \newcommand{\argmin}{\mathop{\mathrm{argmin}}}
   \newcommand{\minimize}{\mathop{\mathrm{minimize}}}
   \newcommand{\maximize}{\mathop{\mathrm{maximize}}}

   \newcommand{\B}[1]{\beta_{#1}}
   \newcommand{\Bh}[1]{\widehat{\beta}_{#1}}
   \newcommand{\V}{\text{Var}}
   \newcommand{\Cov}{\text{Cov}}
   \newcommand{\Vh}{\widehat{\V}}
   \newcommand{\s}{\sigma}
   \newcommand{\sh}{\widehat{\sigma}}

   \newcommand{\argmax}[1]{\mathop{\text{argmax}}_{#1}}
   \newcommand{\argmin}[1]{\mathop{\text{argmin}}_{#1}}
   \newcommand{\Ee}{\mathbb{E}}
   \newcommand{\Pp}{\mathbb{P}}
   \newcommand{\real}{\mathbb{R}}
   \newcommand{\Ybar}{\overline{Y}}
   \newcommand{\Yh}{\widehat{Y}}
   \newcommand{\Xbar}{\overline{X}}
   \newcommand{\Tr}{\text{Tr}}


   \newcommand{\model}{{\cal M}}

   \newcommand{\figvskip}{-0.7in}
   \newcommand{\fighskip}{-0.3in}
   \newcommand{\figheight}{3.5in}

   \newcommand{\Rcode}[1]{{\bf \tt #1 }}
   \newcommand{\Rtcode}[1]{{\tiny \bf \tt #1 }}
   \newcommand{\Rscode}[1]{{\small \bf \tt #1 }}

   \newcommand{\RR}{{\tt R} \;}
   \newcommand{\basename}[1]{http://stats191.stanford.edu/#1}
   \newcommand{\dataname}[1]{\basename{data/#1}}
   \newcommand{\Rname}[1]{\basename{R/#1}}

   \newcommand{\mycolor}[1]{{\color{blue} #1}}
   \newcommand{\basehref}[2]{\href{\basename{#1}}{\mycolor{#2}}}
   \newcommand{\Rhref}[2]{\href{\basename{R/#1}}{\mycolor{#2}}}
   \newcommand{\datahref}[2]{\href{\dataname{#1}}{\mycolor{#2}}}
   \newcommand{\X}{\pmb{X}}
   \newcommand{\Y}{\pmb{Y}}
   \newcommand{\be}{\pmb{varepsilon}}
   \newcommand{\logit}{\text{logit}}


   \title{Statistics 191: Introduction to Applied Statistics}
   \subtitle{Logistic regression (Ch. 12, RABE)}
   \author{\copyright Jonathan Taylor \\
   }
   %}


   \begin{document}

   \begin{frame}
   \titlepage
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Topics}

   \begin{block}
   {Today's class}
     \begin{itemize}


     \item Binary outcomes.

     \item Logistic regression.

     \item Generalized linear models.

     \item Deviance.

   \end{itemize}

   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Logistic regression}

   \begin{block}
       {Binary outcomes}
       \begin{itemize}
       \item Most models so far have had response $Y$ as continuous.

       \item Many responses in practice fall into the $YES/NO$ framework.

       \item Examples:
         \begin{enumerate}
         \item medical: presence or absence of cancer

         \item financial: bankrupt or solvent

         \item industrial: passes a quality control test or not
         \end{enumerate}
       \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Logistic regression}

   \begin{block}
       {Modelling probabilities}
       \begin{itemize}
       \item For $0-1$ responses we need to model
   $$
   \pi(x_1, \dots, x_p) = P(Y=1|X_1=x_1,\dots, X_p=x_p)
   $$

   \item That is, $Y$ is Bernoulli with a probability that
   depends on covariates $\{X_1, \dots, X_p\}$.

   \item {\bf Note:}
   $$
   \text{Var}(Y) = \pi ( 1 - \pi) = E(Y) \cdot ( 1-  E(Y))
   $$

   \item {\bf Or,} the binary nature forces a relation between
   mean and variance of $Y$.
       \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Logistic regression}

   \begin{block}
       {Flu shot example}
       \begin{itemize}
       \item      A local health clinic sent fliers to its clients to encourage
        everyone, but especially older persons at high risk of
        complications, to get a flu shot in time for protection
        against an expected flu epidemic.


      \item      In a pilot follow-up study, 50 clients were randomly
        selected and asked whether they actually received a flu
        shot. $Y={\tt Shot}$


      \item      In addition, data were collected on their age $X_1={\tt Age}$ and their
        health awareness $X_2={\tt Health.Aware}$

       \end{itemize}

   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Logistic regression}

   \begin{block}
       {Model for probabilities}
       \begin{itemize}
       \item Simplest model
   $$
   \pi(X_1,X_2) = \beta_0 + \beta_1 X_1 + \beta_2 X_2
   $$

   \item Problems:
     \begin{itemize}
     \item We must have $0 \leq E(Y) = \pi(X_1,X_2) \leq 1$

     \item Ordinary least squares will not work because of relation
   between mean and variance.
     \end{itemize}
       \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Logistic regression}

   \begin{block}
       {Logistic model}
       \begin{itemize}
       \item Logistic model
   $$
   \pi(X_1,X_2) = \frac{\exp(\beta_0 + \beta_1 X_1 + \beta_2 X_2)}{1 + \exp(\beta_0 + \beta_1 X_1 + \beta_2 X_2)}
   $$

   \item This automatically fixes $0 \leq E(Y) = \pi(X_1,X_2) \leq 1$.


   \item {\bf Note:}
   $$
   \text{logit}(\pi(X_1, X_2)) = \log\left(\frac{\pi(X_1, X_2)}{1 - \pi(X_1,X_2)}\right) = \beta_0 + \beta_1 X_1 + \beta_2 X_2
   $$

       \end{itemize}
   \end{block}
   \end{frame}

   %CODE
       % g <- function(x) {
   %   return(log(x / (1 - x)))
   % }
   % 
   % g.inv <- function(y) {
   %   return(exp(y) / (1 + exp(y)))
   % }
   % 
   % p = seq(g(0.01), g(0.99), length=200)
   % plot(p, g.inv(p), lwd=2, type='l', col='red')


   \begin{frame}
   \frametitle{Logistic curve}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/9aafc90f62.png}}    
   \end{center}
   \href{http://stats191.stanford.edu/logistic.html#logistic-transform}{R code}
   \end{frame}

   %CODE
       % x = seq(0.01,0.99,length=200)
   % plot(x, g(x), lwd=2, type='l', col='red')


   \begin{frame}
   \frametitle{Logistic transform}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/9b69173c09.png}}    
   \end{center}
   \href{http://stats191.stanford.edu/logistic.html#logistic-transform}{R code}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Logistic regression}

   \begin{block}
     {Binary regression models}

       \begin{itemize}

       \item Models $E(Y)$ as some increasing function of
   $\beta_0 + \beta_1 X_1 + \beta_2 X_2$.
     \item The logistic
   model uses the function $f(x)=e^x/(1+e^x)$.

       \item Can be fit using Maximum Likelihood / Iteratively Reweighted Least Squares.


       \item Coefficients have nice interpretation in terms of {\bf odds ratios}
       \item Inference (?)
   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Logistic regression}

   \begin{block}
   {Odds Ratios}

   \begin{itemize}
   \item     One reason logistic models are popular is that the
       parameters have simple interpretations in terms of {\bf odds}
   $$
   ODDS(A) = \frac{P(A)}{1-P(A)}.
   $$

   \item Logistic model:
   $$
   OR_{X_j} = \frac{ODDS(\dots, X_j=x_j+1, \dots)}{ODDS(\dots, X_j=x_j, \dots)} = e^{\beta_j}
   $$


   \item If $X_j \in {0, 1}$ is dichotomous, then odds for group with $X_j = 1$ are
       $e^{\beta_j}$ higher, other parameters being equal.

   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Logistic regression}

   \begin{block}
         {Rare disease hypothesis}
         \begin{itemize}
         \item When incidence is rare, $P(Y=0)\approxeq 1$ no matter
   what the covariates $X_j$'s are.

   \item In this case, odds ratios are almost ratios of probabilities:
   $$
   OR_{X_j} \approxeq \frac{\Pp(Y=1|\dots, X_j=x_j+1, \dots)}{\Pp(Y=1|\dots, X_j=x_j, \dots)}
   $$

   \item Hypothetical example: in a lung cancer study, if $X_j$ is an indicator of smoking or not, a $\beta_j$ of 5 means for smoking vs. non-smoking means smokers are $e^5 \approx 150$ times more likely to develop lung cancer

   \item In flu example, the odds for a 45 year old with health awareness 50
   compared to a 35 year old with the same health awareness are $e^{2.2178}=9.18$, but ratio of probs is $0.1932/0.0254 \approx 7.61$.

   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Binary regression}

   \begin{block}
   {Deviance}
   \begin{itemize}

   \item
   $$
   DEV(\mu| Y) = -2 \log L(\mu| Y) + 2 \log L(Y| Y)$$
   where $\mu$ is a  location estimator for $Y$.

   \item If $Y$ is Gaussian with independent $N(\mu_i,\sigma^2)$ entries
   $$
   DEV(\mu| Y) = \frac{1}{\sigma^2}\sum_{i=1}^n(Y_i - \mu_i)^2$$

   \item If $Y$ is a binary vector,  with mean vector $\pi$
   $$
   \begin{aligned}
   DEV(\pi| Y) &= -2 \sum_{i=1}^n \left( Y_i \log(\pi_i) + (1-Y_i) \log(1-\pi_i) \right) \\
   \end{aligned}
   $$
   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Binary regression}

   \begin{block}
   {Deviance}
   \begin{itemize}

   \item In the logistic model,
   $$
   \begin{aligned}
   DEV(\beta| Y) &=  -2 \sum_{i=1}^n \left( Y_i \logit(\pi_i(\beta)) + \log(1-\pi_i(\beta)) \right) \\
   &= -2 \sum_{i=1}^n \left(Y_i \left(\beta_0 + \sum_{j=1}^p \beta_j X_{ij} \right) + \log(1 - \pi_i(\beta)) \right)
   \end{aligned}
   $$
   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Logistic regression}

   \begin{block}
         {Fitting the model ($g(\pi) = \text{logit}(\pi)$)}
         \begin{enumerate}
         \item Initialize $\widehat{\pi}_i = \bar{Y}, 1 \leq i \leq n$

         \item Define $Z_i = g(\widehat{\pi}_i) + g'(\widehat{\pi}_i) (Y_i - \widehat{\pi_i})$

         \item Fit weighted least squares model
   $$
   Z_i = \beta_0 + \sum_{j=1}^p \beta_j X_{ij}, \qquad w_i = \widehat{\pi_i} (1 - \widehat{\pi}_i)$$

   \item Set $\widehat{\pi}_i = \text{logit}^{-1} \left(\widehat{\beta}_0 + \sum_{j=1}^p \widehat{\beta}_j X_{ij}\right)$.

   \item Repeat steps 2-4 until convergence.
         \end{enumerate}
    This is {\em basically} Newton-Raphson to minimize deviance.
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Logistic regression}

   \begin{block}
         {Inference}
         \begin{itemize}
         \item The IRLS procedure suggests using approximation
   $$
   \widehat{\beta} \approx N(\beta, (X'WX)^{-1})
   $$

   \item This allows us to construct CIs, test linear hypotheses, etc.

   \item What about comparing ${\cal M}_F$ and ${\cal M}_R$?
         \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Logistic regression}

   \begin{block}
         {Deviance}
         \begin{itemize}
         \item For a model ${\cal M}$, $DEV({\cal M})$ replaces $SSE({\cal M})$.


   \item In least squares regression, we use
   $$
   SSE({\cal M}_R) - SSE({\cal M}_F) \sim \chi^2_{df_R-df_F}$$

   \item This is replaced with
   $$
   DEV({\cal M}_R) - DEV({\cal M}_F) \overset{n \rightarrow \infty}{\sim} \chi^2_{df_R-df_F}$$

   \item Resulting tests {\bf do not} agree with those coming from IRLS (Wald tests). Both are often used.
         \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Logistic regression}

   \begin{block}
         {Other points}
         \begin{itemize}
         \item Diagnostics: similar to least square regression, only residuals
   used are {\em deviance residuals}
   $$
   r_i = \text{sign}(Y_i-\widehat{\pi}_i) \sqrt{DEV(\widehat{\pi}_i|Y_i)}.
   $$

   \item Model selection: because it is fit based on likelihood,
   stepwise selection can be used easily \dots
         \end{itemize}
   \end{block}
   \end{frame}

   %CODE
       % flu.table <- read.table('http://www-stat.stanford.edu/~jtaylo/courses/stats191/data/flu.table', header=T)
   % 
   % 
   % flu.glm = glm(Shot ~ Age + Health.Aware, data=flu.table, family=binomial())
   % print(summary(flu.glm))
   % 
   % 
   % par(mfrow=c(2,2))
   % plot(flu.glm)
   % par(mfrow=c(1,1))


   \begin{frame}
   \frametitle{Diagnostics for logistic model}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/ba8132fa7f.png}}    
   \end{center}
   \href{http://stats191.stanford.edu/logistic.html#flu-example}{R code}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Binary regression}

   \begin{block}
   {Probit transform}

    \begin{itemize}

      \item Probit regression model:
   $$
   \Phi^{-1}(\Ee(Y_i))= \beta_0 + \sum_{j=1}^{p-1} \beta_j X_{ij}$$
   where $\Phi$ is CDF of $N(0,1)$, i.e. $\Phi(t) = {\tt pnorm(t)}$.
   \item Complementary log-log model (cloglog):
   $$
   -log(-log(\Ee(Y_i)) = \beta_0 + \sum_{j=1}^{p-1} \beta_j X_{ij}.
   $$
   \item In logit, probit and cloglog $\V(Y_i)=\pi_i(1-\pi_i)$ but the model
   for the mean is different.

   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Generalized linear models}

   \begin{block}
   {Link \& variance functions}

   Given a dataset $(Y_i, X_{i1}, \dots, X_{ip}), 1 \leq i \leq n$
      we consider a model for the distribution of $Y|X_1, \dots, X_p$.
       \begin{itemize}

   \item If
   $$\eta_i=g(\Ee(Y_i)) = g(\mu_i) = \beta_0 + \sum_{j=1}^k \beta_j X_{ij}$$
   then $g$ is called the {\em link} function for the model.

   \item If
   $$
   \V(Y_i) = \phi \cdot V(\Ee(Y_i)) = \phi \cdot V(\mu_i)$$
   for $\phi > 0$ and some function $V$, then $V$ is the called {\em variance} function for the model.

   \item Canonical reference: {\em Generalized Linear Models}, McCullagh and Nelder.
       \end{itemize}
    \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Binary regression as GLM}

   \begin{block}
      {Binary (again)}
      \begin{itemize}

      \item For a logistic model, $$g(\mu)=\logit(\mu), \qquad V(\mu)=\mu(1-\mu).$$

      \item For a probit model, $$g(\mu)=\Phi^{-1}(\mu), \qquad V(\mu)=\mu(1-\mu).$$

      \item For a cloglog model, $$g(\mu)=-\log(-\log(\mu)), \qquad V(\mu)=\mu(1-\mu).$$

      \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} 

   \end{frame}

   \end{document}
