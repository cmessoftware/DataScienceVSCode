{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load_ext rmagic"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Penalized regression"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "Topics\n",
      "\n",
      "* Bias-Variance tradeoff.\n",
      "* Penalized regression.\n",
      "* Cross-validation."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Penalized regression"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "Bias-variance tradeoff \n",
      "\n",
      "* Arguably, the goal of a regression analysis is to \"build\" a model that predicts well.\n",
      "* What does \"predict well\" mean? $\\begin{aligned}\n",
      "     MSE_{pop}({{\\cal M}}) &= {\\mathbb{E}}\\left((Y_{new} - \\widehat{Y}_{new,{\\cal M}})^2\\right) \\\\\n",
      "     &=\n",
      "     {\\text{Var}}(Y_{\\new}) + {\\text{Var}}(\\widehat{Y}_{new,{\\cal M}}) +\n",
      "     \\\\\n",
      "     & \\qquad \\quad \\text{Bias}(\\widehat{Y}_{new,{\\cal M}})^2.\n",
      "     \\end{aligned}$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Bias-Variance Tradeoff"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "Simulation\n",
      "\n",
      "1. Generate $Y_{10 \\times 1} \\sim N(\\mu \\pmb{1}, I_{10 \\times 10})$, with $\\mu=0.5$.\n",
      "1. For $0 \\leq \\alpha \\leq 1$, set $\\hat{Y}(\\alpha) = \\alpha \\bar{Y}.$\n",
      "1. Compute $MSE(\\hat{Y}(\\alpha)) = \\sum_{i=1}^{10} (\\hat{Y}_{\\alpha} - 0.5)^2$\n",
      "1. Repeat 100 times, plot average of $MSE(\\hat{Y}(\\alpha))$."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Bias-Variance Tradeoff"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%R\n",
      "nsample <- 10\n",
      "ntrial <- 100\n",
      "mu <- 0.5\n",
      "\n",
      "MSE <- function(mu.hat, mu) {\n",
      "  return(sum((mu.hat - mu)^2))\n",
      "}\n",
      "\n",
      "alpha <- seq(0.0,1,length=20)\n",
      "\n",
      "mse <- numeric(length(alpha))\n",
      "\n",
      "for (i in 1:ntrial) {\n",
      "  Z <- rnorm(nsample) + mu\n",
      "  for (j in 1:length(alpha)) {\n",
      "    mse[j] <- mse[j] + MSE(alpha[j] * mean(Z), mu) / ntrial\n",
      "  }\n",
      "}\n",
      "\n",
      "plot(alpha, mse, type='l', lwd=2, col='red', ylim=c(0, max(mse)),\n",
      "     xlab='Shrinkage parameter, alpha', ylab='MSE(alpha)')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Penalized regression"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "Shrinkage & Penalties\n",
      "\n",
      "* Shrinkage can be thought of as \"constrained\" minimization.\n",
      "* Minimize $\\sum_{i=1}^n (Y_i - \\mu)^2 \\quad \\text{subject to $\\mu^2 \\leq C$}$\n",
      "* Lagrange: equivalent to minimizing $\\sum_{i=1}^n (Y_i - \\mu)^2 + \\lambda \\mu^2$ for some $\\lambda=\\lambda_C$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Penalized regression"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "Shrinkage & Penalties\n",
      "\n",
      "* Differentiating: $- 2 \\sum_{i=1}^n (Y_i - \\widehat{\\mu}_{\\lambda}) + 2 \\lambda \\widehat{\\mu}_{\\lambda} = 0$\n",
      "* Solving $\\widehat{\\mu}_{\\lambda} = \\frac{\\sum_{i=1}^n Y_i}{n + \\lambda} = \\frac{n}{n+\\lambda} \\overline{Y}.$\n",
      "* As $\\lambda \\rightarrow 0$, $\\widehat{\\mu}_{\\lambda} \\rightarrow {\\overline{Y}}.$\n",
      "* As $\\lambda \\rightarrow \\infty$ $\\widehat{\\mu}_{\\lambda} \\rightarrow 0.$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Penalized regression"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "Penalties & Priors\n",
      "\n",
      "* Minimizing $\\sum_{i=1}^n (Y_i - \\mu)^2 + \\lambda \\mu^2$ is similar to computing \"MLE\" of $\\mu$ if the likelihood was proportional to $\\exp \\left(-\\frac{1}{2\\sigma^2}\\left( \\sum_{i=1}^n (Y_i - \\mu)^2 + \\lambda \\mu^2\\right) \\right).$\n",
      "* If $\\lambda=m$, an integer, then $\\widehat{\\mu}_{\\lambda}$ is the sample mean of $(Y_1, \\dots, Y_n,0 ,\\dots, 0) \\in \\mathbb{R}^{n+m}$.\n",
      "* This is equivalent to adding some data with $Y=0$ \u2026"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Penalized regression"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "Biased regression: penalties \n",
      "\n",
      "* Not all biased models are better \u2013 we need a way to find \"good\" biased models.\n",
      "* Inference ($F$, $\\chi^2$ tests, etc) is not quite exact for biased models.\n",
      "* Generalized one sample problem: penalize large values of ${\\beta_{}}$. This should lead to \"multivariate\" shrinkage of the vector ${\\beta_{}}$.\n",
      "* Heuristically, \"large ${\\beta_{}}$\" is interpreted as \"complex model\". Goal is really to penalize \"complex\" models, i.e. Occam\u2019s razor.\n",
      "* If truth really is complex, this may not work! (But, it will then be hard to build a good model anyways ... (statistical lore))"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Penalized regression"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "How much to shrink, choosing $\\alpha$\n",
      "\n",
      "* In our one-sample example,\n",
      "* $\\begin{aligned}\n",
      "     MSE_{pop}(\\alpha) &= {\\text{Var}}( \\alpha \\bar{Y}) + \\text{Bias}(\\alpha \\bar{Y})^2 \\\\\n",
      "     &= \\frac{\\alpha^2 \\sigma^2}{n} + \\mu^2 (1 - \\alpha)^2\n",
      "     \\end{aligned}$\n",
      "* Differentiating and solving: $\\begin{aligned}\n",
      "     0 &= -2 \\mu^2(1 - \\lambda^*) + 2 \\frac{\\lambda^* \\sigma^2}{n}  \\\\\n",
      "     \\lambda^* & = \\frac{\\mu^2}{\\mu^2+\\sigma^2/n} \\\\\n",
      "     &= \\frac{0.5^2}{0.5^2+1/10} = 0.71\n",
      "     \\end{aligned}$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Bias-Variance Tradeoff"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%R\n",
      "nsample <- 10\n",
      "ntrial <- 100\n",
      "mu <- 0.5\n",
      "\n",
      "\n",
      "MSE <- function(mu.hat, mu) {\n",
      "  return(sum((mu.hat - mu)^2))\n",
      "}\n",
      "\n",
      "alpha <- seq(0.0,1,length=20)\n",
      "\n",
      "mse <- numeric(length(alpha))\n",
      "\n",
      "for (i in 1:ntrial) {\n",
      "  Z <- rnorm(nsample) + mu\n",
      "  for (j in 1:length(alpha)) {\n",
      "    mse[j] <- mse[j] + MSE(alpha[j] * mean(Z), mu) / ntrial\n",
      "  }\n",
      "}\n",
      "\n",
      "plot(alpha, mse, type='l', lwd=2, col='red', ylim=c(0, max(mse)),\n",
      "     xlab='Shrinkage parameter, alpha', ylab='MSE(alpha)')\n",
      "\n",
      "abline(v=mu^2/(mu^2+1/nsample), col='blue', lty=2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Penalized regression"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "Ridge regression\n",
      "\n",
      "* Assume that columns $(X_j)_{1 \\leq j \\leq p}$ have zero mean, and length 1 and $Y$ has zero mean.\n",
      "* This is called the *standardized model*\n",
      "  .\n",
      "* Minimize $SSE_{\\lambda}({\\beta_{}}) = \\sum_{i=1}^n \\left(Y_i - \\sum_{j=1}^{p} X_{ij} {\\beta_{j}}\\right)^2 + \\lambda \\sum_{j=1}^{p} {\\beta_{j}}^2.$\n",
      "  \n",
      "* Corresponds (through Lagrange multiplier) to a quadratic constraint on ${\\beta_{}}$\u2019s."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Penalized regression"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "Solving the normal equations\n",
      "\n",
      "* Normal equations $\\frac{\\partial}{\\partial {\\beta_{l}}} SSE_{\\lambda}({\\beta_{}}) = -2  \\langle Y - X{\\beta_{}}, X_l \\rangle + 2 \\lambda {\\beta_{l}}$\n",
      "* $-2 \\langle Y - X{\\widehat{\\beta}_{\\lambda}}, X_l \\rangle + 2 \\lambda {\\widehat{\\beta}_{l,\\lambda}} = 0, \\qquad 1 \\leq l \\leq p$\n",
      "* In matrix form $-X^TY +  (X^TX + \\lambda I) {\\widehat{\\beta}_{\\lambda}} = 0$\n",
      "* Or ${\\widehat{\\beta}_{\\lambda}} = (X^TX + \\lambda I)^{-1} X^TY.$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Ridge regression"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%R\n",
      "library(lars)\n",
      "data(diabetes)\n",
      "\n",
      "library(MASS)\n",
      "\n",
      "diabetes.ridge <- lm.ridge(diabetes$y ~ diabetes$x, lambda=seq(0,10,0.05))\n",
      "\n",
      "plot(diabetes.ridge)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Penalized regression"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "Choosing $\\lambda$: cross-validation \n",
      "\n",
      "* If we knew $MSE$ as a function of $\\lambda$ then we would simply choose the $\\lambda$ that minimizes $MSE$.\n",
      "* To do this, we need to estimate $MSE$.\n",
      "* A popular method is cross-validation as a function of $\\lambda$. Breaks the data up into smaller groups and uses part of the data to predict the rest.\n",
      "* We saw this in diagnostics (Cook\u2019s distance measured the fit with and without each point in the data set) and model selection."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Penalized regression"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "$K$-fold cross-validation \n",
      "\n",
      "* Fix a model (i.e. fix $\\lambda$). Break data set into $K$ approximately equal sized groups $(G_1, \\dots, G_K)$.\n",
      "* for (i in 1:K)\n",
      "   Use all groups except $G_i$ to fit model, predict outcome in group $G_i$ based on this model $\\widehat{Y}_{j(i),\\lambda}, j \\in G_i$.\n",
      "* Estimate $CV(\\lambda) = \\frac{1}{n}\\sum_{i=1}^K \\sum_{j \\in G_i} (Y_j - \\widehat{Y}_{j(i),\\lambda})^2.$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Penalized regression"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%R\n",
      " ntrial = 500\n",
      " mu = 0.08\n",
      " nsample = 200\n",
      "\n",
      " MSE = function(mu.hat, mu) {\n",
      "     return(sum((mu.hat - mu)^2))\n",
      " }\n",
      "\n",
      "\n",
      " # K fold cross-validation\n",
      "\n",
      " K = 10\n",
      " CV = function(Z, alpha) {\n",
      "     cve = numeric(K)\n",
      "     l = length(Z)\n",
      "     for (i in 1:K) {\n",
      "         g = c((i-1)*l/K+1,i*l/K)\n",
      "         mu.hat = mean(Z[-g]) * alpha\n",
      "         cve[i] = sum((Z[g]-mu.hat)^2)\n",
      "     }\n",
      "     return(c(mean(cve), sd(cve)))\n",
      " }\n",
      "\n",
      " alpha = seq(0.0,1,length=20)\n",
      "\n",
      " mse = numeric(length(alpha))\n",
      " cv = numeric(length(alpha))\n",
      " cv.sd = numeric(length(alpha))\n",
      "\n",
      " for (i in 1:ntrial) {\n",
      "     Z = rnorm(nsample) + mu\n",
      "     for (j in 1:length(alpha)) {\n",
      "         mse[j] = mse[j] + MSE(alpha[j] * mean(Z), mu) / ntrial\n",
      "         d = CV(Z, alpha[j])\n",
      "         cv[j] = cv[j] + d[1] / ntrial\n",
      "         cv.sd[j] = cv.sd[j] + d[2] / ntrial\n",
      "     }\n",
      " }\n",
      "\n",
      " plot(alpha, cv, type='l', lwd=2, col='green',\n",
      "    xlab='Shrinkage parameter, alpha', ylab='CV(alpha)')\n",
      " abline(v=mu^2/(mu^2+1/nsample), col='blue', lty=2)\n",
      " abline(v=min(alpha[cv == min(cv)]), col='red', lty=2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Penalized regression"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "$K$-fold cross-validation (continued) \n",
      "\n",
      "* It is a general principle that can be used in other situations, not just for Ridge.\n",
      "* Pros (partial list): \"objective\" measure of a model.\n",
      "* Cons (partial list): inference is, strictly speaking, \"out the window\" (also true for other model selection procedures in theory).\n",
      "* If goal is not really inference about certain specific parameters, it is a reasonable way to compare models."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Penalized regression"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      " Generalized Cross Validation\n",
      "\n",
      "* A computational shortcut for $n$-fold cross-validation (also known as leave-one out cross-validation).\n",
      "* Let $S_{\\lambda} = (X^TX + \\lambda I)^{-1} X^T$ be the matrix in ridge regression.\n",
      "* Then $GCV(\\lambda) =  \\frac{\\|Y - S_{\\lambda}Y\\|^2}{n - {\\text{Tr}}(S_{\\lambda})}.$\n",
      "* The quantity ${\\text{Tr}}(S_{\\lambda})$ is the *effective degrees of freedom*\n",
      "  ."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Ridge regression"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%R\n",
      "plot(diabetes.ridge$lambda, diabetes.ridge$GCV, xlab='Lambda', ylab='GCV', type='l', lwd=3, col='orange')\n",
      "\n",
      "\n",
      "select(diabetes.ridge)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Penalized regression"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "LASSO regression\n",
      "\n",
      "* Another popular penalized regression technique.\n",
      "* Use the standardized model.\n",
      "* Minimize $SSE_{\\lambda}({\\beta_{}}) = \\sum_{i=1}^n \\left(Y_i - \\sum_{j=1}^{p} X_{ij} {\\beta_{j}}\\right)^2 + \\lambda \\sum_{j=1}^p |{\\beta_{j}}|.$\n",
      "* Corresponds (through Lagrange multiplier) to an $\\ell^1$ constraint on ${\\beta_{}}$\u2019s. In theory, it works well when many ${\\beta_{j}}$\u2019s are 0 and gives \"sparse\" solutions unlike ridge."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Penalized regression"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "Why do we get sparse solutions with the LASSO?"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "LASSO"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%R\n",
      "library(lars)\n",
      "data(diabetes)\n",
      "\n",
      "diabetes.lasso = lars(diabetes$x, diabetes$y, type='lasso')\n",
      "plot(diabetes.lasso)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Penalized regression"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "Solving the normal equations\n",
      "\n",
      "* Normal equations $\\frac{\\partial}{\\partial {\\beta_{l}}} SSE_{\\lambda}({\\beta_{}}) = -2  \\langle Y - X{\\beta_{}}, X_l \\rangle +  \\lambda \\frac{\\partial}{\\partial{\\beta_{l}}} \\|{\\beta_{}}\\|_1$\n",
      "* Problem: the $\\ell_1$ norm is not differentiable.\n",
      "* Normal equations are replaced by KKT (Karush-Kuhn-Tucker) conditions. $-X^TY +  (X^TX + \\lambda I) {\\widehat{\\beta}_{\\lambda}} = \\lambda \\cdot s({\\widehat{\\beta}_{\\lambda}}).$ where $s({\\widehat{\\beta}_{\\lambda}})$ is a *subgradient*\n",
      "   of the $\\ell_1$ norm at ${\\widehat{\\beta}_{\\lambda}}$ \u2026"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Penalized regression"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "LASSO regression\n",
      "\n",
      "* Equivalent to the dual problem:\n",
      "* Minimize $\\sum_{i=1}^n (Y_i - r_i)^2  \\ \\text{subject to} \\ |X_j^Tr| \\leq \\lambda, 1 \\leq j \\leq p.$\n",
      "* So, instead of finding closest point to a plane, it finds closest (residual) point to a polygon..."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Penalized regression"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "LASSO regression\n",
      "\n",
      "* Scales to huge data because for large $\\lambda$ solutions are *sparse*\n",
      "  .\n",
      "* If there is a good model that is sparse, then LASSO can usually find a good model."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "LASSO: cross-validation"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%R\n",
      "cv.lars(diabetes$x, diabetes$y, K=10, type='lasso')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Penalized regression"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "Elastic Net\n",
      "\n",
      "* Mix between LASSO and ridge regression.\n",
      "* Minimize $SSE_{\\lambda}({\\beta_{}}) = \\sum_{i=1}^n \\left(Y_i - \\sum_{j=1}^{p-1} X_{ij} {\\beta_{j}}\\right)^2 + \\lambda_1 \\|{\\beta_{\\|}}_1 + \\lambda_2 \\|{\\beta_{\\|}}_2^2.$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Elastic Net"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%R\n",
      "library(lars)\n",
      "library(elasticnet)\n",
      "data(diabetes)\n",
      "\n",
      "diabetes.enet = enet(diabetes$x, diabetes$y, 0.1)\n",
      "plot(diabetes.enet)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Elastic Net"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%R\n",
      "library(elasticnet)\n",
      "data(diabetes)\n",
      "\n",
      "diabetes.enet = enet(diabetes$x, diabetes$y, 10)\n",
      "plot(diabetes.enet)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}