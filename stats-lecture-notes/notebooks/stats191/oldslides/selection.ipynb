{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load_ext rmagic"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Topics"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "Outline\n",
      "* Goals of model selection.\n",
      "* Criteria to compare models.\n",
      "* (Some) model selection.\n",
      "* Bias- variance trade-off."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Election data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "Description\n",
      "\n",
      "Variable | Description\n",
      "--- | ---\n",
      "$V$ | votes for a presidential candidate\n",
      "$I$ | are they incumbent?\n",
      "$D$ | Democrat or Republican incumbent?\n",
      "$W$ | wartime election?\n",
      "$G$ | GDP growth rate in election year\n",
      "$P$ | (absolute) GDP deflator growth rate\n",
      "$N$ | number of quarters in which GDP growth rate $> 3.2\\%$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Model selection"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "Problem & Goals\n",
      "* When we have many predictors (with many possible interactions), it can be difficult to find a good model.\n",
      "* Which main effects do we include?\n",
      "* Which interactions do we include?\n",
      "* Model selection procedures try to *simplify / automate* this task.\n",
      "* Election data has $2^6=64$ different models with just main effects!"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Model selection"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "General comments\n",
      "* This is an \"unsolved\" problem in statistics: there are no magic procedures to get you the \"best model.\"\n",
      "* In some sense, model selection is \"data mining.\"\n",
      "* Data miners / machine learners often work with very many predictors.\n",
      "* Our model selection problem is generally at a much smaller scale than \"data mining\" problems."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Model selection"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "Hypothetical example\n",
      "* Suppose we fit a a model $F: \\quad Y_{n \\times 1} = X_{n \\times (p+1)} \\beta_{(p+1) \\times 1} + \\varepsilon_{n \\times 1}$ with predictors ${\\pmb X}_1, \\dots, {\\pmb X}_p$.\n",
      "* In reality, some of the $\\beta$\u2019s may be zero. Let\u2019s suppose that $\\beta_{j+1}= \\dots= \\beta_{p+1}=0$.\n",
      "* Then, any model that includes $\\beta_0, \\dots, \\beta_j$ is *correct*: which model gives the *best* estimates of $\\beta_0, \\dots, \\beta_j$?\n",
      "* Principle of *parsimony* (i.e. Occam\u2019s razor) says that the model with *only* $\\pmb{X}_1, \\dots, \\pmb{X}_j$ is \"best\"."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Model selection"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "Hypothetical example: continued\n",
      "* For simplicity, let\u2019s assume that $j=1$ so there is only one coefficient to estimate.\n",
      "* Then, because each model gives an *unbiased* estimate of $\\beta_1$ we can compare models based on $\\text{Var}(\\widehat{\\beta}_1).$\n",
      "* The best model, in terms of this variance, is the one containing only ${\\pmb X}_1$.\n",
      "* What if we didn\u2019t know that only $\\beta_1$ was non-zero (which we don\u2019t know in general)?"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Model selection"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "Strategies\n",
      "* To \"implement\" a model selection procedure, we first need a criterion or benchmark to compare two models.\n",
      "* Given a criterion, we also need a search strategy.\n",
      "* With a limited number of predictors, it is possible to search all possible models (leaps in R)."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Model selection"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "Possible criteria \n",
      "* $R^2$: not a good criterion. Always increase with model size $\\implies$ \"optimum\" is to take the biggest model.\n",
      "* Adjusted $R^2$: better. It \"penalized\" bigger models. Follows principle of parsimony / Occam\u2019s razor.\n",
      "* Mallow\u2019s $C_p$ \u2013 attempts to estimate a model\u2019s predictive power, i.e. the power to predict a new observation."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Best subsets, $R^2$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%R\n",
      "url = 'http://stats191.stanford.edu/data/election.table'\n",
      "\n",
      "election.table = read.table(url, header=T)\n",
      "\n",
      "pairs(election.table[,2:ncol(election.table)], cex.labels=3, pch=23,\n",
      "      bg='orange', cex=2)\n",
      "\n",
      "# Leaps takes a design matrix as argument: throw away the intercept\n",
      "# column or leaps will complain\n",
      "\n",
      "election.lm = lm(V ~ I + D + W +G:I + P + N, election.table)\n",
      "X = model.matrix(election.lm)[,-1]\n",
      "\n",
      "library(leaps)\n",
      "election.leaps = leaps(X, election.table$V, nbest=3, method='r2')\n",
      "plot(election.leaps$size, election.leaps$r2, pch=23, bg='orange', cex=2)\n",
      "best.model.r2 = election.leaps$which[which((election.leaps$r2 == max(election.leaps$r2))),]\n",
      "best.model.r2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Best subsets, adjusted $R^2$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%R\n",
      "election.leaps = leaps(X, election.table$V, nbest=3, method='adjr2')\n",
      "plot(election.leaps$size, election.leaps$adjr2, pch=23, bg='orange', cex=2)\n",
      "best.model.adjr2 = election.leaps$which[which((election.leaps$adjr2 == max(election.leaps$adjr2))),]\n",
      "best.model.adjr2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Model selection"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "Mallow\u2019s $C_p$\n",
      "* $C_p({\\cal M}) = \\frac{SSE({\\cal M})}{\\widehat{\\sigma}^2} + 2 \\cdot p({\\cal M}) - n.$\n",
      "* $\\widehat{\\sigma}^2=SSE(F)/df_F$ is the \"best\" estimate of $\\sigma^2$ we have (use the fullest model), i.e. in the election data it uses all 6 main effects.\n",
      "* $SSE({\\cal M})$ is the $SSE$ of the model ${\\cal M}$.\n",
      "* $p({\\cal M})$ is the number of predictors in ${\\cal M}$.\n",
      "* This is an estimate of the expected mean-squared error of $\\widehat{Y}({\\cal M})$, it takes *bias* and *variance* into account."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Best subsets, Mallow\u2019s $C_p$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%R\n",
      "election.leaps = leaps(X, election.table$V, nbest=3, method='Cp')\n",
      "plot(election.leaps$size, election.leaps$Cp, pch=23, bg='orange', cex=2)\n",
      "best.model.Cp = election.leaps$which[which((election.leaps$Cp == min(election.leaps$Cp))),]\n",
      "best.model.Cp"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Model selection"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "Search strategies \n",
      "* Given a criterion, we now have to decide how we are going to search through all possible models.\n",
      "* \"Best subset\": search all possible models and take the one with highest $R^2_a$ or lowest $C_p$ leaps\n",
      "* Stepwise (forward, backward or both): useful when the number of predictors is large. Choose an initial model and be \"greedy\".\n",
      "* \"Greedy\" means always take the biggest jump (up or down) in your selected criterion."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Model selection"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "Implementations in R \n",
      "* \"Best subset\": use the function leaps. Works only for multiple linear regression models.\n",
      "* Stepwise: use the function step. Works for any model with Akaike Information Criterion (AIC). In multiple linear regression, AIC is (almost) a linear function of $C_p$."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Model selection"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "Akaike / Bayes Information Criterion\n",
      "* Akaike (AIC) defined as $AIC({\\cal M}) = - 2 \\log L({\\cal M}) + 2 p({\\cal M})$ where $L({\\cal M})$ is the maximized likelihood of the model.\n",
      "* Bayes (BIC) defined as $BIC({\\cal M}) = - 2 \\log L({\\cal M}) + \\log n \\cdot p({\\cal M})$\n",
      "* Strategy can be used for whenever we have a likelihood, so this generalizes to many statistical models."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Model selection"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "Akaike / Bayes Information Criterion\n",
      "* In linear regression with unknown $\\sigma^2$ $-2 \\log L({\\cal M}) = n \\log(2\\pi \\widehat{\\sigma}^2_{MLE}) + n$ where $\\widehat{\\sigma}^2_{MLE} = \\frac{1}{n} SSE(\\widehat{\\beta})$\n",
      "* In linear regression with known $\\sigma^2$ $-2 \\log L({\\cal M}) = n \\log(2\\pi \\sigma^2) + \\frac{1}{\\sigma^2} SSE(\\widehat{\\beta})$ so AIC is very much like Mallow\u2019s $C_p$."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Model selection"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "Akaike / Bayes Information Criterion\n",
      "* BIC will always choose a model as small or smaller than AIC.\n",
      "* As our sample size grows, we can show that\n",
      "*     * AIC will (asymptotically) always choose a model that contains the true model, i.e. it won\u2019t leave any variables out.\n",
      "      * BIC will (asymptotically) choose exactly the right model."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Cross-validation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "$K$-fold cross-validation \n",
      "* Fix a model ${\\cal M}$. Break data set into $K$ approximately equal sized groups $(G_1, \\dots, G_K)$.\n",
      "* for (i in 1:K) Use all groups except $G_i$ to fit model, predict outcome in group $G_i$ based on this model $\\widehat{Y}_{j,{\\cal M}, G_i}, j \\in G_i$.\n",
      "* Estimate $CV({\\cal M}) = \\frac{1}{n}\\sum_{i=1}^K \\sum_{j \\in G_i} (Y_j - \\widehat{Y}_{j,{\\cal M},G_i})^2.$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Cross-validation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "Some facts about cross-validation.\n",
      "* It is a general principle that can be used in other situations to \"choose parameters.\"\n",
      "* Pros (partial list): \"objective\" measure of a model.\n",
      "* Cons (partial list): inference is, strictly speaking, \"out the window\" (also true for other model selection procedures).\n",
      "* If goal is not really inference about certain specific parameters, it is a reasonable way to compare models."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "$C_p$ versus 5-fold cross-validation"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%R\n",
      "library(boot)\n",
      "election.leaps = leaps(X, election.table$V, nbest=3, method='Cp')\n",
      "V = election.table$V\n",
      "election.leaps$cv = 0 * election.leaps$Cp\n",
      "for (i in 1:nrow(election.leaps$which)) {\n",
      "    subset = c(1:ncol(X))[election.leaps$which[i,]]\n",
      "    if (length(subset) > 1) {\n",
      "       Xw = X[,subset]\n",
      "       wlm = glm(V ~ Xw)\n",
      "       election.leaps$cv[i] = cv.glm(model.frame(wlm), wlm, K=5)$delta[1]\n",
      "    }\n",
      "    else {\n",
      "       Xw = X[,subset[1]]\n",
      "       wlm = glm(V ~ Xw)\n",
      "       election.leaps$cv[i] = cv.glm(model.frame(wlm), wlm, K=5)$delta[1]\n",
      "    }\n",
      "}\n",
      "plot(election.leaps$Cp, election.leaps$cv, pch=23, bg='orange', cex=2)\n",
      "X=3"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "5-fold cross-validation for all $C_p$ models"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%R\n",
      "plot(election.leaps$size, election.leaps$cv, pch=23, bg='orange', cex=2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Model selection"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "Caveats\n",
      "* Many other \"criteria\" have been proposed.\n",
      "* Some work well for some types of data, others for different data.\n",
      "* Check diagnostics!\n",
      "* These criteria (except cross-validation) are not \"direct measures\" of predictive power, though Mallow\u2019s $C_p$ is a step in this direction.\n",
      "* $C_p$ measures the quality of a model based on both *bias* and *variance* of the model. Why is this important?\n",
      "* *Bias-variance* tradeoff is ubiquitous in statistics."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Bias-variance tradeoff"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "Comparing estimators\n",
      "* When an estimator $\\widehat{\\beta}_1$ of $\\beta_1$ is unbiased: $E((\\widehat{\\beta}_1 - \\beta_1)^2) = \\text{Var}(\\widehat{\\beta}_1)$ so it makes sense to compare unbiased estimators in terms of variance.\n",
      "* Even for biased estimators, the LHS makes sense, called the *mean squared error* of $\\widehat{\\beta}_1$ $\\begin{aligned}\n",
      "     MSE(\\widehat{\\beta}_1) &= E((\\widehat{\\beta}_1 - \\beta_1)^2) \\\\\n",
      "     &= \\text{Var}(\\widehat{\\beta}_1) + \\text{Bias}(\\widehat{\\beta}_1)^2\n",
      "     \\end{aligned}$\n",
      "* Paradoxically, it is sometimes possible to reduce $MSE$ by *biasing* the estimator."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Bias-variance tradeoff"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "Shrinking toward zero\n",
      "* Suppose we observe $Y_i \\sim N(\\mu_i, 1), 1 \\leq i \\leq n$ and our goal is to estimate the entire vector $\\mu$.\n",
      "* Minimum variance unbiased estimator is $\\widehat{\\mu}_i = Y_i, \\qquad 1 \\leq i \\leq n.$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Bias-variance tradeoff"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "Shrinking toward zero\n",
      "* How good an estimator is $\\widehat{\\mu}$? $MSE(\\widehat{\\mu}, \\mu) = \\frac{1}{n} E(\\sum_{i=1}^n (\\widehat{\\mu}_i -\\mu_i)^2)  = 1.$\n",
      "* *However*, we can improve on the MSE very simply by *shrinking* $\\widehat{\\mu}$ toward 0.\n",
      "* Define $\\widehat{\\mu}^{\\alpha}_i = \\alpha \\cdot Y_i, \\qquad 1 \\leq i \\leq n, 0 \\leq \\alpha \\leq 1.$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Shrinking an estimator"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%R\n",
      "nsample = 100\n",
      "ntrial = 50\n",
      "mu = 5 * c(1:nsample) / nsample\n",
      "mu = mu - mean(mu)\n",
      "\n",
      "get.sample = function() {\n",
      "  return(rnorm(nsample) + mu)\n",
      "}\n",
      "\n",
      "MSE = function(mu.hat, mu) {\n",
      "  return(sum((mu.hat - mu)^2) / length(mu))\n",
      "}\n",
      "\n",
      "alpha = seq(0.0,1,length=20)\n",
      "\n",
      "mse = numeric(length(alpha))\n",
      "\n",
      "for (i in 1:ntrial) {\n",
      "  Z = get.sample()\n",
      "  for (j in 1:length(alpha)) {\n",
      "    mse[j] = mse[j] + MSE(alpha[j] * Z, mu) / ntrial\n",
      "  }\n",
      "}\n",
      "\n",
      "plot(alpha, mse, type='l', lwd=2, col='red', ylim=c(0, max(mse)),\n",
      "     xlab='Shrinkage parameter, alpha', ylab='MSE(alpha)')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}