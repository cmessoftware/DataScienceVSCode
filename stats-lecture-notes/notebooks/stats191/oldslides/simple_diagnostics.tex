   \documentclass[handout]{beamer}



   \mode<presentation>
   {
     \usetheme{PaloAlto}
   \setbeamertemplate{footline}[page number]

     \setbeamercolor{sidebar}{bg=white, fg=black}
     \setbeamercolor{frametitle}{bg=white, fg=black}
     % or ...
     \setbeamercolor{logo}{bg=white}
     \setbeamercolor{block body}{parent=normal text,bg=white}
     \setbeamercolor{author in sidebar}{fg=black}
     \setbeamercolor{title in sidebar}{fg=black}


     \setbeamercolor*{block title}{use=structure,fg=structure.fg,bg=structure.fg!20!bg}
     \setbeamercolor*{block title alerted}{use=alerted text,fg=alerted text.fg,bg=alerted text.fg!20!bg}
     \setbeamercolor*{block title example}{use=example text,fg=example text.fg,bg=example text.fg!20!bg}


     \setbeamercolor{block body}{parent=normal text,use=block title,bg=block title.bg!50!bg}
     \setbeamercolor{block body alerted}{parent=normal text,use=block title alerted,bg=block title alerted.bg!50!bg}
     \setbeamercolor{block body example}{parent=normal text,use=block title example,bg=block title example.bg!50!bg}

     % or ...

     \setbeamercovered{transparent}
     % or whatever (possibly just delete it)
     \logo{\resizebox{!}{1.5cm}{\href{\basename{R}}{\includegraphics{image}}}}
   }

   \mode<handout>
   {
     \usetheme{PaloAlto}
     \usecolortheme{default}
     \setbeamercolor{sidebar}{bg=white, fg=black}
     \setbeamercolor{frametitle}{bg=white, fg=black}
     % or ...
     \setbeamercolor{logo}{bg=white}
     \setbeamercolor{block body}{parent=normal text,bg=white}
     \setbeamercolor{author in sidebar}{fg=black}
     \setbeamercolor{title in sidebar}{fg=black}
     \setbeamercovered{transparent}
     % or whatever (possibly just delete it)
     \logo{}
   }

   \usepackage{epsdice,listings,epic}
   \usepackage[latin1]{inputenc}
   \usepackage{graphicx}
   \usepackage{amsmath,eepic,epic,algorithm}

   \newcommand{\figslide}[3]{
   \begin{frame}
   \frametitle{#1}
     \begin{center}
     \resizebox{!}{2.7in}{\includegraphics{#2}}    
     \end{center}
   {#3}
   \end{frame}
   }

   \newcommand{\fighslide}[4]{
   \begin{frame}
   \frametitle{#1}
     \begin{center}
     \resizebox{!}{#4}{\includegraphics{#2}}    
     \end{center}
   {#3}
   \end{frame}
   }

   \newcommand{\figwref}[1]{
   \href{#1}{\tiny \tt #1}}

   \newcommand{\unsupervised}[1]{{\color{red} #1}}
   \newcommand{\supervised}[1]{{\color{green} #1}}
   \newcommand{\argmax}{\mathop{\mathrm{argmax}}}
   \newcommand{\argmin}{\mathop{\mathrm{argmin}}}
   \newcommand{\minimize}{\mathop{\mathrm{minimize}}}
   \newcommand{\maximize}{\mathop{\mathrm{maximize}}}

   \newcommand{\B}[1]{\beta_{#1}}
   \newcommand{\Bh}[1]{\widehat{\beta}_{#1}}
   \newcommand{\V}{\text{Var}}
   \newcommand{\Cov}{\text{Cov}}
   \newcommand{\Vh}{\widehat{\V}}
   \newcommand{\s}{\sigma}
   \newcommand{\sh}{\widehat{\sigma}}

   \newcommand{\argmax}[1]{\mathop{\text{argmax}}_{#1}}
   \newcommand{\argmin}[1]{\mathop{\text{argmin}}_{#1}}
   \newcommand{\Ee}{\mathbb{E}}
   \newcommand{\Pp}{\mathbb{P}}
   \newcommand{\real}{\mathbb{R}}
   \newcommand{\Ybar}{\overline{Y}}
   \newcommand{\Yh}{\widehat{Y}}
   \newcommand{\Xbar}{\overline{X}}
   \newcommand{\Tr}{\text{Tr}}


   \newcommand{\model}{{\cal M}}

   \newcommand{\figvskip}{-0.7in}
   \newcommand{\fighskip}{-0.3in}
   \newcommand{\figheight}{3.5in}

   \newcommand{\Rcode}[1]{{\bf \tt #1 }}
   \newcommand{\Rtcode}[1]{{\tiny \bf \tt #1 }}
   \newcommand{\Rscode}[1]{{\small \bf \tt #1 }}

   \newcommand{\RR}{{\tt R} \;}
   \newcommand{\basename}[1]{http://stats191.stanford.edu/#1}
   \newcommand{\dataname}[1]{\basename{data/#1}}
   \newcommand{\Rname}[1]{\basename{R/#1}}

   \newcommand{\mycolor}[1]{{\color{blue} #1}}
   \newcommand{\basehref}[2]{\href{\basename{#1}}{\mycolor{#2}}}
   \newcommand{\Rhref}[2]{\href{\basename{R/#1}}{\mycolor{#2}}}
   \newcommand{\datahref}[2]{\href{\dataname{#1}}{\mycolor{#2}}}
   \newcommand{\X}{\pmb{X}}
   \newcommand{\Y}{\pmb{Y}}
   \newcommand{\be}{\pmb{varepsilon}}
   \newcommand{\logit}{\text{logit}}


   \title{Statistics 191: Introduction to Applied Statistics}
   \subtitle{Diagnostics for simple linear regression  (Ch. 2 and some of 4, RABE)}
   \author{\copyright Jonathan Taylor \\
   }
   %}


   \begin{document}

   \begin{frame}
   \titlepage
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Outline}

   \begin{block}{Diagnostics for simple regression}
   \begin{itemize}
   \item Goodness of fit of regression: analysis of variance.

   \item $F$-statistics.

   \item Residuals.

   \item Diagnostic plots.
   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Geometry of Least Squares}

   \resizebox{!}{2.5in}{\includegraphics{./figs/simple/axes_simple_full.png}}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Geometry of Least Squares}

   \resizebox{!}{2.5in}{\includegraphics{./figs/simple/axes_simple_reduced.png}}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Geometry of Least Squares}

   \resizebox{!}{2.5in}{\includegraphics{./figs/simple/axes_simple.png}}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Goodness of fit}

   \begin{block}
   {Sums of squares}
   $$
   \begin{aligned}
   SSE &= \sum_{i=1}^n(Y_i - \widehat{Y}_i)^2 = \sum_{i=1}^n (Y_i - \widehat{\beta}_0 - \widehat{\beta}_1 X_i)^2 \\
   SSR &= \sum_{i=1}^n(\overline{Y} - \widehat{Y}_i)^2 = \sum_{i=1}^n (\overline{Y} - \widehat{\beta}_0 - \widehat{\beta}_1 X_i)^2 \\
   SST &= \sum_{i=1}^n(Y_i - \overline{Y})^2 = SSE + SSR \\
   R^2 &= \frac{SSR}{SST} = 1 - \frac{SSE}{SST} = \widehat{Cor}(\pmb{X},\pmb{Y})^2.
   \end{aligned}
   $$


   Basic idea: if $R^2$ is large: a lot  of the variability in $\pmb{Y}$ is explained by $\pmb{X}$.

   \end{block}
   \end{frame}

   %CODE
       % X = seq(0,20, length=21)
   % Y = 0.5*X+1 + rnorm(21)
   % 
   % Y.lm = lm(Y~X)
   % 
   % p = predict(Y.lm)
   % m = mean(Y)
   % 
   % # SST: deviations of Y's around
   % # the horizontal line for the mean
   % 
   % plot(X, Y, pch=23, bg='red')
   % abline(h=m, col='yellow', lwd=2)
   % for (i in 1:21) {
   %   points(X[i], m, pch=23, bg='yellow')
   %   lines(c(X[i], X[i]), c(Y[i], m))
   % }
   % 
   % # show the regression line as well
   % 
   % abline(Y.lm, col='green', lwd=2)
   % 
   % 


   \begin{frame}
   \frametitle{Total sum of squares}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/b704968620.png}}    
   \end{center}
   \href{http://stats191.stanford.edu/simple_diagnostics.html#goodness-of-fit-sums-of-squares}{R code}
   \end{frame}

   %CODE
       % # SSE: deviations of Y's around
   % # the regression line
   % 
   % plot(X, Y, pch=23, bg='red')
   % abline(Y.lm, col='green', lwd=2)
   % for (i in 1:21) {
   %   points(X[i], p[i], pch=23, bg='green')
   %   lines(c(X[i], X[i]), c(Y[i], p[i]))
   % }
   % 
   % m = mean(Y)
   % abline(h=m, col='yellow', lwd=2)
   % 
   % 
   % 


   \begin{frame}
   \frametitle{Error sum of squares}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/6891d23643.png}}    
   \end{center}
   \href{http://stats191.stanford.edu/simple_diagnostics.html#goodness-of-fit-sums-of-squares}{R code}
   \end{frame}

   %CODE
       % # SSR: deviations of Yhat's around
   % # the horizontal line for the mean
   % 
   % plot(X, Y, pch=23, bg='red')
   % abline(Y.lm, col='green', lwd=2)
   % abline(h=m, col='yellow', lwd=2)
   % for (i in 1:21) {
   %   points(X[i], p[i], pch=23, bg='green')
   %   points(X[i], m, pch=23, bg='yellow')
   %   lines(c(X[i], X[i]), c(m, p[i]))
   % }
   % 
   % 


   \begin{frame}
   \frametitle{Regression sum of squares}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/0f6fb88c86.png}}    
   \end{center}
   \href{http://stats191.stanford.edu/simple_diagnostics.html#goodness-of-fit-sums-of-squares}{R code}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{$F$-statistics}

   \begin{block}
   {What is an $F$-statistic?}

   \begin{itemize}


   \item An $F$-statistic is a ratio of ``{\em sample variances (mean squares)}'': it has a numerator, $N$,  and a denominator, $D$ that are independent.


   \item Let $$N \sim \frac{\chi^2_{\rm num} }{ df_{{\rm num}}}, \qquad D \sim \frac{\chi^2_{\rm den} }{ df_{{\rm den}}}
   $$
   and define
   $$F = \frac{N}{D}.$$

   \item We say $F$ has an $F$ distribution with parameters $df_{{\rm num}}, df_{{\rm den}}$ and write $F \sim F_{df_{{\rm num}}, df_{{\rm den}}}$.
   \end{itemize}

   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Geometry of Least Squares}

   \resizebox{!}{2.5in}{\includegraphics{./figs/simple/axes_simple.png}}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{$F$-statistic in simple linear regression}

   \begin{block}
   {Goodness of fit $F$-statistic}
   \begin{itemize}
   \item The ratio $$
   F=\frac{SSR/1}{SSE/(n-2)} = \frac{MSR}{MSE}$$
   can be thought of as a {\em ratio of ``variances''}.

   \item In fact, under $H_0:\beta_1=0$, $$
   F \sim F_{1, n-2}
   $$
   because
   $$
   \begin{aligned}
   SSR &= \|\widehat{\pmb{Y}} - \overline{Y} \cdot \pmb{1}\|^2 \\
   SSE &= \|\pmb{Y} - \widehat{\pmb{Y}}\|^2
   \end{aligned}
   $$
   and from our picture, these vectors are orthogonal.
   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{$F$ and $t$ statistics}

   \begin{block}
   {Relation between $F$ and $t$}
   \begin{itemize}
   \item If $T \sim t_{\nu}$, then
   $$
   T^2 \sim \frac{N(0,1)^2}{\chi^2_{\nu}/\nu} \sim \frac{\chi^2_1/1}{\chi^2_{\nu}/\nu}.$$

   \item In other words, the square of a $t$-statistic is an $F$-statistic.
   Because it is always positive, an $F$-statistic has no {\em direction $(\pm)$} associated with it.


   \item In fact, (see \href{http://stats191.stanford.edu/simple_diagnostics.html#poorly-fitting-linear-model}{R code})
   $$
   F = \frac{MSR}{MSE} = \frac{\widehat{\beta}_1^2}{SE(\widehat{\beta}_1)^2}.$$

   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{$F$-statistics in regression models}

   \begin{block}
   {Interpretation of an $F$-statistic}

   \begin{itemize}

   \item In regression, the numerator is usually a difference in ``goodness'' of fit of two  (nested) models.

   \item The denominator is $\widehat{\sigma}^2$ -- an estimate of $\sigma^2$.

   \item Our example today: the bigger model is the simple linear regression model, the smaller is the model with constant mean (one sample model).

   \item If the $F$ is large, it says that the ``bigger''  model explains a lot more variability in $\pmb{Y}$  (relative to $\sigma^2$) than the smaller one.

   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{$F$-test in simple linear regression}

   \begin{block}
   {Example in more detail}
   \begin{itemize}
   \item {\em Full (bigger) model :}
   $$
   FM: \qquad Y_i = \beta_0 + \beta_1 X_i + \varepsilon_i$$
   \item {\em Reduced (smaller) model:}
   $$
   RM: \qquad Y_i = \beta_0  + \varepsilon_i$$

   \item The $F$-statistic has the form
   $$
   F=\frac{(SSE(RM) - SSE(FM)) / (df_{RM} - df_{FM})}{SSE(FM) / df_{FM}}.$$
   \item Reject $H_0: RM$ is correct, if $F > F_{1-\alpha, 1, n-2}$.

   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Diagnostics}

   \begin{block}
   {What are the assumptions}
   \begin{itemize}
   \item $$
   Y_i = \beta_0 + \beta_1 X_{i} + \varepsilon_i
   $$
   \item Errors $\varepsilon_i$ are assumed independent $N(0,\sigma^2)$.
   \end{itemize}

   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Diagnostics}

   \begin{block}
   {What can go wrong?}

   \begin{itemize}

   \item
   Regression function can be wrong: maybe regression function should be quadratic
   (see \href{http://stats191.stanford.edu/simple_diagnostics.html#poorly-fitting-linear-model}{R code}).

   \item Model for the errors
   may be incorrect:
   \begin{itemize}
   \item  may not be normally distributed.
   \item  may not be independent.

   \item  may not have the same variance.
   \end{itemize}

   \item Detecting problems is more {\em art} then {\em science}, i.e.
   we cannot {\em test} for all possible problems in a regression model.

   \item Basic idea of diagnostic measures: if model is correct then
   residuals $e_i = Y_i -\widehat{Y}_i, 1 \leq i \leq n$ should look like a sample of
   (not quite independent) $N(0, \sigma^2)$ random variables.
   \end{itemize}
   \end{block}
   \end{frame}

   %CODE
       % 
   % url = "http://stats191.stanford.edu/data/anscombe.table"
   % anscombe <- read.table(url, header=T)
   % 
   % # Another little R tip...
   % # If you don't attach a file, you need
   % # to use "$" notation to get the variables
   % 
   % # Add some noise because the curve is almost quadratic already
   % anscombe$Y2 = anscombe$Y2 + rnorm(length(anscombe$Y2)) * 0.45
   % 
   % plot(anscombe$X2, anscombe$Y2, pch=23, bg='orange', cex=2, ylab='Y', xlab='X')
   % 
   % simple.lm <- lm(anscombe$Y2 ~ anscombe$X2)
   % 
   % abline(simple.lm$coef, lwd=2, col='red', lty=2)
   % 
   % 


   \begin{frame}
   \frametitle{A bad simple linear regression model}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/2e8a393fc0.png}}    
   \end{center}
   \href{http://stats191.stanford.edu/simple_diagnostics.html#poorly-fitting-linear-model}{R code}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Diagnostic plots}

   \begin{block}
   {Problems in the regression function}
   \begin{itemize}

   \item True regression function may have higher-order non-linear terms, polynomial or otherwise.
   \item
   Can sometimes be remedied by looking at a plot of
   $\pmb{X}$ vs. residuals $\pmb{e}$. If there is any visible ``trend'' in this
   plot, may consider adding more terms to the model to capture this trend (this makes the model a {\em multiple regression model}).

   \end{itemize}
   \end{block}
   \end{frame}

   %CODE
       % 
   % plot(anscombe$X2, resid(simple.lm), ylab='Residual', xlab='X',
   %      pch=23, bg='orange', cex=2)
   % 
   % ## put a horizontal line through 0
   % abline(h=0, lwd=2, col='red', lty=2)
   % 
   % 


   \begin{frame}
   \frametitle{Residuals from linear model}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/26830ab3f8.png}}    
   \end{center}
   \href{http://stats191.stanford.edu/simple_diagnostics.html#poorly-fitting-linear-model}{R code}
   \end{frame}

   %CODE
       % 
   % # Improve model by adding quadratic term
   % # Instead of attaching, can use the "data" argument to lm
   % 
   % quadratic.lm <- lm(Y2 ~ poly(X2, 2), data=anscombe)
   % 
   % # Replot data, adding fitted quadratic
   % Xsort <- sort(anscombe$X2)
   % plot(anscombe$X2, anscombe$Y2, pch=23, bg='orange', cex=2, ylab='Y', xlab='X')
   % lines(Xsort, predict(quadratic.lm, list(X2=Xsort)), col='red', lty=2, lwd=2)
   % 
   % 


   \begin{frame}
   \frametitle{Quadratic model}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/20e15ae786.png}}    
   \end{center}
   \href{http://stats191.stanford.edu/simple_diagnostics.html#poorly-fitting-linear-model}{R code}
   \end{frame}

   %CODE
       % 
   % plot(anscombe$X2, resid(quadratic.lm), ylab='Residual',
   %      xlab='X', pch=23, bg='orange', cex=2)
   % ## put a horizontal line through 0
   % abline(h=0, lwd=2, col='red', lty=2)
   % 
   % 


   \begin{frame}
   \frametitle{Residuals from quadratic model}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/ef7e20b21e.png}}    
   \end{center}
   \href{http://stats191.stanford.edu/simple_diagnostics.html#poorly-fitting-linear-model}{R code}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Problems with the errors}

   \begin{block}
   {Possible problems \& diagnostic checks}
   \begin{itemize}

   \item
   Errors may not be normally distributed or may not have  the same variance -- {\tt qqnorm} can help with this.

   \item Variance may not be constant. Can also be addressed in a plot of
   $\pmb{X}$ vs. $\pmb{e}$: {\em fan shape} or other trend indicate
   non-constant variance.

   \item Outliers: points where the model really does not fit! Possibly mistakes in data transcription, lab errors, who knows? Should be recognized and (hopefully) explained.

   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Non-normality}

   \begin{block}
   {{\tt qqnorm}}
   \begin{itemize}
   \item If $e_i, 1\leq i \leq n$ were really a sample of
   $N(0, \sigma^2)$ then their sample quantiles should be close to the
   sample quantiles of the $N(0, \sigma^2)$ distribution.

   \item Plot:
   $$
   e_{(i)}  \ {\rm vs.} \  \Ee(\varepsilon_{(i)}), \qquad 1 \leq i \leq n.$$
   where $e_{(i)}$ is the $i$-th smallest residual (order statistic) and
   $\Ee(\varepsilon_{(i)})$ is the expected value for independent $\varepsilon_i$'s $\sim N(0,\sigma^2)$.
   \end{itemize}
   \end{block}
   \end{frame}

   %CODE
       % qqnorm(resid(simple.lm), pch=23, bg='orange', cex=2)


   \begin{frame}
   \frametitle{QQplot of residuals from linear model}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/d2641c1df6.png}}    
   \end{center}
   \href{http://stats191.stanford.edu/simple_diagnostics.html#poorly-fitting-linear-model}{R code}
   \end{frame}

   %CODE
       % qqnorm(resid(quadratic.lm), pch=23, bg='orange', cex=2)


   \begin{frame}
   \frametitle{QQplot of residuals from quadratic model}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/0669147262.png}}    
   \end{center}
   \href{http://stats191.stanford.edu/simple_diagnostics.html#poorly-fitting-linear-model}{R code}
   \end{frame}

   %CODE
       % url = 'http://stats191.stanford.edu/data/HIV.VL.table'
   % 
   % viral.load = read.table(url, header=T)
   % attach(viral.load)
   % 
   % plot(GSS, VL, pch=23, bg='orange', cex=2)
   % viral.lm = lm(VL ~ GSS)
   % abline(viral.lm, col='red', lwd=2)
   % 
   % ## let's remove the outlier
   % good = (VL < 250000)
   % viral.lm = lm(VL ~ GSS, subset=good)
   % abline(viral.lm, col='green', lwd=2)


   \begin{frame}
   \frametitle{Outlier and nonconstant variance}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/a4e3d1542a.png}}    
   \end{center}
   \href{http://stats191.stanford.edu/simple_diagnostics.html#outlier-and-nonconstant-variance}{R code}
   \end{frame}

   %CODE
       % plot(GSS[good], resid(viral.lm), pch=23,
   % bg='orange', cex=2, xlab='GSS', ylab='Residual')
   % abline(h=0, lwd=2, col='red', lty=2)


   \begin{frame}
   \frametitle{Outlier and nonconstant variance}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/0824cf9384.png}}    
   \end{center}
   \href{http://stats191.stanford.edu/simple_diagnostics.html#outlier-and-nonconstant-variance}{R code}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} 

   \end{frame}

   \end{document}
