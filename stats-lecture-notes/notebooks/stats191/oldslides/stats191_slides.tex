   \documentclass[handout]{beamer}



   \mode<presentation>
   {
     \usetheme{PaloAlto}
   \setbeamertemplate{footline}[page number]

     \setbeamercolor{sidebar}{bg=white, fg=black}
     \setbeamercolor{frametitle}{bg=white, fg=black}
     % or ...
     \setbeamercolor{logo}{bg=white}
     \setbeamercolor{block body}{parent=normal text,bg=white}
     \setbeamercolor{author in sidebar}{fg=black}
     \setbeamercolor{title in sidebar}{fg=black}


     \setbeamercolor*{block title}{use=structure,fg=structure.fg,bg=structure.fg!20!bg}
     \setbeamercolor*{block title alerted}{use=alerted text,fg=alerted text.fg,bg=alerted text.fg!20!bg}
     \setbeamercolor*{block title example}{use=example text,fg=example text.fg,bg=example text.fg!20!bg}


     \setbeamercolor{block body}{parent=normal text,use=block title,bg=block title.bg!50!bg}
     \setbeamercolor{block body alerted}{parent=normal text,use=block title alerted,bg=block title alerted.bg!50!bg}
     \setbeamercolor{block body example}{parent=normal text,use=block title example,bg=block title example.bg!50!bg}

     % or ...

     \setbeamercovered{transparent}
     % or whatever (possibly just delete it)
     \logo{\resizebox{!}{1.5cm}{\href{\basename{R}}{\includegraphics{image}}}}
   }

   \mode<handout>
   {
     \usetheme{PaloAlto}
     \usecolortheme{default}
     \setbeamercolor{sidebar}{bg=white, fg=black}
     \setbeamercolor{frametitle}{bg=white, fg=black}
     % or ...
     \setbeamercolor{logo}{bg=white}
     \setbeamercolor{block body}{parent=normal text,bg=white}
     \setbeamercolor{author in sidebar}{fg=black}
     \setbeamercolor{title in sidebar}{fg=black}
     \setbeamercovered{transparent}
     % or whatever (possibly just delete it)
     \logo{}
   }

   \usepackage{epsdice,listings,epic}
   \usepackage[latin1]{inputenc}
   \usepackage{graphicx}
   \usepackage{amsmath,eepic,epic,algorithm}

   \newcommand{\figslide}[3]{
   \begin{frame}
   \frametitle{#1}
     \begin{center}
     \resizebox{!}{2.7in}{\includegraphics{#2}}    
     \end{center}
   {#3}
   \end{frame}
   }

   \newcommand{\fighslide}[4]{
   \begin{frame}
   \frametitle{#1}
     \begin{center}
     \resizebox{!}{#4}{\includegraphics{#2}}    
     \end{center}
   {#3}
   \end{frame}
   }

   \newcommand{\figwref}[1]{
   \href{#1}{\tiny \tt #1}}

   \newcommand{\unsupervised}[1]{{\color{red} #1}}
   \newcommand{\supervised}[1]{{\color{green} #1}}
   \newcommand{\argmax}{\mathop{\mathrm{argmax}}}
   \newcommand{\argmin}{\mathop{\mathrm{argmin}}}
   \newcommand{\minimize}{\mathop{\mathrm{minimize}}}
   \newcommand{\maximize}{\mathop{\mathrm{maximize}}}

   \newcommand{\B}[1]{\beta_{#1}}
   \newcommand{\Bh}[1]{\widehat{\beta}_{#1}}
   \newcommand{\V}{\text{Var}}
   \newcommand{\Cov}{\text{Cov}}
   \newcommand{\Vh}{\widehat{\V}}
   \newcommand{\s}{\sigma}
   \newcommand{\sh}{\widehat{\sigma}}

   \newcommand{\argmax}[1]{\mathop{\text{argmax}}_{#1}}
   \newcommand{\argmin}[1]{\mathop{\text{argmin}}_{#1}}
   \newcommand{\Ee}{\mathbb{E}}
   \newcommand{\Pp}{\mathbb{P}}
   \newcommand{\real}{\mathbb{R}}
   \newcommand{\Ybar}{\overline{Y}}
   \newcommand{\Yh}{\widehat{Y}}
   \newcommand{\Xbar}{\overline{X}}
   \newcommand{\Tr}{\text{Tr}}


   \newcommand{\model}{{\cal M}}

   \newcommand{\figvskip}{-0.7in}
   \newcommand{\fighskip}{-0.3in}
   \newcommand{\figheight}{3.5in}

   \newcommand{\Rcode}[1]{{\bf \tt #1 }}
   \newcommand{\Rtcode}[1]{{\tiny \bf \tt #1 }}
   \newcommand{\Rscode}[1]{{\small \bf \tt #1 }}

   \newcommand{\RR}{{\tt R} \;}
   \newcommand{\basename}[1]{http://stats191.stanford.edu/#1}
   \newcommand{\dataname}[1]{\basename{data/#1}}
   \newcommand{\Rname}[1]{\basename{R/#1}}

   \newcommand{\mycolor}[1]{{\color{blue} #1}}
   \newcommand{\basehref}[2]{\href{\basename{#1}}{\mycolor{#2}}}
   \newcommand{\Rhref}[2]{\href{\basename{R/#1}}{\mycolor{#2}}}
   \newcommand{\datahref}[2]{\href{\dataname{#1}}{\mycolor{#2}}}
   \newcommand{\X}{\pmb{X}}
   \newcommand{\Y}{\pmb{Y}}
   \newcommand{\be}{\pmb{varepsilon}}
   \newcommand{\logit}{\text{logit}}


   \title{Statistics 191: Introduction to Applied Statistics}
   \subtitle{Full notes}
   \author{\copyright Jonathan Taylor \\
   }
   %}


   \begin{document}

   \begin{frame}
   \titlepage
   \end{frame}

   \part{Review}
   \frame{\partpage}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} 

   \begin{block}
   {Outline}
   \begin{itemize}

   \item What is a regression model?
   \item Descriptive statistics -- numerical
   \item Descriptive statistics -- graphical
   \item Inference about a population mean
   \item Difference between two population means
   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} 

   \begin{block}
   {What is course about?}
   \begin{itemize}
   \item It is a course on applied statistics.

   \item Hands-on: we use \href{http://cran.r-project.org}{R}, an open-source statistics software environment.

   \item We will start out with a review of introductory statistics to see {\tt R} in action.
   \item Main topic is ``(linear) regression models'': these are the {\em bread and butter} of applied statistics.

   \end{itemize}
   \end{block}

   \begin{block}
   {What is a ``regression'' model? }
   A regression model is a model of the relationships between some
   {\em covariates (predictors)} and an {\em outcome}.
   Specifically, regression is a model of the {\em average} outcome {\em given}
   the covariates.
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} 

   \begin{block}{Heights of couples}
   \begin{itemize}

   \item To study height of the wife in a couple, based on the
   husband's height and her parents height: {\tt Wife} is the
   outcome, and the covariates are {\tt Husband, Mother, Father}.


   \item A mathematical  model, using only {\tt Husband}'s height:
   $$
   {\tt Wife} = f({\tt Husband}) + \varepsilon$$
   where $f$ gives the average height of the wife
   of a man of height {\tt Husband} and
   $\varepsilon$ is ``error'': not {\em every} man of height of
   {\tt Husband} marries
   a woman of height $f({\tt Husband})$.

   \item A statistical question: is there {\em any}
   relationship between covariates and outcomes -- is $f$ just a constant?

   \item Here is some  \href{http://stats191.stanford.edu/review.html}{data}
   using only {\tt Husband}'s height.
   \end{itemize}

   \end{block}
   \end{frame}

   %CODE
       % # Load and attach the data in R
   % # The sep indiciates that they are comma separated
   % # the header option indicates that the first line
   % # contains the variable names
   % 
   % url = 'http://stats191.stanford.edu/data/heights.table'
   % heights <- read.table(url, sep=',', header=T)
   % 
   % # Tell R to put WIFE and HUSBAND in R's toplevel namespace
   % attach(heights)
   % 
   % # Fit simple linear regression model
   % height.lm = lm(WIFE ~ HUSBAND)
   % 
   % # Plot the data
   % plot(HUSBAND, WIFE, pch=23, bg='red', cex=2)
   % 
   % # Add a line to the plot indicating the best
   % # fitting line
   % 
   % abline(height.lm, lwd=3, col='yellow')
   % 
   % 


   \begin{frame}
   \frametitle{Heights data}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/453b25bc12.png}}    
   \end{center}
   \href{http://stats191.stanford.edu/review.html}{R code}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Heights data}

   \begin{block}
   {Linear regression models}
   \begin{itemize}

   \item We might model the data as
   $$
   {\tt Wife} = \B{0} + \B{1} {\tt Husband} + \varepsilon.
   $$

   \item This model is {\em linear} in {\tt Husband}, it is a
   {\em simple linear regression model}.

   \item Another model:
   $$
   {\tt Wife} = \B{0} + \B{1} {\tt Husband} + \B{2} {\tt Mother} + \B{3}
   {\tt Father} + \varepsilon.
   $$

   \item Also linear (in {\tt Husband}, {\tt Mother}, {\tt Father}).

   \item Which model is better? We need a tool to compare models \dots
   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Right-to-work example}

   \begin{block}
   {\href{http://www.ilr.cornell.edu/~hadi/RABE4/Data4/P005.txt}{Data} description}
   \begin{itemize}
   \item Income: income for a four-person family
   \item COL: cost of living for a four-person family
   \item PD: Population density
   \item URate: rate of unionization in 1978
   \item Pop: Population
   \item Taxes: Property taxes in 1972
   \item RTWL: right-to-work indicator
   \end{itemize}
   \end{block}
   \end{frame}

   %CODE
       % url = "http://www.ilr.cornell.edu/~hadi/RABE4/Data4/P005.txt"
   % rtw.table <- read.table(url, header=T, sep='\t')
   % attach(rtw.table)
   % boxplot(COL ~ RTWL, col='orange', pch=23, bg='red')


   \begin{frame}
   \frametitle{Right-to-work vs. cost of living}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/25d3613aca.png}}    
   \end{center}
   \href{http://stats191.stanford.edu/review.html}{R code}
   \end{frame}

   %CODE
       % boxplot(Income ~ RTWL, col='orange', pch=23, bg='red')


   \begin{frame}
   \frametitle{Right-to-work vs. income}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/c3f8116055.png}}    
   \end{center}
   \href{http://stats191.stanford.edu/review.html}{R code}
   \end{frame}

   %CODE
       % plot(URate, COL, pch=23, bg='red')


   \begin{frame}
   \frametitle{Unionization vs. cost of living}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/b2e8bbe568.png}}    
   \end{center}
   \href{http://stats191.stanford.edu/review.html}{R code}
   \end{frame}

   %CODE
       % plot(URate, Income, pch=23, bg='red')


   \begin{frame}
   \frametitle{Unionization vs. income}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/ed40338846.png}}    
   \end{center}
   \href{http://stats191.stanford.edu/review.html}{R code}
   \end{frame}

   %CODE
       % plot(URate, Income, pch=23, bg='red')


   \begin{frame}
   \frametitle{Unionization vs. income}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/ed40338846.png}}    
   \end{center}
   \href{http://stats191.stanford.edu/review.html}{R code}
   \end{frame}

   %CODE
       % plot(URate, Pop, pch=23, bg='red')


   \begin{frame}
   \frametitle{Unionization vs. population}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/5cf7ce0169.png}}    
   \end{center}
   \href{http://stats191.stanford.edu/review.html}{R code}
   \end{frame}

   %CODE
       % plot(COL, Income, pch=23, bg='red')


   \begin{frame}
   \frametitle{Cost-of-living vs. income}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/cc7f27df0a.png}}    
   \end{center}
   \href{http://stats191.stanford.edu/review.html}{R code}
   \end{frame}

   %CODE
       % pairs(rtw.table, pch=23, bg='red')


   \begin{frame}
   \frametitle{Full dataset}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/9bcf84dca6.png}}    
   \end{center}
   \href{http://stats191.stanford.edu/review.html}{R code}
   \end{frame}

   %CODE
       % pairs(rtw.table[-27,], pch=23, bg='red')


   \begin{frame}
   \frametitle{Without NYC}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/cc0169edee.png}}    
   \end{center}
   \href{http://stats191.stanford.edu/review.html}{R code}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Right-to-work example}

   \begin{block}
   {Building a model}
   Some of the main goals of this course:

   \begin{itemize}
   \item Build a statistical model describing the ``effect of RTWL'' on ``COL''

   \item This model should recognize that other variables also affect ``COL''

   \item What sort of ``statistical confidence'' do we have in our
   conclusion about ``RTWL'' and ``COL''?

   \item Is the model adequate do describe this dataset?

   \item Are there other (simpler, more complicated) models?
   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Descriptive statistics -- numerical}

   \begin{block}
   {Mean of a sample}
   Given a sample of numbers $X=(X_1, \dots, X_n)$ the sample mean,
   $\overline{X}$ is
   $$
   \overline{X} = \frac1n \sum_{i=1}^n X_i.$$
   \end{block}

   \begin{block}
   {Standard deviation of a sample}
   Given a sample of numbers $X=(X_1, \dots, X_n)$ the sample
   standard deviation $S_X$ is
   $$
   S^2_X = \frac{1}{n-1}  \sum_{i=1}^n (X_i-\overline{X})^2.$$
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Descriptive statistics -- numerical}

   \begin{block}
   {Median of a sample}
   Given a sample of numbers $X=(X_1, \dots, X_n)$ the sample median is
   the ``middle'' of the sample:
   if $n$ is even, it is the average of the middle two points.
   If $n$ is odd, it is the midpoint.
   \end{block}

   \begin{block}
   {Quantiles of a sample}
   Given a sample of numbers $X=(X_1, \dots, X_n)$ the  $q$-th quantile is
   a point $x_q$ in the data such that $q \cdot 100\%$ of the data lie to the
   left of $x_q$.

   {\bf Example:} the $0.5$-quantile is the median: half
   of the data lie to the right of the median.
   \end{block}
   \end{frame}

   %CODE
       % hist(treated, main='', xlab='Decrease', col='orange')


   \begin{frame}
   \frametitle{Histogram}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/f79d8915ad.png}}    
   \end{center}
   \href{http://stats191.stanford.edu/review.html}{R code}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Inference about a population mean}

   \begin{block}
   {A testing scenario}
   \begin{itemize}

   \item Suppose we want to determine the efficacy of a
   new drug on blood pressure.

   \item Our study design is: we will treat
   a large patient population with the drug and measure their
   blood pressure before and after taking the drug.

   \item One way to conclude that the drug is effective if the blood pressure has decreased. That is,
   if the average difference is negative.

   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Testing hypotheses}

   \begin{block}
   {Setting up the test}
   \begin{itemize}

   \item We could set this up as drawing from a box of {\em differences
   in blood pressure}.

   \item The {\em null hypothesis}, $H_0$ is: ``the average difference is greater than zero.''

   \item The {\em alternative hypothesis}, $H_a$, is: ``the average difference is less than zero.''

   \item Sometimes, people will test the alternative, $H_a$: ``the
   average difference is not zero'' and $H_0$: ``the average difference is zero.''

   \item We test the null with observed data by estimating
    the average difference and converting to standardized units.
   \end{itemize}
   \end{block}
   \end{frame}

   %CODE
       % from matplotlib import rc
   % import pylab, numpy as np, sys
   % np.random.seed(0);import random; random.seed(0)
   % sys.path.append('/private/var/folders/dq/4_9bwd013ln6vvf_q110mwrh0000gn/T/tmp1gpSKi')
   % f=pylab.gcf(); f.set_size_inches(8.0,6.0)
   % datadir ='/private/var/folders/dq/4_9bwd013ln6vvf_q110mwrh0000gn/T/tmp1gpSKi/data'
   % import random
   % X = np.mgrid[0:1:10j,0:1:5j].reshape((2,50)) + np.random.sample((2,50)) * 0.05
   % X = X.T
   % sample = [random.randint(-6,3) for _  in range(50)]
   % for i in range(50):
   %     pylab.text(X[i,0], X[i,1], '%d' % sample[i])
   % 
   %     pylab.gca().set_xticks([]);    pylab.gca().set_xlim([-0.1,1.1])
   %     pylab.gca().set_yticks([]);    pylab.gca().set_ylim([-0.1,1.1])
   % pylab.title(r"$\bar{X}$=average(sample)=%0.1f, $S_X$=SD$^+$(sample)=%0.1f" % (np.mean(sample), np.std(sample) * np.sqrt(50/49.)))
   % 


   \begin{frame}
   \frametitle{Sample of blood pressures}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/eec113e550.pdf}}    
   \end{center}
   Sample of 50
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Inference about a population mean}

   \begin{block}
   {Testing whether mean is 0: two-sided}
   \begin{itemize}
   \item  Suppose we want a two-sided test of whether $\mu=0$ based
   on a sample $X$, at level $\alpha$.
   \item Compute
   $$
   T = \frac{\overline{X}}{S_X/\sqrt{n}} = \frac{-0.7}{2.7/\sqrt{50}}=-1.8$$
   \item If $|T| > t_{n-1, 1-\alpha/2}$, then reject $H_0:\mu=0$.
   \item Above, $t_{n-1, 1-\alpha/2}$ is the $1-\frac \alpha 2$ quantile of $t_{n-1}$ random variable, defined by
   $$
   \Pp(T_{n-1} \leq t_{n-1,1-\alpha/2}) = 1 - \frac\alpha 2.$$

   \item With $df=49, \alpha=0.05$, we see that $t_{49,0.975}=2.00$. So,
   we do not reject $H_0$.
   \end{itemize}
   \end{block}
   \end{frame}

   %CODE
       % from matplotlib import rc
   % import pylab, numpy as np, sys
   % np.random.seed(0);import random; random.seed(0)
   % sys.path.append('/private/var/folders/dq/4_9bwd013ln6vvf_q110mwrh0000gn/T/tmp1gpSKi')
   % f=pylab.gcf(); f.set_size_inches(8.0,6.0)
   % datadir ='/private/var/folders/dq/4_9bwd013ln6vvf_q110mwrh0000gn/T/tmp1gpSKi/data'
   % import pylab, numpy as np
   % import scipy.stats
   % 
   % df = 49
   % x = np.linspace(-4,4,101)
   % pylab.plot(x,scipy.stats.t.pdf(x, df)*100, linewidth=2, label=r'$T$, df=49')
   % 
   % # The t region
   % 
   % x2 = np.linspace(scipy.stats.t.isf(0.025,df),4, 101)
   % y2 = scipy.stats.t.pdf(x2, df)
   % xf, yf = pylab.poly_between(x2, 0*x2, y2*100)
   % pylab.fill(xf, yf, facecolor='gray', hatch='\\', alpha=0.5)
   % 
   % x2 = np.linspace(-4,-scipy.stats.t.isf(0.025,df), 101)
   % y2 = scipy.stats.t.pdf(x2, df)
   % xf, yf = pylab.poly_between(x2, 0*x2, y2*100)
   % pylab.fill(xf, yf, facecolor='gray', hatch='\\', alpha=0.5)
   % 
   % pylab.gca().set_xlabel('standardized units')
   % pylab.gca().set_ylabel('% per standardized unit')
   % pylab.legend()
   % #pylab.gca().set_xlim([-2,4])
   % #pylab.gca().set_yticks([])
   % 


   \begin{frame}
   \frametitle{Student's $T$}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/4b49f343db.pdf}}    
   \end{center}
   Two-sided {\color{blue} 5\% rejection rule}, df=49
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Inference about a population mean}

   \begin{block}
   {Testing whether mean is $<$ 0: one-sided}
   \begin{itemize}
   \item  Suppose we want a one-sided test of whether $\mu < 0$ based
   on a sample $X$, at level $\alpha$.
   \item For this test, the {\em null} is $H_0:\mu \geq 0$ and
   the alternative is $H_a: \mu < 0$.
   \item Compute
   $$
   T = \frac{\overline{X}}{S_X/\sqrt{n}} = \frac{-0.7}{2.7/\sqrt{50}}=-1.8$$
   \item If $T < t_{n-1, \alpha}$, then reject $H_0:\mu=0$.
   \item With $df=49, \alpha=0.05$, we see that $t_{49,0.05}=-1.68$. So,
   we reject $H_0$.
   \end{itemize}
   \end{block}
   \end{frame}

   %CODE
       % from matplotlib import rc
   % import pylab, numpy as np, sys
   % np.random.seed(0);import random; random.seed(0)
   % sys.path.append('/private/var/folders/dq/4_9bwd013ln6vvf_q110mwrh0000gn/T/tmp1gpSKi')
   % f=pylab.gcf(); f.set_size_inches(8.0,6.0)
   % datadir ='/private/var/folders/dq/4_9bwd013ln6vvf_q110mwrh0000gn/T/tmp1gpSKi/data'
   % import pylab, numpy as np
   % import scipy.stats
   % 
   % df = 49
   % x = np.linspace(-4,4,101)
   % pylab.plot(x,scipy.stats.t.pdf(x, df)*100, linewidth=2, label=r'$T$, df=49')
   % 
   % # The t region
   % 
   % #x2 = np.linspace(scipy.stats.t.isf(0.025,df),4, 101)
   % #y2 = scipy.stats.t.pdf(x2, df)
   % #xf, yf = pylab.poly_between(x2, 0*x2, y2*100)
   % #pylab.fill(xf, yf, facecolor='gray', hatch='\\', alpha=0.5)
   % 
   % x2 = np.linspace(-4,-scipy.stats.t.isf(0.05,df), 101)
   % y2 = scipy.stats.t.pdf(x2, df)
   % xf, yf = pylab.poly_between(x2, 0*x2, y2*100)
   % pylab.fill(xf, yf, facecolor='gray', hatch='\\', alpha=0.5)
   % 
   % pylab.gca().set_xlabel('standardized units')
   % pylab.gca().set_ylabel('% per standardized unit')
   % pylab.legend()
   % #pylab.gca().set_xlim([-2,4])
   % #pylab.gca().set_yticks([])
   % 


   \begin{frame}
   \frametitle{Student's $T$}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/e101a35f76.pdf}}    
   \end{center}
   One-sided {\color{blue} 5\% rejection rule} for $H_0:\mu \geq 0$, df=49
   \end{frame}

   %CODE
       % from matplotlib import rc
   % import pylab, numpy as np, sys
   % np.random.seed(0);import random; random.seed(0)
   % sys.path.append('/private/var/folders/dq/4_9bwd013ln6vvf_q110mwrh0000gn/T/tmp1gpSKi')
   % f=pylab.gcf(); f.set_size_inches(8.0,6.0)
   % datadir ='/private/var/folders/dq/4_9bwd013ln6vvf_q110mwrh0000gn/T/tmp1gpSKi/data'
   % import pylab, numpy as np
   % import scipy.stats
   % 
   % df = 5
   % x = np.linspace(-4,4,101)
   % pylab.plot(x,scipy.stats.t.pdf(x, df)*100, linewidth=2, label=r'$T$, df=5')
   % 
   % # The t region
   % 
   % #x2 = np.linspace(scipy.stats.t.isf(0.025,df),4, 101)
   % #y2 = scipy.stats.t.pdf(x2, df)
   % #xf, yf = pylab.poly_between(x2, 0*x2, y2*100)
   % #pylab.fill(xf, yf, facecolor='gray', hatch='\\', alpha=0.5)
   % 
   % x2 = np.linspace(-4,-scipy.stats.t.isf(0.05,df), 101)
   % y2 = scipy.stats.t.pdf(x2, df)
   % xf, yf = pylab.poly_between(x2, 0*x2, y2*100)
   % pylab.fill(xf, yf, facecolor='gray', hatch='\\', alpha=0.5)
   % 
   % pylab.gca().set_xlabel('standardized units')
   % pylab.gca().set_ylabel('% per standardized unit')
   % pylab.legend()
   % #pylab.gca().set_xlim([-2,4])
   % #pylab.gca().set_yticks([])
   % 


   \begin{frame}
   \frametitle{Student's $T$}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/a31bc72a1c.pdf}}    
   \end{center}
   One-sided {\color{blue} 5\% rejection rule} for $H_0:\mu \geq 0$, df=5
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Inference about a population mean}

   \begin{block}
   {Confidence interval}
   \begin{itemize}
   \item     If $(X_1, \dots, X_n)$ are independent, all having a normal distribution  $N(\mu, \sigma^2)$, then a $(1 - \alpha)$-confidence interval for $\mu$ is
   $$
   \overline{ X} \pm t_{n-1, 1 - \alpha/2}\cdot S_X / \sqrt{n}
   $$
   \item That is, if $\alpha=0.05$, and we repeat the experiment
   many times then 95\% of the time,
   the true $\mu$ will be in the interval
   $$
   [\overline{ X} - t_{n-1, 1 - \alpha/2}\cdot S_X / \sqrt{n},\overline{ X} + t_{n-1, 1 - \alpha/2}\cdot S_X / \sqrt{n}]
   $$
   \item Again, $t_{n-1, 1-\alpha/2}$ is the $1-\frac \alpha 2$ quantile of $t_{n-1}$ random variable, defined by
   $$
   \Pp(T_{n-1} \leq t_{n-1,1-\alpha/2}) = 1 - \frac\alpha 2.$$
   \end{itemize}
   \end{block}
   \end{frame}

   %CODE
       % mu = 2
   % nsample <- 500 # how many samples to generate
   % nobs <- 10  # how many observations in each sample
   % alpha <- 0.15
   % 
   % CI.data <- matrix(0, nrow=nsample, ncol=2) # a matrix to store the data
   % 
   % cover <- numeric(nsample) # a counter to see how many CIs contain 0
   % for (i in 1:nsample) {
   %   CI.data[i,] <- t.test(rnorm(nobs) + mu, conf.level=1-alpha)$conf.int
   %   cover[i] <- (CI.data[i,1] < mu) * (CI.data[i,2] > mu) # add 1 if
   %                                         # CI contains 0
   % }
   % print(sum(cover)/nsample) # coverage percentage, should be approx 1-alpha
   % 
   % simulate = function() {
   % nplot <- 20 # how many intervals to plot
   % 
   % plot(c(-2+mu,2+mu), c(1, nplot), type='n', xlab='Confidence Intervals', ylab='Sample')
   % for (i in 1:min(nsample, 20)) {
   %   if (cover[i]) {
   %     lines(CI.data[i,], rep(i,2), col='red', lwd=2) # add a red bar for
   %                                         # each CI that covers
   %   }
   %   else {
   %     lines(CI.data[i,], rep(i,2), col='blue', lwd=2) # add a red bar for
   %                                         # each CI that covers
   %   }
   % }
   % }
   % simulate()


   \begin{frame}
   \frametitle{20 different confidence 85\% intervals}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/ef87c09474.png}}    
   \end{center}

   \end{frame}

   %CODE
       % mu = 2
   % x=3
   % nsample <- 500 # how many samples to generate
   % nobs <- 10  # how many observations in each sample
   % alpha <- 0.15
   % 
   % CI.data <- matrix(0, nrow=nsample, ncol=2) # a matrix to store the data
   % 
   % cover <- numeric(nsample) # a counter to see how many CIs contain 0
   % for (i in 1:nsample) {
   %   CI.data[i,] <- t.test(rnorm(nobs) + mu, conf.level=1-alpha)$conf.int
   %   cover[i] <- (CI.data[i,1] < mu) * (CI.data[i,2] > mu) # add 1 if
   %                                         # CI contains 0
   % }
   % print(sum(cover)/nsample) # coverage percentage, should be approx 1-alpha
   % 
   % simulate = function() {
   % nplot <- 20 # how many intervals to plot
   % 
   % plot(c(-2+mu,2+mu), c(1, nplot), type='n', xlab='Confidence Intervals', ylab='Sample')
   % for (i in 1:min(nsample, 20)) {
   %   if (cover[i]) {
   %     lines(CI.data[i,], rep(i,2), col='red', lwd=2) # add a red bar for
   %                                         # each CI that covers
   %   }
   %   else {
   %     lines(CI.data[i,], rep(i,2), col='blue', lwd=2) # add a red bar for
   %                                         # each CI that covers
   %   }
   % }
   % }
   % simulate()


   \begin{frame}
   \frametitle{Another 20}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/ae224efefc.png}}    
   \end{center}

   \end{frame}

   %CODE
       % mu = 2
   % x=4
   % nsample <- 500 # how many samples to generate
   % nobs <- 10  # how many observations in each sample
   % alpha <- 0.15
   % 
   % CI.data <- matrix(0, nrow=nsample, ncol=2) # a matrix to store the data
   % 
   % cover <- numeric(nsample) # a counter to see how many CIs contain 0
   % for (i in 1:nsample) {
   %   CI.data[i,] <- t.test(rnorm(nobs) + mu, conf.level=1-alpha)$conf.int
   %   cover[i] <- (CI.data[i,1] < mu) * (CI.data[i,2] > mu) # add 1 if
   %                                         # CI contains 0
   % }
   % print(sum(cover)/nsample) # coverage percentage, should be approx 1-alpha
   % 
   % simulate = function() {
   % nplot <- 20 # how many intervals to plot
   % 
   % plot(c(-2+mu,2+mu), c(1, nplot), type='n', xlab='Confidence Intervals', ylab='Sample')
   % for (i in 1:min(nsample, 20)) {
   %   if (cover[i]) {
   %     lines(CI.data[i,], rep(i,2), col='red', lwd=2) # add a red bar for
   %                                         # each CI that covers
   %   }
   %   else {
   %     lines(CI.data[i,], rep(i,2), col='blue', lwd=2) # add a red bar for
   %                                         # each CI that covers
   %   }
   % }
   % }
   % simulate()


   \begin{frame}
   \frametitle{Yet another 20}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/fb88fdd606.png}}    
   \end{center}

   \end{frame}

   %CODE
       % mu = 2
   % x=5
   % nsample <- 500 # how many samples to generate
   % nobs <- 10  # how many observations in each sample
   % alpha <- 0.15
   % 
   % CI.data <- matrix(0, nrow=nsample, ncol=2) # a matrix to store the data
   % 
   % cover <- numeric(nsample) # a counter to see how many CIs contain 0
   % for (i in 1:nsample) {
   %   CI.data[i,] <- t.test(rnorm(nobs) + mu, conf.level=1-alpha)$conf.int
   %   cover[i] <- (CI.data[i,1] < mu) * (CI.data[i,2] > mu) # add 1 if
   %                                         # CI contains 0
   % }
   % print(sum(cover)/nsample) # coverage percentage, should be approx 1-alpha
   % 
   % simulate = function() {
   % nplot <- 20 # how many intervals to plot
   % 
   % plot(c(-2+mu,2+mu), c(1, nplot), type='n', xlab='Confidence Intervals', ylab='Sample')
   % for (i in 1:min(nsample, 20)) {
   %   if (cover[i]) {
   %     lines(CI.data[i,], rep(i,2), col='red', lwd=2) # add a red bar for
   %                                         # each CI that covers
   %   }
   %   else {
   %     lines(CI.data[i,], rep(i,2), col='blue', lwd=2) # add a red bar for
   %                                         # each CI that covers
   %   }
   % }
   % }
   % simulate()


   \begin{frame}
   \frametitle{Yes, 20 more}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/6f9a4bcdc4.png}}    
   \end{center}

   \end{frame}

   %CODE
       % mu = 2
   % x=6
   % nsample <- 500 # how many samples to generate
   % nobs <- 10  # how many observations in each sample
   % alpha <- 0.15
   % 
   % CI.data <- matrix(0, nrow=nsample, ncol=2) # a matrix to store the data
   % 
   % cover <- numeric(nsample) # a counter to see how many CIs contain 0
   % for (i in 1:nsample) {
   %   CI.data[i,] <- t.test(rnorm(nobs) + mu, conf.level=1-alpha)$conf.int
   %   cover[i] <- (CI.data[i,1] < mu) * (CI.data[i,2] > mu) # add 1 if
   %                                         # CI contains 0
   % }
   % print(sum(cover)/nsample) # coverage percentage, should be approx 1-alpha
   % 
   % simulate = function() {
   % nplot <- 20 # how many intervals to plot
   % 
   % plot(c(-2+mu,2+mu), c(1, nplot), type='n', xlab='Confidence Intervals', ylab='Sample')
   % for (i in 1:min(nsample, 20)) {
   %   if (cover[i]) {
   %     lines(CI.data[i,], rep(i,2), col='red', lwd=2) # add a red bar for
   %                                         # each CI that covers
   %   }
   %   else {
   %     lines(CI.data[i,], rep(i,2), col='blue', lwd=2) # add a red bar for
   %                                         # each CI that covers
   %   }
   % }
   % }
   % simulate()


   \begin{frame}
   \frametitle{A final 20}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/b543efd3af.png}}    
   \end{center}
   Out of 100, 90 covered the true mean...
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Review}

   \begin{block}
   {Effect of calcium on BP}
   \begin{itemize}
   \item A study was conducted to study the effect of calcium supplements
   on blood pressure.
   \item More detailed data description can be found
   \href{http://lib.stat.cmu.edu/DASL/Datafiles/Calcium.html}{\mycolor{here}}.
   \end{itemize}
   \end{block}

   \begin{block}
   {Questions}
   \begin{itemize}
   \item What is the mean decrease in BP in the treated group? placebo group?
   \item What is the median decrease in BP in the treated group? placebo group?
   \item What is the standard deviation of decrease in BP in the treated group? placebo group?
   \item Is there a difference between the two groups? Did BP decrease more in the treated group?
   \end{itemize}
   \end{block}
   \end{frame}

   %CODE
       % url = 'http://lib.stat.cmu.edu/DASL/Datafiles/Calcium.html'
   % calcium.table <- read.table(url,header=T,skip=28,nrow=21)
   % 
   % # Attach the table so R can find the variables Decrease and Treatment
   % 
   % attach(calcium.table)
   % 
   % # Numerical summaries of the two groups
   % 
   % treated <- Decrease[(Treatment == 'Calcium')]
   % placebo <- Decrease[(Treatment == 'Placebo')]
   % 
   % boxplot(Decrease ~ Treatment, col='orange', pch=23, bg='red')


   \begin{frame}
   \frametitle{Boxplot}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/427bf56cd4.png}}    
   \end{center}
   \href{http://stats191.stanford.edu/review.html}{R code}
   \end{frame}

   %CODE
       % hist(treated, main='', xlab='Decrease', col='orange')


   \begin{frame}
   \frametitle{Histogram of Treated response}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/f79d8915ad.png}}    
   \end{center}
   \href{http://stats191.stanford.edu/review.html}{R code}
   \end{frame}

   %CODE
       % hist(placebo, main='', xlab='Decrease', col='orange')


   \begin{frame}
   \frametitle{Histogram of Placebo response}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/b1fb4584c5.png}}    
   \end{center}
   \href{http://stats191.stanford.edu/review.html}{R code}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Difference between means}

   \begin{block}
   {BP example}
   \begin{itemize}
   \item In our setting, we have two groups that we have reason to believe are different.
   \item We have two samples:
   \begin{enumerate}
   \item $(X_1, \dots, X_{10})$ (Calcium)
   \item  $(Z_1, \dots, Z_{11})$ (Placebo)
   \end{enumerate}
   \item Does treatment have an effect?
   \item We can answer this statistically by testing the null hypothesis  $H_0:\mu_X = \mu_Z$?
   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Difference between means}

   \begin{block}
   {Testing $H_0:\mu_X=\mu_Z$}
   \begin{itemize}
   \item If variances are assumed equal, pooled $t$-test is appropriate
   $$
   T = \frac{\overline{X} - \overline{Z}}{S_P \sqrt{\frac{1}{10}
   + \frac{1}{11}}}, \qquad S^2_P = \frac{9 \cdot S^2_X + 10 \cdot S^2_Z}{19}.$$
   \item For two-sided test at level $\alpha=0.05$, reject if $|T| > t_{19, 0.975}$.
   \item Confidence interval: for example, a $90\%$ confidence interval
   for $\mu_X-\mu_Z$ is
   $$
   \overline{X}-\overline{Z} \pm S_P \sqrt{\frac{1}{10}
   + \frac{1}{11}} \cdot  t_{19,0.95}
   $$
   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Difference between means}

   \begin{block}
   {Pooled estimate $S_P$}
   \begin{itemize}
   \item The rule for the $SD$ of differences is
   $$
   SD(\overline{X}-\overline{Z}) = \sqrt{SD(\overline{X})^2+SD(\overline{Z})^2}.
   $$
   \item By this rule, we might take our estimate to be
   $$
   \widehat{SD(\overline{X}-\overline{Z})} = \sqrt{\frac{S^2_X}{10} + \frac{S^2_Z}{11}}
   $$
   \item But, the pooled estimate assumes $\Ee(S^2_X)=\Ee(S^2_Z)=\sigma^2$ and replaces
   the $S^2$'s above with $S^2_P$, a ``better'' estimate of
   $\sigma^2$ than either $S^2_X$ or $S^2_Z$.
   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Difference between means}

   \begin{block}
   {Pooled estimate $S_P$}
   \begin{itemize}
   \item Where do we get 19 degrees of freedom?
   \item Well, the $X$ ({\tt Treatment}) sample has $10-1=9$ degrees of freedom
   to estimate $\sigma^2$ while the $Z$ ({\tt Placebo}) sample
   has $11-1=10$ degrees of freedom.
   \item The total degrees of freedom is $9+10=19$.
   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Our first regression model}

   \begin{block}
   {Unified dataset}
   \begin{itemize}

   \item Put two samples together:
   $$Y=(X_1,\dots, X_{10}, Z_1, \dots, Z_{11}).$$

   \item Under the same assumptions as the pooled $t$-test:
   $$
   \begin{aligned}
   Y_i &\sim N(\mu_i, \sigma^2)\\
   \mu_i &=
   \begin{cases}
   \mu_X & 1 \leq i \leq 10 \\ \mu_Z & 11 \leq i \leq 21
   \end{cases}
   \end{aligned}
   $$
   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Our first regression model}

   \begin{block}
    {$t$-test as regression model}
   \begin{itemize}
   \item This is a (regression) model for the sample $Y$. The
   (qualitative) variable \Rscode{Treatment} is
   called a ``covariate'' or ``predictor''.
   \item The decrease in BP is an outcome variable.
   \item We assume that the relationship between treatment and average
   decrease in BP is simple: it depends only on which group a subject is in.
   \item This relationship is ``modelled'' through the mean
   vector $\mu=(\mu_1, \dots, \mu_{21})$.
   \end{itemize}
   \end{block}
   \end{frame}

   \part{Simple linear regression (Ch. 2, RABE)}
   \frame{\partpage}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Outline}

   \begin{block}
   {Simple Linear Regression}
   \begin{itemize}
   \item Some definitions for regression models.
   \item Specifying the model.
   \item Fitting the model: least squares.
   \item Inference.

   \item What is a $T$-statistic?

   \item ``Inference'' for $\beta_1$.

   \item Linear combinations of $\beta_0, \beta_1$.

   \end{itemize}
   \end{block}
   \vfill
   \end{frame}

   %CODE
       % from matplotlib import rc
   % import pylab, numpy as np, sys
   % np.random.seed(0);import random; random.seed(0)
   % sys.path.append('/private/var/folders/dq/4_9bwd013ln6vvf_q110mwrh0000gn/T/tmp1gpSKi')
   % f=pylab.gcf(); f.set_size_inches(8.0,6.0)
   % datadir ='/private/var/folders/dq/4_9bwd013ln6vvf_q110mwrh0000gn/T/tmp1gpSKi/data'
   % import matplotlib.mlab as ML
   % H = ML.csv2rec('%s/pearson_lee.csv' % datadir)
   % M = H['mother']
   % D = H['daughter']
   % pylab.scatter(M, D, c='red')
   % pylab.gca().set_xlabel("Mother's height (inches)")
   % pylab.gca().set_ylabel("Daughter's height (inches)")
   % 


   \begin{frame}
   \frametitle{Height data}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/a84b90b10a.pdf}}    
   \end{center}

   \end{frame}

   %CODE
       % from matplotlib import rc
   % import pylab, numpy as np, sys
   % np.random.seed(0);import random; random.seed(0)
   % sys.path.append('/private/var/folders/dq/4_9bwd013ln6vvf_q110mwrh0000gn/T/tmp1gpSKi')
   % f=pylab.gcf(); f.set_size_inches(8.0,6.0)
   % datadir ='/private/var/folders/dq/4_9bwd013ln6vvf_q110mwrh0000gn/T/tmp1gpSKi/data'
   % import matplotlib.mlab as ML
   % H = ML.csv2rec('%s/pearson_lee.csv' % datadir)
   % M = H['mother']
   % D = H['daughter']
   % pylab.scatter(M, D, c='red')
   % x = 66
   % xf, yf = pylab.poly_between([x-.5,x+.5], [50,50], [75, 75])
   % g = (M < x+.5) * (M >= x-.5)
   % pylab.fill(xf, yf, facecolor='blue', alpha=0.2, hatch='/', label='_nolegend_')
   % pylab.gca().set_xlabel("Mother's height (inches)")
   % pylab.gca().set_ylabel("Daughter's height (inches)")
   % s = pylab.scatter([x],D[g].mean(), s=130, c='yellow', marker='^')
   % s.set_label('Average at %d' % int(x))
   % Dbar = D.mean(); Dsd = np.sqrt(((D - Dbar)**2).mean())
   % Mbar = M.mean(); Msd = np.sqrt(((M - Mbar)**2).mean())
   % r = np.corrcoef([M, D])[0,1]
   % pylab.plot([Mbar-3.5*Msd,Mbar,Mbar+3.5*Msd],
   %            [Dbar-r*3.5*Dsd,Dbar,Dbar+r*3.5*Dsd], '-', linewidth=3, label='D on M', color='black')
   % 


   \begin{frame}
   \frametitle{Regression}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/9ba9fbc6ac.pdf}}    
   \end{center}

   \end{frame}

   %CODE
       % from matplotlib import rc
   % import pylab, numpy as np, sys
   % np.random.seed(0);import random; random.seed(0)
   % sys.path.append('/private/var/folders/dq/4_9bwd013ln6vvf_q110mwrh0000gn/T/tmp1gpSKi')
   % f=pylab.gcf(); f.set_size_inches(8.0,6.0)
   % datadir ='/private/var/folders/dq/4_9bwd013ln6vvf_q110mwrh0000gn/T/tmp1gpSKi/data'
   % import matplotlib.mlab as ML
   % H = ML.csv2rec('%s/pearson_lee.csv' % datadir)
   % M = H['mother']
   % D = H['daughter']
   % pylab.scatter(M, D, c='red')
   % x = 64
   % xf, yf = pylab.poly_between([x-.5,x+.5], [50,50], [75, 75])
   % g = (M < x+.5) * (M >= x-.5)
   % pylab.fill(xf, yf, facecolor='blue', alpha=0.2, hatch='/', label='_nolegend_')
   % pylab.gca().set_xlabel("Mother's height (inches)")
   % pylab.gca().set_ylabel("Daughter's height (inches)")
   % s = pylab.scatter([x],D[g].mean(), s=130, c='yellow', marker='^')
   % s.set_label('Average at %d' % int(x))
   % Dbar = D.mean(); Dsd = np.sqrt(((D - Dbar)**2).mean())
   % Mbar = M.mean(); Msd = np.sqrt(((M - Mbar)**2).mean())
   % r = np.corrcoef([M, D])[0,1]
   % pylab.plot([Mbar-3.5*Msd,Mbar,Mbar+3.5*Msd],
   %            [Dbar-r*3.5*Dsd,Dbar,Dbar+r*3.5*Dsd], '-', linewidth=3, label='D on M', color='black')
   % 


   \begin{frame}
   \frametitle{Regression}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/b611c13e4e.pdf}}    
   \end{center}

   \end{frame}

   %CODE
       % from matplotlib import rc
   % import pylab, numpy as np, sys
   % np.random.seed(0);import random; random.seed(0)
   % sys.path.append('/private/var/folders/dq/4_9bwd013ln6vvf_q110mwrh0000gn/T/tmp1gpSKi')
   % f=pylab.gcf(); f.set_size_inches(8.0,6.0)
   % datadir ='/private/var/folders/dq/4_9bwd013ln6vvf_q110mwrh0000gn/T/tmp1gpSKi/data'
   % import matplotlib.mlab as ML
   % H = ML.csv2rec('%s/pearson_lee.csv' % datadir)
   % M = H['mother']
   % D = H['daughter']
   % pylab.scatter(M, D, c='red')
   % x = 62
   % xf, yf = pylab.poly_between([x-.5,x+.5], [50,50], [75, 75])
   % g = (M < x+.5) * (M >= x-.5)
   % pylab.fill(xf, yf, facecolor='blue', alpha=0.2, hatch='/', label='_nolegend_')
   % pylab.gca().set_xlabel("Mother's height (inches)")
   % pylab.gca().set_ylabel("Daughter's height (inches)")
   % s = pylab.scatter([x],D[g].mean(), s=130, c='yellow', marker='^')
   % s.set_label('Average at %d' % int(x))
   % Dbar = D.mean(); Dsd = np.sqrt(((D - Dbar)**2).mean())
   % Mbar = M.mean(); Msd = np.sqrt(((M - Mbar)**2).mean())
   % r = np.corrcoef([M, D])[0,1]
   % pylab.plot([Mbar-3.5*Msd,Mbar,Mbar+3.5*Msd],
   %            [Dbar-r*3.5*Dsd,Dbar,Dbar+r*3.5*Dsd], '-', linewidth=3, label='D on M', color='black')
   % 


   \begin{frame}
   \frametitle{Regression}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/276a37f541.pdf}}    
   \end{center}

   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Regression model}

   \begin{block}
   {What is a ``regression'' model? }
   A regression model is a model of the relationships between some {\em covariates (predictors)} and an {\em outcome}.
   Specifically, regression is a model of the {\em average} outcome {\em given}
   the covariates.
   \end{block}
   \begin{block}
   {Mathematical formulation}
   For height of couples data:
   a mathematical  model:
   $$
   {\tt Daughter} = f({\tt Mother}) + \varepsilon$$
   where $f$ gives the average height of the daughter of a mother of height {\tt Mother} and
   $\varepsilon$ is the random error.
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Regression models}

   \begin{block}
   {Linear regression models}
   \begin{itemize}
   \item A {\em linear} regression model says that
   the function $f$ is a sum (linear combination) of functions of ${\tt Mother}$.

   \item Simple linear regression model:
   $$f({\tt Mother}) = \beta_0 + \beta_1 \cdot {\tt Mother}$$
   for some unknown parameter vector $(\beta_0, \beta_1)$.
   \item Could also be a sum (linear combination) of {\em known} functions of ${\tt Mother}$:
   $$f({\tt Mother}) = \beta_0 + \beta_1 \cdot {\tt Mother} + \beta_2 \cdot {\tt Mother}^2
   $$
   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Simple linear regression model}

   \begin{block}
   {Specifying the (statistical) model}
   \begin{itemize}
   \item
   {\em Simple linear} regression is the case when there is only one predictor:
   $$
   f({\tt Mother}) = \beta_0 + \beta_1  \cdot {\tt Mother}.$$

   \item Let $Y_i$ be the height of the $i$-th daughter in the sample, $X_i$ be the height of the $i$-th mother.


   \item Model:
   $$
   Y_i = \underbrace{\beta_0 + \beta_1 X_i}_{\text{{\small regression equation}}} + \underbrace{\varepsilon_i}_{\text{{\small error}}}$$
   where $\varepsilon_i \sim N(0, \sigma^2)$ are independent.

   \item This specifies a {\em distribution} for the $Y$'s given the $X$'s, i.e.
   it is a statistical model.

   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Fitting the model}

   \begin{block}
   {Least squares}
   \begin{itemize}
   \item We will be using ``least squares'' regression. This measures
   the goodness of fit of a line by the sum of squared errors, $SSE$.
   \item Least squares regression chooses the line that minimizes
   $$
   SSE(\beta_0, \beta_1) = \sum_{i=1}^n (Y_i - \beta_0 - \beta_1 \cdot X_i)^2.$$

   \item In principle, we might measure ``goodness of fit'' differently:
   $$
   SAD(\beta_0, \beta_1) = \sum_{i=1}^n |Y_i - \beta_0 - \beta_1 \cdot X_i|.$$

   \item Why do we use least squares?
   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Least Squares}

   \begin{block}
   {Why Least Squares?}
   \begin{itemize}[<+->]
   \item With least squares, the minimizers have explicit formulae -- not so important with today's computer power -- especially when $L$ is convex.
   \item Resulting formulae are {\em linear} in the outcome $Y$. This is important
   for inferential reasons. For only predictive power, this is also not so important.
   \item If assumptions are correct, then this is ``maximum likelihood'' estimation.

   \item Statistical theory tells us the ``maximum likelihood'' estimators are generally good estimators.
   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Least squares}

   \begin{block}
   {Alternative definition of (sample / population) mean}
   The mean of a sample $(Y_1, \dots, Y_n)$ (or population $Y$ ) is the number that minimizes
   $$
   SSE(\mu) = \sum_{i=1}^n (Y_i - \mu)^2 \qquad \left(\text{population: \ } = \Ee((Y-\mu)^2) \right).$$
   \end{block}
   \begin{block}
   {Alternative definition of (sample / population) median}
   The median of a sample $(Y_1, \dots, Y_n)$ (or population $Y$ ) is any number that minimizes
   $$
   SAD(\mu) = \sum_{i=1}^n |Y_i - \mu| \qquad \left(\text{population: \ } = \Ee(|Y-\mu|) \right).$$
   \end{block}
   \end{frame}

   %CODE
       % set.seed(100)
   % X = rnorm(50)
   % Y = 2 + 1.5 * rnorm(50)
   % plot(X, Y, pch=23, bg='orange')


   \begin{frame}
   \frametitle{Least squares}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/0b585690e1.png}}    
   \end{center}

   \end{frame}

   %CODE
       % set.seed(100)
   % X = rnorm(50)
   % Y = 2 + 1.5 * rnorm(50)
   % #library(alr3)
   % #data(heights)
   % #Y = heights$Dheight
   % #X = heights$Mheight
   % 
   % # Squared error loss: 'least squares'
   % 
   % squared.error = function(b0, b1) {
   % return(sqrt(sum((Y - b0 - b1*X)^2)))
   % }
   % 
   % # Absolute value loss: 'absolute deviation'
   % 
   % absolute.loss = function(b0, b1) {
   % return(sum(abs(Y - b0 - b1*X)))
   % }
   % 
   % # Plot the loss over a grid of values
   % 
   % plot.loss = function(loss, b0, b1) {
   %    l = matrix(0, length(b0), length(b1))
   %    for (i in 1:length(b0)) {
   %       for (j in 1:length(b1)) {
   %           l[i,j] = loss(b0[i], b1[j])
   %       }
   %    }
   %    image(b0, b1, l, col=rainbow(1000))
   %    return(l)
   % }
   % 
   % ll = plot.loss(squared.error, seq(0,3,length=100), seq(-1,3,length=100))


   \begin{frame}
   \frametitle{Least squares}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/7abf182b36.png}}    
   \end{center}

   \end{frame}

   %CODE
       % ll = plot.loss(absolute.loss, seq(0,3,length=100), seq(-1,3,length=100))


   \begin{frame}
   \frametitle{Absolute deviation}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/1d67104fb5.png}}    
   \end{center}

   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Geometry of Least Squares: Simple Linear Model}

   \resizebox{!}{3in}{\includegraphics{./figs/simple/axes_simple_full.png}}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Least Squares Solutions}

   \begin{block}
   {Regression line parameters: $(\beta_0, \beta_1)$}
   \begin{itemize}[<+->]
   \item $$
   \widehat{\beta}_1 = \frac{\sum_{i=1}^n(X_i - \overline{X})(Y_i - \overline{Y})}{\sum_{i=1}^n (X_i-\overline{X})^2} = \frac{\widehat{Cov}(X,Y)}{\widehat{Var}(X)}.$$

   \item $$\widehat{\beta}_0 = \overline{Y} - \widehat{\beta}_1 \cdot \overline{X}.$$

   \end{itemize}
   \end{block}
   \begin{block}
   {Estimating variance: $\sigma^2$}
   \begin{itemize}[<+->]
   \item  Strength of association between $Y$ and $X$ will depend on variability of errors $\varepsilon$,  as in two sample $t$-test.

   \item $$
   \widehat{\sigma}^2 = \frac{1}{n-2} \sum_{i=1}^n (Y_i - \widehat{\beta}_0 - \widehat{\beta}_1 X_i)^2 = \frac{SSE}{n-2} = MSE.$$
   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Least Squares}

   \begin{block}
   {Predicting the mean}
   (Conditional) mean can be estimated for any given mother of height $X$ as
   $$
   \widehat{Y} = \widehat{f}(X) = \widehat{\beta}_0 + \widehat{\beta}_1 \cdot X.$$
   where $(\widehat{\beta}_0, \widehat{\beta}_1)$ are the minimizers of SSE.
   \end{block}

   \begin{block}
   {Estimate of $\sigma^2$}
   \begin{itemize}


   \item
   $$
   \widehat{\sigma}^2 = \frac{1}{n-2} \sum_{i=1}^n \left(Y_i - \widehat{f}(X_i)\right)^2 = \frac{1}{n-2} \sum_{i=1}^n \left(Y_i - \widehat{Y}_i\right)^2.$$
   \item Why $n-2$? According to our statistical model
   $$
   \frac{\widehat{\sigma}^2}{\sigma^2} \sim \frac{\chi^2_{n-2}}{n-2}.$$
   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Geometry of Least Squares: Simple Linear Model}

   \resizebox{!}{2.5in}{\includegraphics{./figs/simple/axes_simple_full.png}}

   \begin{center}
   Why $\chi^2_{n-2}$?
   \end{center}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Inference}

   \begin{block}
   {What do we mean by inference?}
   \begin{itemize}[<+->]
   \item Generally: ``learning something about
   the relationship between the sample $(X_1, \dots, X_n)$ and $(Y_1, \dots, Y_n)$.''

   \item In the simple linear regression model, learning about $\beta_0, \beta_1$:
   \begin{itemize}
   \item {\em confidence intervals, hypothesis tests}.
   \end{itemize}
   \end{itemize}
   \end{block}
   \begin{block}
   {Tools for inference}
   \begin{itemize}[<+->]
   \item Most of the questions of ``inference'' in this course
   can be answered in terms of $t$-statistics or $F$-statistics.

   \item First we will talk about $t$-statistics, later $F$-statistics.
   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Hypothesis tests}

   \begin{block}
   {What is a (statistical) hypothesis?}
   Examples:
   \begin{itemize}[<+->]
   \item One sample problem: given an independent sample $\pmb{X}=(X_1, \dots, X_n)$ where $X_i\sim N(\mu,\sigma^2)$, the {\em null hypothesis $H_0:\mu=0$} says that in fact the population mean is 0.

   \item Two sample problem: given two independent samples $\pmb{Z}=(Z_1, \dots, Z_n)$, $\pmb{W}=(W_1, \dots, W_m)$  where $Z_i\sim N(\mu_1,\sigma^2)$ and $W_i \sim N(\mu_2, \sigma^2)$, the {\em null hypothesis $H_0:\mu_1=\mu_2$} says that in fact the population mean of the two samples are identical.
   \end{itemize}
   \end{block}
   \begin{block}
   {Testing a hypothesis}
   \begin{itemize}
   \item We test a null hypothesis, $H_0$ based on some test statistic $T$ whose distribution is fully known when $H_0$ is true.
   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{$t$-statistics}

   \begin{block}    {What is a $t$-statistic?}

   \begin{itemize}
   \item Start with $Z \sim N(0,1)$ is standard normal and $S^2 \sim \chi^2_{\nu}$, independent of $Z$.
   \item Compute
   $$
   T = \frac{Z}{\sqrt{\frac{S^2}{\nu}}}.$$

   \item Then,  $T \sim t_{\nu}$ has a $t$-distribution with $\nu$ degrees of freedom.


   \item Generally, a $t$-statistic has the form
   $$
   T = \frac{\text{parameter estimate - true parameter, i.e. $\widehat{\beta}_1-\beta_1$}}{\text{standard error of parameter, i.e. $SE(\widehat{\beta}_1)$}}$$
   \end{itemize}
   \end{block}
   \end{frame}

   %CODE
       % xseq <- seq(-3,3,0.01)
   % plot(xseq, dt(xseq, 10), xlab='s', ylab='Density -- f(s)', type='l', lwd=3, col='red')
   % lines(xseq, dnorm(xseq), lty=2, lwd=3, col='orange')
   % abline(v=qt(0.975,10), lty=1, col='red')
   % abline(v=qt(0.025,10), lty=1, col='red')
   % abline(v=qnorm(0.975), lty=2, col='orange')
   % abline(v=qnorm(0.025), lty=2, col='orange')
   % legend(-1,0.2, c('t, 10 df', 'Normal'), lty=c(1,2), col=c('red', 'orange'))
   % 
   % 
   % 


   \begin{frame}
   \frametitle{$t$ vs. Normal}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/d291f0ec75.png}}    
   \end{center}
   \href{http://stats191.stanford.edu/simple.html#density-of-a-random-variable}{R code}
   \end{frame}

   %CODE
       % from matplotlib import rc
   % import pylab, numpy as np, sys
   % np.random.seed(0);import random; random.seed(0)
   % sys.path.append('/private/var/folders/dq/4_9bwd013ln6vvf_q110mwrh0000gn/T/tmp1gpSKi')
   % f=pylab.gcf(); f.set_size_inches(8.0,6.0)
   % datadir ='/private/var/folders/dq/4_9bwd013ln6vvf_q110mwrh0000gn/T/tmp1gpSKi/data'
   % import pylab, numpy as np
   % import scipy.stats
   % 
   % df = 10
   % x = np.linspace(-4,4,101)
   % y = np.exp(-x**2/2) / np.sqrt(2*np.pi)
   % pylab.plot(x,y*100, linewidth=2, label='Normal')
   % pylab.plot(x,scipy.stats.t.pdf(x, df)*100, linewidth=2, label=r'$T$, df=10')
   % 
   % x2 = np.linspace(2,4,101)
   % y2 = np.exp(-x2**2/2) / np.sqrt(2*np.pi)
   % xf, yf = pylab.poly_between(x2, 0*x2, y2*100)
   % pylab.fill(xf, yf, facecolor='red', hatch='\\', alpha=0.5)
   % 
   % 
   % x2 = np.linspace(-4,-2,101)
   % y2 = np.exp(-x2**2/2) / np.sqrt(2*np.pi)
   % xf, yf = pylab.poly_between(x2, 0*x2, y2*100)
   % pylab.fill(xf, yf, facecolor='red', hatch='\\', alpha=0.5)
   % 
   % # The t region
   % 
   % x2 = np.linspace(scipy.stats.t.isf(0.025,df),4, 101)
   % y2 = scipy.stats.t.pdf(x2, df)
   % xf, yf = pylab.poly_between(x2, 0*x2, y2*100)
   % pylab.fill(xf, yf, facecolor='gray', hatch='\\', alpha=0.5)
   % 
   % 
   % 
   % x2 = np.linspace(-4,-scipy.stats.t.isf(0.025,df), 101)
   % y2 = scipy.stats.t.pdf(x2, df)
   % xf, yf = pylab.poly_between(x2, 0*x2, y2*100)
   % pylab.fill(xf, yf, facecolor='gray', hatch='\\', alpha=0.5)
   % 
   % pylab.gca().set_xlabel('standardized units')
   % pylab.gca().set_ylabel('% per standardized unit')
   % pylab.legend()
   % #pylab.gca().set_xlim([-2,4])
   % #pylab.gca().set_yticks([])
   % 


   \begin{frame}
   \frametitle{Student's $T$}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/c30434d44c.pdf}}    
   \end{center}
   Comparison of two-sided {\color{blue} 5\% rejection rule}, df=10
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Example of a $t$-statistic}

   \begin{block}
   {One sample $t$-test}
   \begin{itemize}

   \item Given an independent sample $\pmb{X}=(X_1, \dots, X_n)$ where $X_i\sim N(\mu,\sigma^2)$ we can test $H_0:\mu=0$ using a $T$-statistic.

   \item We can prove that the random variables
   $$\overline{X} \sim N(\mu, \sigma^2/n), \qquad \frac{S^2_X}{\sigma^2} \sim \frac{\chi^2_{n-1}}{n-1}$$
   are independent.

   \item Therefore, for any $\mu$
   $$
   \frac{\overline{X} - \mu}{S_X / \sqrt{n}} = \frac{ (\overline{X}-\mu) / (\sigma/\sqrt{n})}{S_X / \sigma} \sim t_{n-1}.$$
   \item Finally, under $H_0:\mu=0$, $\overline{X}/(S_X/\sqrt{n}) \sim t_{n-1}
   $
   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Confidence intervals}

   \begin{block}
   {What is a confidence interval?}
   Examples:
   \begin{itemize}[<+->]
   \item One sample problem: instead of deciding whether $\mu=0$, we might want to come up with an (random) interval $[L,U]$ based on the sample $\pmb{X}$ such that the probability
   the true (nonrandom) $\mu$ is contained in $[L,U]$ equal to $1-\alpha$, i.e. 95\%.
   \item Two sample problem: find a (random) interval $[L,U]$ based on the samples $\pmb{Z}$ and $\pmb{W}$ such that
   the probability the true (nonrandom) $\mu_1-\mu_2$ is contained in $[L,U]$ is equal to $1-\alpha$, i.e. 95\%.
   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Example of a confidence interval}

   \begin{block}
   {One sample: confidence interval for $\mu$}
   \begin{itemize}[<+->]

   \item Given an independent sample $(X_1, \dots, X_n)$ where
   $X_i\sim N(\mu,\sigma^2)$ we can test construct
   a $(1-\alpha)*100\%$ using the
   numerator and denominator of the $t$-statistic...

   \item Let $q=t_{n-1,(1-\alpha/2)}$

   $$
   \begin{aligned}
   1 - \alpha &= P\left(-q \leq \frac{\mu - \overline{X}}
   {S_X / \sqrt{n}} \leq q \right) \\
   &= P\left(-q \cdot {S_X / \sqrt{n}} \leq {\mu - \overline{X}}
   \leq q  \cdot {S_X / \sqrt{n}} \right) \\
   &= P\left(\overline{X} - q  \cdot {S_X / \sqrt{n}}
   \leq {\mu} \leq \overline{X} + q  \cdot {S_X / \sqrt{n}} \right) \\
   \end{aligned}
   $$

   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Inference in regression}

   \begin{block}
   {Heights example}
   \begin{itemize}

   \item Model:
   $$
   Y_i = \beta_0 + \beta_1 X_i + \varepsilon_i,$$
   errors $\varepsilon_i$ are independent $N(0, \sigma^2)$.
   \item In our ``prototypical'' data example, we might want to now if there
   really is a linear association between ${\tt Daughter}=Y$
   and ${\tt Mother}=X$, {\em hypothesis test} of $H_0:\beta_1=0$.
   This assumes the model above is correct, but that $\beta_1=0$.

   \item Or, we might want to have a range of values that we can be fairly certain $\beta_1$ lies between: a {\em confidence interval} for $\beta_1$.
   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Geometry of Least Squares: Full Model}

   \resizebox{!}{3in}{\includegraphics{./figs/simple/axes_simple_full.png}}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Geometry of Least Squares: Reduced Model}

   \resizebox{!}{3in}{\includegraphics{./figs/simple/axes_simple_reduced.png}}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Geometry of Least Squares: Both Models}

   \resizebox{!}{3in}{\includegraphics{./figs/simple/axes_simple.png}}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Simple linear regression: setup for inference}

   \begin{block}{Geometry}

   \begin{itemize}

   \item Let $L$ be the subspace of $\real^n$ spanned $\pmb{1}=(1, \dots, 1)$ and $\pmb{X}=(X_1, \dots, X_n)$.
   \item Then,
   $$\pmb{Y} = P_L\pmb{Y} + (\pmb{Y} - P_L\pmb{Y}) = \widehat{\pmb{Y}} + (Y - \widehat{\pmb{Y}}) = \widehat{\pmb{Y}} + e$$
   \item In our model $\mu=\beta_0 \pmb{1} + \beta_1 \pmb{X} \in L$ so that
   $$
   \widehat{\pmb{Y}} = \mu + P_L\pmb{\varepsilon}, \qquad \pmb{e} = P_{L^{\perp}}{\pmb{Y}} = P_{L^{\perp}}\pmb{\varepsilon}$$
   \item Our assumption that $\varepsilon_i$'s are independent $N(0,\sigma^2)$ tells us that (don't worry about proving this)
   \begin{itemize}
   \item $\pmb{e}$ and $\widehat{\pmb{Y}}$ are independent

   \item $\widehat{\sigma}^2 = \|\pmb{e}\|^2 / (n-2) \sim \sigma^2 \cdot \chi^2_{n-2} / (n-2)$.
   \end{itemize}

   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Simple linear regression: distributions}

   \begin{block}
   {Distribution of $\widehat{\beta}_1$}
   \begin{itemize}

   \item Our assumptions tell us that
   $$
   \widehat{\beta}_1 \sim N\left(\beta_1, \frac{\sigma^2}{\sum_{i=1}^n(X_i-\overline{X})^2}\right)$$

   \item Therefore,
   $$\frac{\widehat{\beta}_1 - \beta_1}{\sigma \sqrt{\frac{1}{\sum_{i=1}^n(X_i-\overline{X})^2}}} \sim N(0,1).$$

   \end{itemize}
   \end{block}
   \begin{block}
   {Standard error of $\widehat{\beta}_1$}
   $$
   SE(\widehat{\beta}_1) = \widehat{\sigma} \sqrt{\frac{1}{\sum_{i=1}^n(X_i-\overline{X})^2}} \qquad \text{independent of $\widehat{\beta}_1$}$$

   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Simple linear regression: testing}

   \begin{block}
   {$t$-test of $H_0:\beta_1=\beta_1^0$}
   \begin{itemize}[<+->]

   \item Suppose we want to test that $\beta_1$ is some pre-specified
   value, $\beta_1^0$ (this is often 0: i.e. is there a linear association)

   \item Under $H_0:\beta_1=\beta_1^0$
   $$\frac{\widehat{\beta}_1 - \beta^0_1}{\widehat{\sigma} \sqrt{\frac{1}{\sum_{i=1}^n(X_i-\overline{X})^2}}} = \frac{\widehat{\beta}_1 - \beta^0_1}{ \frac{\widehat{\sigma}}{\sigma}\cdot \sigma \sqrt{\frac{1}{\sum_{i=1}^n(X_i-\overline{X})^2}}} \sim t_{n-2}.$$


   \item Reject $H_0:\beta_1=\beta_1^0$ if $|T| > t_{n-2, 1-\alpha/2}$.

   \end{itemize}

   \end{block}

   \begin{block}
   {Why reject for large $|T|$?}

   \begin{itemize}[<+->]

   \item Observing a large $|T|$ is unlikely if $\beta_1 = \beta_1^0$: reasonable to conclude that $H_0$ is false.

   \item Common to report $p$-value $= \Pp(|T_{n-2}| > |T|).$

   \end{itemize}

   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Confidence intervals based on $t$ distribution}

   \begin{block}
   {Generic setup}
   \begin{itemize}[<+->]
   \item Suppose we have a parameter estimate $\widehat{\theta} \sim N(\theta, \widetilde{\sigma}^2)$, and standard error $SE(\widehat{\theta})$ such that
   $$
   \frac{\widehat{\theta}-\theta}{SE(\widehat{\theta})} \sim t_{\nu}.$$

   \item $(1-\alpha) \cdot 100 \%$ confidence interval:
   $$
   \widehat{\theta} \pm SE(\widehat{\theta}) \cdot t_{\nu, 1-\alpha/2}.$$

   \item Why? Expand absolute value as we did for the one-sample CI
   $$
   1 - \alpha = \Pp\left(\left|\frac{\widehat{\theta} - \theta}{SE(\widehat{\theta})} \right| < t_{\nu, 1-\alpha/2}\right)
   $$
   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Confidence intervals for regression parameters}

   \begin{block}
   {Interval for $\beta_1$}
   A $(1-\alpha) \cdot 100 \%$ confidence interval:
   $$
   \widehat{\beta}_1 \pm SE(\widehat{\beta}_1) \cdot t_{n-2, 1-\alpha/2}.$$


   \end{block}

   \begin{block}
   {Interval for regression line $\beta_0 + \beta_1 \cdot X$}
   \begin{itemize}[<+->]

   \item $(1-\alpha) \cdot 100 \%$ confidence interval:
   $$
   \widehat{\beta}_0 + \widehat{\beta}_1 X \pm SE(\widehat{\beta}_0 + \widehat{\beta}_1 X) \cdot t_{n-2, 1-\alpha/2}$$
   where $$
   SE(a_0\widehat{\beta}_0 + a_1\widehat{\beta}_1) = \widehat{\sigma} \sqrt{\frac{a_0^2}{n} + \frac{(a_0\overline{X} - a_1)^2}{\sum_{i=1}^n \left(X_i-\overline{X}\right)^2}}$$
   \end{itemize}

   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Forecasting (prediction) interval}

   \begin{block}
   {Predicting a new observation}
   \begin{itemize}[<+->]

   \item Suppose we want an interval that will contain
   the height of the daughter in a new family sampled from the population
   where the mother has height $X_{\text{new}}$, i.e. an
   interval that will cover
   $$
   Y_{\text{new}} = \beta_0 + \beta_1 X_{\text{new}} + \varepsilon_{\text{new}}$$
   with a certain probability.
   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Forecasting (prediction) interval}

   \begin{block}
   {Predicting a new observation}
   \begin{itemize}[<+->]

   \item
   $$
   SE(\widehat{\beta}_0 + \widehat{\beta}_1 X_{\text{new}} + \varepsilon_{\text{new}}) = \widehat{\sigma} \sqrt{1 + \frac{1}{n} + \frac{(\overline{X} - X_{\text{new}})^2}{\sum_{i=1}^n \left(X_i-\overline{X}\right)^2}}.$$

   \item Prediction interval  is
   $$ \widehat{\beta}_0 +  \widehat{\beta}_1 X_{\text{new}} \pm t_{n-2, 1-\alpha/2} \cdot SE(\widehat{\beta}_0 + \widehat{\beta}_1 X_{\text{new}} + \varepsilon_{\text{new}})
   $$
   \end{itemize}
   \end{block}
   \end{frame}

   \part{Diagnostics for simple linear regression  (Ch. 2 and some of 4, RABE)}
   \frame{\partpage}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Outline}

   \begin{block}{Diagnostics for simple regression}
   \begin{itemize}
   \item Goodness of fit of regression: analysis of variance.

   \item $F$-statistics.

   \item Residuals.

   \item Diagnostic plots.
   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Geometry of Least Squares}

   \resizebox{!}{2.5in}{\includegraphics{./figs/simple/axes_simple_full.png}}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Geometry of Least Squares}

   \resizebox{!}{2.5in}{\includegraphics{./figs/simple/axes_simple_reduced.png}}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Geometry of Least Squares}

   \resizebox{!}{2.5in}{\includegraphics{./figs/simple/axes_simple.png}}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Goodness of fit}

   \begin{block}
   {Sums of squares}
   $$
   \begin{aligned}
   SSE &= \sum_{i=1}^n(Y_i - \widehat{Y}_i)^2 = \sum_{i=1}^n (Y_i - \widehat{\beta}_0 - \widehat{\beta}_1 X_i)^2 \\
   SSR &= \sum_{i=1}^n(\overline{Y} - \widehat{Y}_i)^2 = \sum_{i=1}^n (\overline{Y} - \widehat{\beta}_0 - \widehat{\beta}_1 X_i)^2 \\
   SST &= \sum_{i=1}^n(Y_i - \overline{Y})^2 = SSE + SSR \\
   R^2 &= \frac{SSR}{SST} = 1 - \frac{SSE}{SST} = \widehat{Cor}(\pmb{X},\pmb{Y})^2.
   \end{aligned}
   $$


   Basic idea: if $R^2$ is large: a lot  of the variability in $\pmb{Y}$ is explained by $\pmb{X}$.

   \end{block}
   \end{frame}

   %CODE
       % X = seq(0,20, length=21)
   % Y = 0.5*X+1 + rnorm(21)
   % 
   % Y.lm = lm(Y~X)
   % 
   % p = predict(Y.lm)
   % m = mean(Y)
   % 
   % # SST: deviations of Y's around
   % # the horizontal line for the mean
   % 
   % plot(X, Y, pch=23, bg='red')
   % abline(h=m, col='yellow', lwd=2)
   % for (i in 1:21) {
   %   points(X[i], m, pch=23, bg='yellow')
   %   lines(c(X[i], X[i]), c(Y[i], m))
   % }
   % 
   % # show the regression line as well
   % 
   % abline(Y.lm, col='green', lwd=2)
   % 
   % 


   \begin{frame}
   \frametitle{Total sum of squares}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/b704968620.png}}    
   \end{center}
   \href{http://stats191.stanford.edu/simple_diagnostics.html#goodness-of-fit-sums-of-squares}{R code}
   \end{frame}

   %CODE
       % # SSE: deviations of Y's around
   % # the regression line
   % 
   % plot(X, Y, pch=23, bg='red')
   % abline(Y.lm, col='green', lwd=2)
   % for (i in 1:21) {
   %   points(X[i], p[i], pch=23, bg='green')
   %   lines(c(X[i], X[i]), c(Y[i], p[i]))
   % }
   % 
   % m = mean(Y)
   % abline(h=m, col='yellow', lwd=2)
   % 
   % 
   % 


   \begin{frame}
   \frametitle{Error sum of squares}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/6891d23643.png}}    
   \end{center}
   \href{http://stats191.stanford.edu/simple_diagnostics.html#goodness-of-fit-sums-of-squares}{R code}
   \end{frame}

   %CODE
       % # SSR: deviations of Yhat's around
   % # the horizontal line for the mean
   % 
   % plot(X, Y, pch=23, bg='red')
   % abline(Y.lm, col='green', lwd=2)
   % abline(h=m, col='yellow', lwd=2)
   % for (i in 1:21) {
   %   points(X[i], p[i], pch=23, bg='green')
   %   points(X[i], m, pch=23, bg='yellow')
   %   lines(c(X[i], X[i]), c(m, p[i]))
   % }
   % 
   % 


   \begin{frame}
   \frametitle{Regression sum of squares}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/0f6fb88c86.png}}    
   \end{center}
   \href{http://stats191.stanford.edu/simple_diagnostics.html#goodness-of-fit-sums-of-squares}{R code}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{$F$-statistics}

   \begin{block}
   {What is an $F$-statistic?}

   \begin{itemize}


   \item An $F$-statistic is a ratio of ``{\em sample variances (mean squares)}'': it has a numerator, $N$,  and a denominator, $D$ that are independent.


   \item Let $$N \sim \frac{\chi^2_{\rm num} }{ df_{{\rm num}}}, \qquad D \sim \frac{\chi^2_{\rm den} }{ df_{{\rm den}}}
   $$
   and define
   $$F = \frac{N}{D}.$$

   \item We say $F$ has an $F$ distribution with parameters $df_{{\rm num}}, df_{{\rm den}}$ and write $F \sim F_{df_{{\rm num}}, df_{{\rm den}}}$.
   \end{itemize}

   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Geometry of Least Squares}

   \resizebox{!}{2.5in}{\includegraphics{./figs/simple/axes_simple.png}}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{$F$-statistic in simple linear regression}

   \begin{block}
   {Goodness of fit $F$-statistic}
   \begin{itemize}
   \item The ratio $$
   F=\frac{SSR/1}{SSE/(n-2)} = \frac{MSR}{MSE}$$
   can be thought of as a {\em ratio of ``variances''}.

   \item In fact, under $H_0:\beta_1=0$, $$
   F \sim F_{1, n-2}
   $$
   because
   $$
   \begin{aligned}
   SSR &= \|\widehat{\pmb{Y}} - \overline{Y} \cdot \pmb{1}\|^2 \\
   SSE &= \|\pmb{Y} - \widehat{\pmb{Y}}\|^2
   \end{aligned}
   $$
   and from our picture, these vectors are orthogonal.
   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{$F$ and $t$ statistics}

   \begin{block}
   {Relation between $F$ and $t$}
   \begin{itemize}
   \item If $T \sim t_{\nu}$, then
   $$
   T^2 \sim \frac{N(0,1)^2}{\chi^2_{\nu}/\nu} \sim \frac{\chi^2_1/1}{\chi^2_{\nu}/\nu}.$$

   \item In other words, the square of a $t$-statistic is an $F$-statistic.
   Because it is always positive, an $F$-statistic has no {\em direction $(\pm)$} associated with it.


   \item In fact, (see \href{http://stats191.stanford.edu/simple_diagnostics.html#poorly-fitting-linear-model}{R code})
   $$
   F = \frac{MSR}{MSE} = \frac{\widehat{\beta}_1^2}{SE(\widehat{\beta}_1)^2}.$$

   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{$F$-statistics in regression models}

   \begin{block}
   {Interpretation of an $F$-statistic}

   \begin{itemize}

   \item In regression, the numerator is usually a difference in ``goodness'' of fit of two  (nested) models.

   \item The denominator is $\widehat{\sigma}^2$ -- an estimate of $\sigma^2$.

   \item Our example today: the bigger model is the simple linear regression model, the smaller is the model with constant mean (one sample model).

   \item If the $F$ is large, it says that the ``bigger''  model explains a lot more variability in $\pmb{Y}$  (relative to $\sigma^2$) than the smaller one.

   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{$F$-test in simple linear regression}

   \begin{block}
   {Example in more detail}
   \begin{itemize}
   \item {\em Full (bigger) model :}
   $$
   FM: \qquad Y_i = \beta_0 + \beta_1 X_i + \varepsilon_i$$
   \item {\em Reduced (smaller) model:}
   $$
   RM: \qquad Y_i = \beta_0  + \varepsilon_i$$

   \item The $F$-statistic has the form
   $$
   F=\frac{(SSE(RM) - SSE(FM)) / (df_{RM} - df_{FM})}{SSE(FM) / df_{FM}}.$$
   \item Reject $H_0: RM$ is correct, if $F > F_{1-\alpha, 1, n-2}$.

   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Diagnostics}

   \begin{block}
   {What are the assumptions}
   \begin{itemize}
   \item $$
   Y_i = \beta_0 + \beta_1 X_{i} + \varepsilon_i
   $$
   \item Errors $\varepsilon_i$ are assumed independent $N(0,\sigma^2)$.
   \end{itemize}

   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Diagnostics}

   \begin{block}
   {What can go wrong?}

   \begin{itemize}

   \item
   Regression function can be wrong: maybe regression function should be quadratic
   (see \href{http://stats191.stanford.edu/simple_diagnostics.html#poorly-fitting-linear-model}{R code}).

   \item Model for the errors
   may be incorrect:
   \begin{itemize}
   \item  may not be normally distributed.
   \item  may not be independent.

   \item  may not have the same variance.
   \end{itemize}

   \item Detecting problems is more {\em art} then {\em science}, i.e.
   we cannot {\em test} for all possible problems in a regression model.

   \item Basic idea of diagnostic measures: if model is correct then
   residuals $e_i = Y_i -\widehat{Y}_i, 1 \leq i \leq n$ should look like a sample of
   (not quite independent) $N(0, \sigma^2)$ random variables.
   \end{itemize}
   \end{block}
   \end{frame}

   %CODE
       % 
   % url = "http://stats191.stanford.edu/data/anscombe.table"
   % anscombe <- read.table(url, header=T)
   % 
   % # Another little R tip...
   % # If you don't attach a file, you need
   % # to use "$" notation to get the variables
   % 
   % # Add some noise because the curve is almost quadratic already
   % anscombe$Y2 = anscombe$Y2 + rnorm(length(anscombe$Y2)) * 0.45
   % 
   % plot(anscombe$X2, anscombe$Y2, pch=23, bg='orange', cex=2, ylab='Y', xlab='X')
   % 
   % simple.lm <- lm(anscombe$Y2 ~ anscombe$X2)
   % 
   % abline(simple.lm$coef, lwd=2, col='red', lty=2)
   % 
   % 


   \begin{frame}
   \frametitle{A bad simple linear regression model}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/2e8a393fc0.png}}    
   \end{center}
   \href{http://stats191.stanford.edu/simple_diagnostics.html#poorly-fitting-linear-model}{R code}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Diagnostic plots}

   \begin{block}
   {Problems in the regression function}
   \begin{itemize}

   \item True regression function may have higher-order non-linear terms, polynomial or otherwise.
   \item
   Can sometimes be remedied by looking at a plot of
   $\pmb{X}$ vs. residuals $\pmb{e}$. If there is any visible ``trend'' in this
   plot, may consider adding more terms to the model to capture this trend (this makes the model a {\em multiple regression model}).

   \end{itemize}
   \end{block}
   \end{frame}

   %CODE
       % 
   % plot(anscombe$X2, resid(simple.lm), ylab='Residual', xlab='X',
   %      pch=23, bg='orange', cex=2)
   % 
   % ## put a horizontal line through 0
   % abline(h=0, lwd=2, col='red', lty=2)
   % 
   % 


   \begin{frame}
   \frametitle{Residuals from linear model}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/26830ab3f8.png}}    
   \end{center}
   \href{http://stats191.stanford.edu/simple_diagnostics.html#poorly-fitting-linear-model}{R code}
   \end{frame}

   %CODE
       % 
   % # Improve model by adding quadratic term
   % # Instead of attaching, can use the "data" argument to lm
   % 
   % quadratic.lm <- lm(Y2 ~ poly(X2, 2), data=anscombe)
   % 
   % # Replot data, adding fitted quadratic
   % Xsort <- sort(anscombe$X2)
   % plot(anscombe$X2, anscombe$Y2, pch=23, bg='orange', cex=2, ylab='Y', xlab='X')
   % lines(Xsort, predict(quadratic.lm, list(X2=Xsort)), col='red', lty=2, lwd=2)
   % 
   % 


   \begin{frame}
   \frametitle{Quadratic model}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/20e15ae786.png}}    
   \end{center}
   \href{http://stats191.stanford.edu/simple_diagnostics.html#poorly-fitting-linear-model}{R code}
   \end{frame}

   %CODE
       % 
   % plot(anscombe$X2, resid(quadratic.lm), ylab='Residual',
   %      xlab='X', pch=23, bg='orange', cex=2)
   % ## put a horizontal line through 0
   % abline(h=0, lwd=2, col='red', lty=2)
   % 
   % 


   \begin{frame}
   \frametitle{Residuals from quadratic model}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/ef7e20b21e.png}}    
   \end{center}
   \href{http://stats191.stanford.edu/simple_diagnostics.html#poorly-fitting-linear-model}{R code}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Problems with the errors}

   \begin{block}
   {Possible problems \& diagnostic checks}
   \begin{itemize}

   \item
   Errors may not be normally distributed or may not have  the same variance -- {\tt qqnorm} can help with this.

   \item Variance may not be constant. Can also be addressed in a plot of
   $\pmb{X}$ vs. $\pmb{e}$: {\em fan shape} or other trend indicate
   non-constant variance.

   \item Outliers: points where the model really does not fit! Possibly mistakes in data transcription, lab errors, who knows? Should be recognized and (hopefully) explained.

   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Non-normality}

   \begin{block}
   {{\tt qqnorm}}
   \begin{itemize}
   \item If $e_i, 1\leq i \leq n$ were really a sample of
   $N(0, \sigma^2)$ then their sample quantiles should be close to the
   sample quantiles of the $N(0, \sigma^2)$ distribution.

   \item Plot:
   $$
   e_{(i)}  \ {\rm vs.} \  \Ee(\varepsilon_{(i)}), \qquad 1 \leq i \leq n.$$
   where $e_{(i)}$ is the $i$-th smallest residual (order statistic) and
   $\Ee(\varepsilon_{(i)})$ is the expected value for independent $\varepsilon_i$'s $\sim N(0,\sigma^2)$.
   \end{itemize}
   \end{block}
   \end{frame}

   %CODE
       % qqnorm(resid(simple.lm), pch=23, bg='orange', cex=2)


   \begin{frame}
   \frametitle{QQplot of residuals from linear model}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/d2641c1df6.png}}    
   \end{center}
   \href{http://stats191.stanford.edu/simple_diagnostics.html#poorly-fitting-linear-model}{R code}
   \end{frame}

   %CODE
       % qqnorm(resid(quadratic.lm), pch=23, bg='orange', cex=2)


   \begin{frame}
   \frametitle{QQplot of residuals from quadratic model}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/0669147262.png}}    
   \end{center}
   \href{http://stats191.stanford.edu/simple_diagnostics.html#poorly-fitting-linear-model}{R code}
   \end{frame}

   %CODE
       % url = 'http://stats191.stanford.edu/data/HIV.VL.table'
   % 
   % viral.load = read.table(url, header=T)
   % attach(viral.load)
   % 
   % plot(GSS, VL, pch=23, bg='orange', cex=2)
   % viral.lm = lm(VL ~ GSS)
   % abline(viral.lm, col='red', lwd=2)
   % 
   % ## let's remove the outlier
   % good = (VL < 250000)
   % viral.lm = lm(VL ~ GSS, subset=good)
   % abline(viral.lm, col='green', lwd=2)


   \begin{frame}
   \frametitle{Outlier and nonconstant variance}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/a4e3d1542a.png}}    
   \end{center}
   \href{http://stats191.stanford.edu/simple_diagnostics.html#outlier-and-nonconstant-variance}{R code}
   \end{frame}

   %CODE
       % plot(GSS[good], resid(viral.lm), pch=23,
   % bg='orange', cex=2, xlab='GSS', ylab='Residual')
   % abline(h=0, lwd=2, col='red', lty=2)


   \begin{frame}
   \frametitle{Outlier and nonconstant variance}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/0824cf9384.png}}    
   \end{center}
   \href{http://stats191.stanford.edu/simple_diagnostics.html#outlier-and-nonconstant-variance}{R code}
   \end{frame}

   \part{Multiple linear regression (Ch. 3, RABE)}
   \frame{\partpage}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Multiple linear regression}

   \begin{block}
   {Outline}
   \begin{itemize}


   \item Specifying the model.

   \item Fitting the model: least squares.

   \item Interpretation of the coefficients.

   \item More on $F$-statistics.

   \item Matrix approach to linear regression.

   \item $T$-statistics revisited.

   \item More $F$ statistics.

   \item Tests involving more than one $\beta$.

   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Job supervisor data: $n=30$}

   \begin{block}
   {Description}

   \begin{tabular}{c|l}
   Variable & Description \\ \hline
   $Y$ & Overall supervisor job rating \\
   $X_1$ & How well do they handle complaints \\
   $X_2$ & Do they allow  special priveleges \\
   $X_3$ & Give opportunity to learn new things \\
   $X_4$ & Raises based on performance \\
   $X_5$ & Too critical of poor performance\\
   $X_6$ & Good rate of advancement \\
   \end{tabular}
   \end{block}
   \end{frame}

   %CODE
       % url = 'http://stats191.stanford.edu/data/supervisor.table'
   % supervisor.table <- read.table(url, header=T)
   % 
   % pairs(supervisor.table, pch=23, bg='orange',
   % cex.labels=6, cex=2)


   \begin{frame}
   \frametitle{Job supervisor data}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/a7ea3b76fe.png}}    
   \end{center}
   \href{http://stats191.stanford.edu/multiple.html#job-supervisor-example}{R code}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Specifying the model}

   \begin{block}
   {Multiple linear regression      model}

   \begin{itemize}

   \item Rather than one predictor, we have $p=6$ predictors.

   \item $$
   Y_i = \beta_0 + \beta_1 X_{i1} + \dots + \beta_p X_{ip} + \varepsilon_i
   $$

   \item Errors $\varepsilon$ are assumed independent $N(0,\sigma^2)$, as in simple linear regression.

   \item Coefficients are called (partial) regression coefficients because they ``allow'' for the effect of other variables.

   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Geometry of Least Squares: Multiple Regression}

   \resizebox{!}{3in}{\includegraphics{./figs/simple/axes_multiple_full.png}}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Fitting the model}

   \begin{block}
   {Least squares}
   \begin{itemize}

   \item Just as in simple linear regression, model is fit by minimizing
   $$
   \begin{aligned}
   SSE(\beta_0, \dots, \beta_p) &= \sum_{i=1}^n(Y_i - (\beta_0 + \sum_{j=1}^p \beta_j X_{ij}))^2 \\
   &= \|Y - \widehat{Y}(\beta)\|^2
   \end{aligned}
   $$

   \item Minimizers: $\widehat{\beta} = (\widehat{\beta}_0, \dots, \widehat{\beta}_p)$ are
   the ``least squares estimates'': are also normally distributed as in simple linear regression.


   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Error component}

   \begin{block}
   {Estimating $\sigma^2$}
   \begin{itemize}


   \item As in simple regression
   $$
   \widehat{\sigma}^2 = \frac{SSE}{n-p-1} \sim \sigma^2 \cdot \frac{\chi^2_{n-p-1}}{n-p-1}$$
   independent of $\widehat{\beta}$.


   \item
   Why $\chi^2_{n-p-1}$?
   Typically, the degrees of freedom in the estimate of $\sigma^2$ is $n-\# \text{number of parameters in regression function}$.
   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Interpretation of $\beta_j$'s}

   \begin{block}
   {Supervisor example}
   \begin{itemize}

   \item Take $\beta_1$ for example. This is the amount the average
   job rating increases for one ``unit'' of ``Handles complaints'',
   keeping everything else constant.

   \item Units of ``Handles complaints'' are individual favorable responses, so on average
   for every extra person who rated the supervisor as good at handling complaints
   (other things being fixed), the average job rating increases by $\beta_1$.

   \item We often phrase this as the effect of ``Handles complaints'' {\em
   allowing for} or {\em controlling for} the other variables.
   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Interpretation of $\beta_j$'s}

   \begin{block}
   {Why are they {\em partial} regression coefficients?}
   \begin{itemize}

   \item The term {\em partial} refers to the fact that the coefficient $\beta_j$
   represent the partial effect of $\pmb{X}_j$ on $\pmb{Y}$, i.e. after the effect of all other variables have been removed.

   \item Specifically,
   $$
   Y_i - \sum_{l=1, l \neq j}^k X_{il} \beta_l = \beta_0 + \beta_j X_{ij} + \varepsilon_i.$$


   \item Let $e_{i,(j)}$ be the residuals from regressing $\pmb{Y}$ onto all $\pmb{X}_{\cdot}$'s except $\pmb{X}_j$, and let $X_{i,(j)}$ be the residuals from
   regressing $\pmb{X}_j$ onto all $\pmb{X}_{\cdot}$'s except $\pmb{X}_j$, and let $X_{i,(j)}$.

   \item If we regress $e_{i,(j)}$ against $X_{i,(j)}$, the coefficient
   is {\em exactly} the same as in the original model (see \href{http://stats191.stanford.edu/multiple.html}{R code}).

   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Goodness of fit for multiple regression}

   \begin{block}
   {Sums of squares}
   $$
   \begin{aligned}
   SSE &= \sum_{i=1}^n(Y_i - \widehat{Y}_i)^2 \\
   SSR &= \sum_{i=1}^n(\overline{Y} - \widehat{Y}_i)^2 \\
   SST &= \sum_{i=1}^n(Y_i - \overline{Y})^2 \\
   R^2 &= \frac{SSR}{SST}
   \end{aligned}
   $$
   $R^2$ is called the {\em multiple correlation coefficient} of the model, or
   the {\em coefficient of multiple determination}.
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Adjusted $R^2$}

   \begin{block}
   {Compensating for more variables}
   \begin{itemize}

   \item As we add more and more variables to the model -- even random ones, $R^2$ will increase to 1.

   \item Adjusted $R^2$ tries to take this into account by replacing
   sums of squares by {\em mean squares}
   $$
   R^2_a = 1 - \frac{SSE/(n-p-1)}{SST/(n-1)} = 1 - \frac{MSE}{MST}.$$
   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Goodness of fit test}

   \begin{block}
   {Another $F$-test}
   \begin{itemize}
   \item As in simple linear regression, we measure the goodness of fit
   of the regression model by
   $$
   F = \frac{MSR}{MSE} = \frac{\|\overline{Y}\cdot \pmb{1} - \widehat{\pmb{Y}}\|^2/p}{\|Y - \widehat{\pmb{Y}}\|^2/(n-p-1)}.$$

   \item Under $H_0:\beta_1 = \dots = \beta_p=0$,
   $$
   F \sim F_{p, n-p-1}$$
   so reject $H_0$ at level $\alpha$ if $F > F_{p,n-p-1,1-\alpha}.$
   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Geometry of Least Squares: Full Model}

   \resizebox{!}{3in}{\includegraphics{./figs/simple/axes_multiple_full.png}}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Geometry of Least Squares: Reduced Model}

   \resizebox{!}{3in}{\includegraphics{./figs/simple/axes_multiple_reduced.png}}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Geometry of Least Squares: Both Models}

   \resizebox{!}{3in}{\includegraphics{./figs/simple/axes_multiple.png}}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Intuition behind the $F$ test}

   \begin{block}
   {Measuring lengths}
   \begin{itemize}
   \item The $F$ statistic is a ratio of lengths of orthogonal vectors (divided by degrees of freedom).

   \item We can prove that our model implies
   $$
   \begin{aligned}
   \Ee \left(MSR\right) &= \sigma^2 + \underbrace{\|\pmb{\mu} - \overline{\mu} \cdot \pmb{1}\|^2 / p}_{(*)} \\
   \Ee \left(MSE\right) &= \sigma^2 \\
   \mu_i &= \Ee(Y_i) = \beta_0 + X_{i1} \beta_1  + \dots +  X_{ip} \beta_p
   \end{aligned}
   $$
   so $F$ should be not be too far from 1 if $H_0$ is true, i.e. $(*)=0$.


   \item If $F$ is large, it is evidence that $(*) \neq 0$, i.e. $H_0$ is false.
   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{$F$-test revisited}

   \begin{block}
   {Example in more detail}
   \begin{itemize}
   \item {\em Full (bigger) model :}
   $$
   Y_i = \beta_0 + \beta_1 X_{i1} + \dots \beta_p X_{ip} + \varepsilon_i$$
   \item {\em Reduced (smaller) model:}
   $$
   Y_i = \beta_0  + \varepsilon_i$$

   \item The $F$-statistic has the form
   $$
   F=\frac{(SSE(R) - SSE(F)) / (df_R - df_F)}{SSE(F) / df_F}.$$

   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Geometry of Least Squares: Goodness of Fit Test}

   \resizebox{!}{3in}{\includegraphics{./figs/simple/axes_multiple.png}}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Geometry of Least Squares: Goodness of Fit Test}

   \resizebox{!}{3in}{\includegraphics{./figs/simple/axes_general.png}}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Matrix formulation}

   \begin{block}
   {Equivalent formulation}
   $${\pmb Y}_{n \times 1} = \pmb{X}_{n \times (p + 1)} \pmb{\beta}_{(p+1) \times 1} + \pmb{\varepsilon}_{n \times 1}$$
   \begin{itemize}

   \item $\pmb{X}$ is called the {\em design matrix} of the model
   \item $\pmb{\varepsilon} \sim N(0, \sigma^2 I_{n \times n})$ is multivariate normal
   \end{itemize}
   \end{block}
   \begin{block}
   {$SSE$ in matrix form}
   $$
   SSE(\beta) = (\pmb{Y} - \pmb{X} \pmb{\beta})'(\pmb{Y} - \pmb{X} \pmb{\beta})
   $$

   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Matrix formulation}

   \begin{block}
   {Design matrix}
   \begin{itemize}
   \item The design matrix is the
   $n \times (p+1)$ matrix with entries
   $$
   \pmb{X} =
   \begin{pmatrix}
   1 & X_{11} & X_{12} & \dots & X_{1,p} \\
   \vdots &   \vdots & \ddots & \vdots \\
   1 & X_{n1} & X_{n2} &\dots & X_{n,p} \\
   \end{pmatrix}
   $$
   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Least squares solution}

   \begin{block}
   {Solving for $\Bh{}$}
   \begin{itemize}
   \item Normal equations
   $$
   \frac{\partial}{\partial \beta_j} SSE \biggl|_{\Bh{}} = -2 \left(\pmb{Y} - \pmb{X} \Bh{} \right)^t \pmb{X}_j = 0, \qquad 0 \leq j \leq p.$$

   \item Equivalent to
   $$
   \begin{aligned}
   (\pmb{Y} - \pmb{X}\pmb{\Bh{}})^t\pmb{X} &= 0 \\
   \pmb{\Bh{}} &= (\pmb{X}^t\pmb{X})^{-1}\pmb{X}^t\pmb{Y}
   \end{aligned}
   $$

   \item Properties:
   $$
   \pmb{\Bh{}} \sim N\left(\pmb{\beta}, \sigma^2 (\pmb{X}^t \pmb{X})^{-1} \right), \text{indep. of $\widehat{\sigma}^2$}
   $$
   \item  \href{http://stats191.stanford.edu/multiple.html}{R code}
   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Inference for multiple regression}

   \begin{block}
   {Regression function at one point}

   \begin{itemize}
   \item One thing one might want to {\em learn} about the regression function in the supervisor example is something about the regression function at some fixed values of $\pmb{X}_{1}, \dots, \pmb{X}_{6}$, i.e. what can be said about
   \begin{equation}
   \label{eq:comb}
   \beta_0 + 65 \cdot \beta_1  + 50 \cdot \beta_2  + 55 \cdot \beta_3 + 64 \cdot \beta_4 + 75 \cdot \beta_5 + 40 \cdot \beta_6   \tag{*}
   \end{equation}
   roughly the regression function at ``typical'' values of the predictors.

   \item The expression \eqref{eq:comb} is equivalent to
   $$
   \sum_{j=0}^6 a_j \beta_j, \qquad a=(1,65,50,55,64,75,40).$$
   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Inference for $\sum_{j=0}^p a_j \beta_j$}

   \begin{block}
   {Confidence interval for $\sum_{j=0}^p a_j \beta_j$}
   \begin{itemize}
   \item Suppose we want  a $(1-\alpha)\cdot 100\%$ CI for $\sum_{j=0}^p a_j\beta_j$.

   \item Just as in simple linear regression:

   $$
   \sum_{j=0}^p a_j \widehat{\beta}_j \pm t_{1-\alpha/2, n-p-1} \cdot SE\left(\sum_{j=0}^p a_j\widehat{\beta}_j\right).$$
   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Inference for $\sum_{j=0}^p a_j \beta_j$}

   \begin{block}
   {$T$-statistics revisited}
   \begin{itemize}
   \item Suppose we want to test
   $$H_0:\sum_{j=0}^p a_j\beta_j= h.$$ As in simple linear regression, it is based on
   $$
   T = \frac{\sum_{j=0}^p a_j \widehat{\beta}_j - h}{SE(\sum_{j=0}^p a_j \widehat{\beta}_j)}.$$
   \item If $H_0$ is true, then $T \sim t_{n-p-1}$, so we reject
   $H_0$ at level $\alpha$ if
   $$
   \begin{aligned}
   |T| &\geq t_{1-\alpha/2,n-p-1}, \qquad \text{ OR} \\
   p-\text{value} &= {\tt 2*(1-pt(|T|, n-p-1))} \leq \alpha.
   \end{aligned}
   $$
   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Inference for $\sum_{j=0}^p a_j \beta_j$}

   \begin{block}
   {One-sided tests}
   \begin{itemize}
   \item Suppose, instead, we wanted to test the one-sided hypothesis
   $$H_0:\sum_{j=0}^p a_j\beta_j \leq  h, \  \text{vs.} \ H_a: \sum_{j=0}^p a_j\beta_j >  h$$
   \item If $H_0$ is true, then $T$ is no longer exactly $t_{n-p-1}$
   but
   $$
   \Pp \left(T > t_{1-\alpha, n-p-1}\right) \leq 1 - \alpha$$
   so we reject $H_0$ at level $\alpha$ if
   $$
   \begin{aligned}
   T &\geq t_{1-\alpha,n-p-1}, \qquad \text{ OR} \\
   p-\text{value} &= {\tt (1-pt(T, n-p-1))} \leq \alpha.
   \end{aligned}
   $$
   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Inference for $\sum_{j=0}^p a_j \beta_j$}

   \begin{block}
   {Standard error of $\sum_{j=0}^p a_j \widehat{\beta}_j$}
   \begin{itemize}
   \item Based on matrix approach to regression
   $$
   SE\left(\sum_{j=0}^p a_j\widehat{\beta}_j \right) = \sqrt{\widehat{\sigma}^2 a (X^TX)^{-1} a^T}.$$

   \item Don't worry too much about implementation -- \RR will do this for you in general, \href{http://stats191.stanford.edu/multiple.html}{R code}
   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Inference for $\sum_{j=0}^p a_j \beta_j$}

   \begin{block}
   {Prediction interval}
   \begin{itemize}

   \item ``Identical'' to simple linear regression.

   \item Prediction interval at $X_{1,new}, \dots, X_{p,new}$:
   $$
   \begin{aligned}
   \lefteqn{\widehat{\beta}_0 + \sum_{j=1}^p X_{j,new} \widehat{\beta}_j\pm t_{1-\alpha/2, n-p-1}} \\
   & \qquad \qquad  \times \ \sqrt{\widehat{\sigma}^2 + SE\left(\widehat{\beta}_0 + \sum_{j=1}^p X_{j,new}\widehat{\beta}_j\right)^2}.
   \end{aligned}
   $$

   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Inference for multiple regression}

   \begin{block}
   {Questions about many (combinations) of $\beta_j$'s}
   \begin{itemize}

   \item In multiple regression we can ask more complicated questions
   than in simple regression.


   \item For instance, we could ask whether
   \begin{itemize}

   \item $X_2:$ Do they allow  special priveleges

   \item $X_3:$ Give opportunity to learn new things
   \end{itemize}
   explains little of the variability in the data, and might be dropped
   from the regression model.
   \item These questions can be answered answered by $F$-statistics.
   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Inference for more than one $\beta$}

   \begin{block}
   {Dropping one or more variables}
   \begin{itemize}

   \item       Suppose we wanted to test whether how the supervisor
   handles special priveleges, or allows employees to try new things
   explains a significant amount of the variability in the overall job rating. Formally, this is:
   $$
   H_0:\beta_{2}=\beta_3=0, \  \text{ vs.} \  H_a: \text{one of $\beta_2, \beta_3 \neq 0$}$$

   \item This test is again an $F$-test based on two models
   $$
   \begin{aligned}
   R: Y_i &= \beta_0 + \beta_1 X_{i1} + \beta_4 X_{i4} + \beta_5 X_{i5} + \beta_6 X_{i6} + \varepsilon_i \\
   F: Y_i &= \beta_0 + \sum_{j=1}^6 \beta_j X_{ij} + \varepsilon_i \\
   \end{aligned}
   $$

   \item {\bf Note:} The reduced model $R$ must be a special case of the full model $F$ to use the $F$-test.
   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Geometry of Least Squares: Full Model}

   \resizebox{!}{3in}{\includegraphics{./figs/simple/axes_general1_full.png}}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Geometry of Least Squares: Reduced Model}

   \resizebox{!}{3in}{\includegraphics{./figs/simple/axes_general1_reduced.png}}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Geometry of Least Squares: Both Models}

   \resizebox{!}{3in}{\includegraphics{./figs/simple/axes_general1.png}}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Inference for more than one $\beta$}

   \begin{block}
   {$SSE$ of a model}
   \begin{itemize}[<+->]
   \item In the graphic, a ``model'', ${\cal M}$ is a subspace of $\mathbb{R}^n$ = column space of $\pmb{X}$.


   \item Least squares fit = projection onto the subspace of ${\cal M}$,
   yielding predicted values $\widehat{Y}_{{\cal M}}$

   \item Error sum of squares:
   $$
   SSE({\cal M}) = \|Y - \widehat{Y}_{{\cal M}}\|^2.
   $$
   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Inference for more than one $\beta$}

   \begin{block}
   {$F$-statistic for $H_0:\beta_2=\beta_3=0$}
   \begin{itemize}
   \item
   \begin{equation}
   \begin{aligned}
   F &=\frac{\frac{SSE(R) - SSE(F)}{2}}{\frac{SSE(F)}{n-1-p}} \\
   & \sim F_{2, n-p-1}       \qquad   (\text{if $H_0$ is true})
   \end{aligned}
   \end{equation}
   \item Reject $H_0$ at level $\alpha$ if $F > F_{1-\alpha, 2, n-1-p}$.

   \item Here is an example \href{http://stats191.stanford.edu/multiple.html}{R code}.
   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Inference for more than one $\beta$}

   \begin{block}
   {Dropping an arbitrary subset}
   \begin{itemize}
   \item For an arbitrary model, suppose we want to test
   $$    \begin{aligned}
   H_0:&\beta_{i_1}=\dots=\beta_{i_j}=0 \\
   H_a:& \text{one of $\beta_{i_1}, \dots \beta_{i_j} \neq 0$}
   \end{aligned}
   $$
   for some subset $\{i_1, \dots, i_j\} \subset \{0, \dots, p\}$.

   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Inference for more than one $\beta$}

   \begin{block}
   {$F$-statistic for $H_0:\beta_{i_1}=\dots=\beta_{i_j}=0$}
   \begin{itemize}
   \item You guessed it: it is based on the two models:
   $$
   \begin{aligned}
   R: Y_i &= \sum_{l=0, l \not \in \{i_1, \dots, i_j\}}^p \beta_j X_{il} + \varepsilon_i \\
   F: Y_i &=  \sum_{j=0}^p \beta_j X_{il} + \varepsilon_i \\
   \end{aligned}
   $$
   where $X_{i0}=1$ for all $i$.
   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Inference for more than one $\beta$}

   \begin{block}
   {$F$-statistic for $H_0:\beta_{i_1}=\dots=\beta_{i_j}=0$}
   \begin{itemize}
   \item  $$
   \begin{aligned}
   F &=\frac{\frac{SSE(R) - SSE(F)}{j}}{\frac{SSE(F)}{n-p-1}} \\
   & \sim F_{j, n-p-1}     \qquad    (\text{if $H_0$ is true})
   \end{aligned}
   $$
   \item Reject $H_0$ at level $\alpha$ if $F > F_{1-\alpha, j, n-1-p}$.

   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Geometry of Least Squares: Full Model}

   \resizebox{!}{3in}{\includegraphics{./figs/simple/axes_general_full.png}}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Geometry of Least Squares: Reduced Model}

   \resizebox{!}{3in}{\includegraphics{./figs/simple/axes_general_reduced.png}}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Geometry of Least Squares: Both Models}

   \resizebox{!}{3in}{\includegraphics{./figs/simple/axes_general.png}}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Inference for more than one $\beta$}

   \begin{block}
   { General   $F$-tests}
   \begin{itemize}
   \item Given two models $R \subset F$ (i.e. $R$ is a subspace of $F$),
   we can consider testing
   $$
   H_0: \text{$R$ is adequate (i.e. $\mathbb{E}(Y) \in R$)}$$
   vs.
   $$
   H_a: \text{$F$ is adequate (i.e. $\mathbb{E}(Y) \in F$)}.$$

   \item The test statistic is
   $$
   F = \frac{(SSE(R) - SSE(F)) / (df_R - df_F)}{SSE(F)/df_F}$$

   \item If $H_0$ is true, $F \sim F_{df_R-df_F, df_F}$ so we reject
   $H_0$ at level $\alpha$ if  $F > F_{df_R-df_F, df_F, 1-\alpha}$.
   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Constraints}

   \begin{block}
   {Constraining $\beta_1=\beta_3$ (after deciding $\beta_2=\beta_4=\beta_5=\beta_6=0$)}
   \begin{itemize}
   \item Full model:
   $$
   Y_i = \beta_0 + \beta_1 X_{i1}  + \beta_3 X_{i3} + \varepsilon_i
   $$

   \item Reduced model:
   $$
   \begin{aligned}
   Y_i &= \beta_0 + \tilde{\beta}_1 X_{i1}  + \tilde{\beta}_1 X_{i3} + \varepsilon_i \\
   &= \beta_0 + \tilde{\beta}_1 (X_{i1}  + X_{i3}) + \varepsilon_i \\
   \end{aligned}
   $$
   \end{itemize}
   \href{http://stats191.stanford.edu/multiple.html}{R code}.

   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Constraints}

   \begin{block}
   {Constraining $\beta_1+\beta_3=1$ (after deciding $\beta_2=\beta_4=\beta_5=\beta_6=0$)}
   \begin{itemize}
   \item Full model:
   $$
   Y_i = \beta_0 + \beta_1 X_{i1}  + \beta_3 X_{i3} + \varepsilon_i
   $$

   \item Reduced model:
   $$
   \begin{aligned}
   Y_i &= \beta_0 + \tilde{\beta}_1 X_{i1}  + (1 - \tilde{\beta}_1) X_{i3} + \varepsilon_i \\
   Y_i - X_{i3} &= \beta_0 + \tilde{\beta}_1 (X_{i1}  - X_{i3}) + \varepsilon_i \\
   \end{aligned}
   $$
   \end{itemize}
   \href{http://stats191.stanford.edu/multiple.html}{R code}.

   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Back to interpretation of $\beta_j$'s}

   \begin{block}
   {Supervisor example}
   \begin{itemize}

   \item Earlier, we said that $\beta_1$ is the effect
   for  ``Handles complaints'' controlling for the other variables.

   \item Why? Let's look at the $T$ for testing $H_0:\beta_1=0$:
   $$
   T = \frac{\widehat{\beta}_1}{SE(\widehat{\beta}_1)}
   $$
   \item Under $H_0:\beta_1=0$, $T^2 \sim F_{1, n-p-1}$ and
   $F$ tests are ``always'' a comparison of two models.

   \item The full model has all variables while the reduced model
   has $\beta_1=0$. Hence, both model include or {\em control for}
   the other variables.

   \end{itemize}
   \end{block}
   \end{frame}

   \part{Diagnostics \& Influence  (Ch. 4, RABE)}
   \frame{\partpage}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Diagnostics in multiple linear model}

   \begin{block}
   {Outline}
   \begin{itemize}

   \item Diagnostics -- again
   \item Different residuals


   \item Influence

   \item Outlier detection

   \item Residual plots:
   \begin{itemize}
   \item partial regression (added variable) plot,

   \item partial residual (residual plus component) plot
   \end{itemize}
   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Scottish hill races data}

   \begin{block}
   {Description}

   \begin{tabular}{c|l}
   Variable & Description \\ \hline
   {\tt Time} & Record time to complete course \\
   {\tt Distance} & Distance in the course \\
   {\tt Climb} & Vertical climb in the course \\
   \end{tabular}
   \end{block}
   \end{frame}

   %CODE
       % library(car)
   % 
   % url = 'http://stats191.stanford.edu/data/scottish_races.table'
   % races.table = read.table(url, header=T)
   % attach(races.table)
   % plot(races.table[,3:5], pch=23, bg='orange', cex=2)
   % races.lm = lm(Time ~ Distance + Climb)


   \begin{frame}
   \frametitle{Scottish hill races data}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/653bdcdd78.png}}    
   \end{center}
   \href{http://stats191.stanford.edu/diagnostics.html#scottish-hill-races}{R code}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Diagnostics}

   \begin{block}
   {What can go wrong?}

   \begin{itemize}[<+->]

   \item
   Regression function can be wrong: maybe regression function should be quadratic
   (see \Rhref{simple_diagnostics/anscombeout.html}{R code}).

   \item Model for the errors
   may be incorrect:
   \begin{itemize}
   \item  may not be normally distributed.
   \item  may not be independent.

   \item  may not have the same variance.
   \end{itemize}

   \item Detecting problems is more {\em art} then {\em science}, i.e.
   we cannot {\em test} for all possible problems in a regression model.

   \item Basic idea of diagnostic measures: if model is correct then
   residuals $e_i = Y_i -\widehat{Y}_i, 1 \leq i \leq n$ should look like a sample of
   (not quite independent) $N(0, \sigma^2)$ random variables.
   \end{itemize}
   \end{block}
   \end{frame}

   %CODE
       % par(mfrow=c(2,2))
   % plot(races.lm, pch=23 ,bg='orange',cex=2)


   \begin{frame}
   \frametitle{Standard diagnostic plots}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/c158b29344.png}}    
   \end{center}
   \href{http://stats191.stanford.edu/diagnostics.html#standard-diagnostic-plots}{R code}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Problems with the errors}

   \begin{block}
   {Possible problems \& diagnostic checks}
   \begin{itemize}[<+->]

   \item
   Errors may not be normally distributed or may not have  the same
   variance -- {\tt qqnorm} can help with this. This may not
   be too important in large samples.

   \item Variance may not be constant. Can also be addressed in a plot of
   $\pmb{X}$ vs. $\pmb{e}$: {\em fan shape} or other trend indicate
   non-constant variance.


   \item Influential observations. Which points ``affect'' the regression line the most?

   \item Outliers: points where the model really does not fit! Possibly mistakes in data transcription, lab errors, who knows? Should be recognized and (hopefully) explained.

   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Residuals}

   \begin{block}
   {Types of residuals}
   \begin{itemize}[<+->]
   \item Ordinary residuals: $e_i = Y_i - \widehat{Y}_i$. These measure the deviation of predicted value from observed value, but their distribution depends on unknown scale, $\sigma$.
   \item Internally studentized residuals ({\tt rstandard} in {\tt R}): $r_i = e_i / s(e_i) = e_i / \widehat{\sigma} \sqrt{1 - H_{ii}}$, $H$ is the ``hat'' matrix. These are almost $t$-distributed, except $\widehat{\sigma}$ depends on $e_i$.
   \item Externally studentized residuals ({\tt rstudent} in {\tt R}): $t_i = e_i / \widehat{\sigma_{(i)}} \sqrt{1 - H_{ii}} \sim t_{n-p-2}.$ These are
   exactly $t$ distributed so we know their distribution and can use them for tests, if desired.
   \end{itemize}
   \end{block}
   \end{frame}

   %CODE
       % par(mfrow=c(2,2))
   % plot(races.lm, pch=23 ,bg='orange',cex=2)


   \begin{frame}
   \frametitle{Standard diagnostic plots}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/c158b29344.png}}    
   \end{center}
   \href{http://stats191.stanford.edu/diagnostics.html#standard-diagnostic-plots}{R code}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Influence of an observation}

   \begin{block}
   {Dropping an observation}
   \begin{itemize}[<+->]

   \item In this setting, a $\cdot_{(i)}$ indicates $i$-th observation was not used in fitting the model.

   \item For example: $\widehat{Y}_{j(i)}$ is the regression
   function evaluated at the $j$-th observations  predictors BUT
   the coefficients $(\widehat{\beta}_{0(i)}, \dots, \widehat{\beta}_{p(i)})$
   were fit after deleting $i$-th row of data.

   \item Idea: if $\widehat{Y}_{j(i)}$ is very different than $\widehat{Y}_j$ (using all the data) then $i$ is an influential point, at least for estimating the regression function at $(X_{1,j}, \dots, X_{p,j})$.
   \end{itemize}

   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Influence of an observation}

   \begin{block}
   {$DFFITS$}
   \begin{itemize}[<+->]

   \item $$
   DFFITS_i = \frac{\widehat{Y}_i - \widehat{Y}_{i(i)}}{\widehat{\sigma}_{(i)} \sqrt{H_{ii}}}$$

   \item This quantity measures how much the regression function changes at the
   $i$-th case / observation when the $i$-th case / observation is deleted.

   \item For small/medium datasets: value of 1 or greater is ``suspicious''. For large dataset: value of $2 \sqrt{(p+1)/n}$.
   \end{itemize}

   \end{block}
   \end{frame}

   %CODE
       % plot(dffits(races.lm), pch=23, bg='orange',
   % cex=2, ylab="DFFITS")


   \begin{frame}
   \frametitle{Influence of an observation: DFFITS}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/7f072c5ce5.png}}    
   \end{center}
   \href{http://stats191.stanford.edu/diagnostics.html#dffits}{R code}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Influence of an observation}

   \begin{block}
   {Cook's distance}
   \begin{itemize}[<+->]

   \item $$
   D_i = \frac{\sum_{j=1}^n(\widehat{Y}_j - \widehat{Y}_{j(i)})^2}{(p+1) \, \widehat{\sigma}^2}$$

   \item This quantity measures how much the entire regression function changes when the $i$-th case is deleted.

   \item Should be comparable to $F_{p+1,n-p-1}$: if the ``$p$-value'' of $D_i$ is 50 percent or more, then the $i$-th case is likely influential: investigate further.
   \end{itemize}

   \end{block}
   \end{frame}

   %CODE
       % plot(cooks.distance(races.lm), pch=23,
   % bg='orange', cex=2, ylab="Cook's distance")


   \begin{frame}
   \frametitle{Influence of an observation: Cook's distance}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/d90b17541a.png}}    
   \end{center}
   \href{http://stats191.stanford.edu/diagnostics.html#cook-s-distance}{R code}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Influence of an observation}

   \begin{block}
   {$DFBETAS$}
   \begin{itemize}[<+->]

   \item $$
   DFBETAS_{j(i)} = \frac{\widehat{\beta}_j - \widehat{\beta}_{j(i)}}{\sqrt{\widehat{\sigma}^2_{(i)} (X^TX)^{-1}_{jj}}}.$$

   \item This quantity measures how much the coefficients change when the $i$-th case is deleted.

   \item For small/medium datasets: absolute value of 1 or greater is ``suspicious''. For large dataset: absolute value of $2 /  \sqrt{n}$.

   \end{itemize}

   \end{block}
   \end{frame}

   %CODE
       % plot(dfbetas(races.lm)[,'Climb'], pch=23,
   % bg='orange', cex=2, ylab="DFBETA (Climb)")


   \begin{frame}
   \frametitle{Influence of an observation: DFBETA, Climb}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/22337f9a00.png}}    
   \end{center}
   \href{http://stats191.stanford.edu/diagnostics.html#dfbetas}{R code}
   \end{frame}

   %CODE
       % plot(dfbetas(races.lm)[,'Distance'], pch=23,
   % bg='orange', cex=2, ylab="DFBETA (Distance)")


   \begin{frame}
   \frametitle{Influence of an observation: DFBETA, Distance}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/aaf46be76f.png}}    
   \end{center}
   \href{http://stats191.stanford.edu/diagnostics.html#dfbetas}{R code}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Outliers}

   \begin{block}
   {Basic definition}
   \begin{itemize}[<+->]
   \item {\em Outlier:} an observation pair $(Y, X_1, \dots, X_p)$ that
   does not follow the model, while most other observations seem to follow the model.
   \item Outlier in predictors: the $X$ values of the observation may lie
   outside the ``cloud'' of other $X$ values. This means you may be extrapolating your model inappropriately. The values $H_{ii}$ can be used to measure
   how ``outlying'' the $X$ values are.

   \item Outlier in response: the $Y$ value of the observation may lie very far from the fitted model. If the studentized residuals are large: observation may be an outlier.
   \end{itemize}
   \end{block}
   \end{frame}

   %CODE
       % plot(hatvalues(races.lm), pch=23,
   % bg='orange', cex=2, ylab='Hat values')


   \begin{frame}
   \frametitle{Outlying $X$ values}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/e1c1e810fa.png}}    
   \end{center}
   \href{http://stats191.stanford.edu/diagnostics.html#hat-values}{R code}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Outliers}

   \begin{block}
   {Crude (response) outlier detection test}
   \begin{itemize}[<+->]


   \item Strategy to detect outliers: ``flag'' large residuals.

   \item Problem: if $n$ is large, if we ``threshold'' at $t_{1-\alpha/2, n-p-2}$ we will get many outliers by chance even if model is correct. In fact, we expect to see $n \cdot \alpha$ ``outliers'' by this test. Every large data set would have outliers in it, even if model was entirely correct!

   \item Problem is known as {\em multiple comparisons} or {\em simultaneous inference.} We are performing $n$ hypothesis tests, but would still like
   to control the probability of making {\em any} false positive errors.
   \item One solution: Bonferroni correction, threshold at
   $t_{1 - \alpha/(2*n), n-p-2}$.
   \end{itemize}

   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Multiple comparisons}

   \begin{block}
   {      Bonferroni correction}
   \begin{itemize}[<+->]

   \item If we are doing many $t$ (or other) tests, say $m >>1$ we can control
   overall false positive rate at $\alpha$ by testing each one at level $\alpha/m$.

   \item Proof, when the model is correct, with studentized residuals $T_i$:
   $$
   \begin{aligned}
   \lefteqn{  P\left( \text{at least one false positive} \right)} \\
   & \qquad = P \left(\cup_{i=1}^m |T_i| \geq t_{1 - \alpha/(2*m), n-p-2} \right) \\
   & \qquad \leq \sum_{i=1}^m P \left( |T_i| \geq t_{1 - \alpha/(2*m), n-p-2} \right) \\
   & = \sum_{i=1}^m  \frac{\alpha}{m} = \alpha.
   \end{aligned}
   $$
   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Diagnostic plots}

   \begin{block}
   {Problems in the regression function}
   \begin{itemize}[<+->]

   \item True regression function may have higher-order non-linear terms, polynomial or otherwise.

   \item We may be missing terms involving more than one $\pmb{X}_{(\cdot)}$, i.e.
   $\pmb{X}_i \cdot \pmb{X}_j$ (called an {\em interaction}).

   \item Some simple plots: {\em added-variable} and {\em component plus residual} plots can help to find nonlinear functions of {\em one variable}.
   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Diagnostic plots}

   \begin{block}
   {Added variable plots}
   \begin{itemize}[<+->]
   \item Useful for finding influential points, outliers.


   \item Procedure:
   \begin{itemize}
   \item let $\tilde{e}_{X_j,i}, 1\leq i \leq n$ be the residuals after regressing $\pmb{X}_j$ onto all columns of $\pmb{X}$ except $\pmb{X}_j$;
   \item let $e_{X_j,i}$ be the residuals after regressing $\pmb{Y}$ onto all columns of $\pmb{X}$ except $\pmb{X}_j$;

   \item Plot $\tilde{e}_{X_j}$ against $e_{X_j}$.
   \end{itemize}

   \end{itemize}
   \end{block}
   \end{frame}

   %CODE
       % av.plots(races.lm, 'Distance')


   \begin{frame}
   \frametitle{Added variable: {\tt Distance}}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/1f52e9a470.png}}    
   \end{center}
   \href{http://stats191.stanford.edu/diagnostics.html#added-variable-plots}{R code}
   \end{frame}

   %CODE
       % av.plots(races.lm, 'Climb')


   \begin{frame}
   \frametitle{Added variable: {\tt Climb}}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/4155e1c682.png}}    
   \end{center}
   \href{http://stats191.stanford.edu/diagnostics.html#added-variable-plots}{R code}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Diagnostic plots}

   \begin{block}
   {Component + residual plots}
   \begin{itemize}
   \item Can help to determine non-linear trend in data.


   \item Procedure: plot $X_{ij}, 1 \leq i \leq n$ vs. $e_i + \widehat{\beta}_j \cdot X_{ij} , 1 \leq i \leq n$.

   \end{itemize}
   \end{block}
   \end{frame}

   %CODE
       % cr.plots(races.lm, 'Distance')


   \begin{frame}
   \frametitle{Component + residual: {\tt Distance}}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/56dda9b3a1.png}}    
   \end{center}
   \href{http://stats191.stanford.edu/diagnostics.html#component-residual-plots}{R code}
   \end{frame}

   %CODE
       % cr.plots(races.lm, 'Climb')


   \begin{frame}
   \frametitle{Component + residual: {\tt Climb}}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/97da419d62.png}}    
   \end{center}
   \href{http://stats191.stanford.edu/diagnostics.html#component-residual-plots}{R code}
   \end{frame}

   \part{Qualitative Variables, Interactions \& ANOVA  (Ch. 5, RABE)}
   \frame{\partpage}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Qualitative variables + interactions}

   \begin{block}
   {Outline}
   \begin{itemize}

   \item Qualitative / categorical variables.

   \item Regression equations differing by group.

   \item Interactions.

   \item Analysis of Variance Models

   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Categorical variables}

   \begin{block}
   {Categorical variables}
   \begin{itemize}

   \item       Most variables we have looked at so far
   were continuous: height, rating, etc.

   \item In many situations, we record a categorical variable:
   sex, state, country, etc.

   \item How do we include this in our model?
   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Categorical variables}

   \begin{block}
   {A simple example}
   \begin{itemize}

   \item       One example that we have looked at
   {\em does} have categorical variables.

   \item Two sample problem with equal variances:
   suppose
   $$
   Y = (Z_1, \dots, Z_m, W_1, \dots, W_n)$$
   with $Z_j \sim N(\mu_1, \sigma^2), 1 \leq j \leq m$ and $W_j \sim N(\mu_2, \sigma^2), 1 \leq j \leq n + m$.

   \item For  $1 \leq i \leq n$, let
   $$
   X_i =
   \begin{cases}
   1 & 1 \leq i \leq m \\
   0 & \text{otherwise.}
   \end{cases}
   $$
   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Categorical variables}

   \begin{block}
   {A simple example}
   \begin{itemize}

   \item Design matrix
   $$
   X_{(n+m) \times 2} =
   \begin{pmatrix}
   1 & 1 \\
   \vdots & \vdots \\
   1 & 1 \\
   1 & 0 \\
   \vdots & \vdots \\
   1 & 0
   \end{pmatrix}
   $$
   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Example}

   \begin{block}
   {IT salary data}
   \begin{itemize}

   \item Outcome: S, salaries for IT staff in a corporation.

   \item Predictors: X, experience (years);  E, education (3 levels): 1=Bachelor's, 2=Master's, 3=Ph.D; M, management (2 levels): 1=management, 0=not management.

   \end{itemize}
   \end{block}
   \end{frame}

   %CODE
       % url = 'http://stats191.stanford.edu/data/salary.table'
   % 
   % salary.table <- read.table(url, header=T)
   % salary.table$E <- factor(salary.table$E)
   % salary.table$M <- factor(salary.table$M)
   % attach(salary.table)
   % 
   % plot(X,S, type='n', xlab='Experience', ylab='Salary')
   % colors <- c('red', 'green', 'blue')
   % symbols <- c(23,24)
   % for (i in 1:3) {
   %   for (j in 0:1) {
   %     subset <- as.logical((E == i) * (M == j))
   %     points(X[subset], S[subset], pch=symbols[j+1], bg=colors[i], cex=2)
   %   }
   % }


   \begin{frame}
   \frametitle{IT salary}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/f1602de9ec.png}}    
   \end{center}
   \href{http://stats191.stanford.edu/interactions.html#salary-example}{R code}
   \end{frame}

   %CODE
       % salary.lm <- lm(S ~ E + M + X)
   % summary(salary.lm)
   % 
   % model.matrix(salary.lm)
   % 
   % model.frame(salary.lm)
   % influence.measures(salary.lm)
   % 
   % outlier.test(salary.lm)
   % 
   % r = resid(salary.lm)
   % 
   % k = 1
   % 
   % # this type gives an empty plotting window 'n' for 'none'
   % 
   % plot(X, r, xlim=c(1,6), type='n', xlab='Group', ylab='Residuals')
   % 
   % for (i in 1:3) {
   %   for (j in 0:1) {
   %     subset <- as.logical((E == i) * (M == j))
   %     points(rep(k, length(r[subset])), r[subset], pch=symbols[j+1], bg=colors[i], cex=2)
   %     k = k+1
   %   }
   % }


   \begin{frame}
   \frametitle{IT salary}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/e3a20f8449.png}}    
   \end{center}
   \href{http://stats191.stanford.edu/interactions.html#salary-example}{R code}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Two solutions}

   \begin{block}
   {Solution \#1: stratification                     }
   \begin{itemize}[<+->]

   \item One solution is to ``stratify'' data set by this categorical variable.

   \item We could break data set up into groups by education and management, and fit fit model
   $$
   S_i = \beta_0 + \beta_1 X_i + \varepsilon_i$$
   in each group.
   \item Problem: this results in smaller samples in each group: lose degrees of freedom for estimating $\sigma^2$ within each group.

   \end{itemize}

   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Two solutions}

   \begin{block}
   {Solution \#2: qualitative predictors                     }
   \begin{itemize}

   \item IF it is reasonable to assume that $\sigma^2$ is constant
   for each observation.

   \item THEN, we can incorporate all observations into 1 model.

   $$
   S_i = \beta_0 + \beta_1 X_i + \beta_2 E_{i2} + \beta_3 E_{i3} + \beta_4 M_i + \varepsilon_i
   $$
   where
   $$
   E_{i2} = \begin{cases}
   1 & \text{if  $E_i = 2$},\\
   0 & \text{otherwise.}
   \end{cases}, E_{i3} = \begin{cases}
   1 & \text{if  $E_i = 3$},\\
   0 & \text{otherwise,}
   \end{cases},
   $$
   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Categorical variables: details}

   \begin{block}
   {Things to notice }
   \begin{itemize}

   \item Although $E$ has 3 levels, we only added 2 variables to the model.
   In a sense, this is because ``intercept'' absorbs one level.

   \item If we added three variables then the columns of design matrix would be linearly dependent.


   \item Assumes $\beta_1$ -- effect of experience is the same in all groups, unlike when we fit the model separately. This may or may not be reasonable.
   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Interactions}

   \begin{block}
   {Effect of experience}
   \begin{itemize}

   \item Our model has enforced the constraint the $\beta_1$ is the same within each group.

   \item Graphically, this seems OK, but how can we ``test'' this?

   \item We could fit a model with different slopes in each group, but keeping as many d.f. as we can.

   \item This model has ``interactions'' in it: the effect of experience depends on what level of education you have.
   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Interactions}

   \begin{block}
   {Interaction between experience and education}
   \begin{itemize}[<+->]

   \item Model:
   $$
   \begin{aligned}
   \lefteqn{S_i = \beta_0 + \beta_1 X_i + \beta_2 E_{i2} + \beta_3 E_{i3} + \beta_4 M_i} \\
   & \qquad  + \beta_5 E_{i2} X_i + \beta_6 E_{i3} X_i + \varepsilon_i.
   \end{aligned}
   $$
   \item Note that we took each column corresponding to education and multiplied it by the column for experience to get two new predictors.
   \item To test whether the slope is the same in each group we would just test
   $H_0:\beta_5 = \beta_6=0$.

   \item Based on figure, we expect not to reject $H_0$.
   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Interactions}

   \begin{block}
   {Interaction between management and education}
   \begin{itemize}[<+->]

   \item Based on figure, we expect an interaction effect.

   \item Fit model
   $$
   \begin{aligned}
   \lefteqn{S_i = \beta_0 + \beta_1 X_i + \beta_2 E_{i2} + \beta_3 E_{i3} + \beta_4 M_i} \\
   & \qquad  + \beta_5 E_{i2} M_i + \beta_6 E_{i3} M_i + \varepsilon_i.
   \end{aligned}
   $$

   \item Again, testing for interaction is testing $H_0:\beta_5=\beta_6=0.$
   \end{itemize}
   \end{block}
   \end{frame}

   %CODE
       % subs33 = c(1:length(S))[-33]
   % 
   % salary.lm33 = lm(S ~ E + X + M, subset=subs33)
   % interactionX.lm33 = lm(S ~ E * X + M, subset=subs33)
   % interactionM.lm33 = lm(S ~ X + E * M, subset=subs33)
   % 
   % summary(salary.lm33)
   % summary(interactionX.lm33)
   % summary(interactionM.lm33)
   % 
   % anova(salary.lm33,interactionX.lm33)
   % anova(salary.lm33,interactionM.lm33)
   % 
   % 
   % r = rstandard(interactionM.lm33)
   % plot(X[subs33], r, type='n')
   % for (i in 1:3) {
   %   for (j in 0:1) {
   %     subset <- as.logical((E == i) * (M == j))
   %     points(X[subset], r[subset], pch=symbols[j+1], bg=colors[i], cex=2)
   %   }
   % }


   \begin{frame}
   \frametitle{IT salary, outlier removed}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/f6b8520b26.png}}    
   \end{center}
   \href{http://stats191.stanford.edu/interactions.html#salary-example}{R code}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Example}

   \begin{block}
   { Minority employment data                   }
   \begin{tabular}{cl}
   $TEST$ & job aptitude test score \\
   $ETHN$ & 1 if minority, 0 otherwise \\
   $JPERF$ & job performance evaluation
   \end{tabular}
   \end{block}
   \end{frame}

   %CODE
       % url = 'http://stats191.stanford.edu/data/minority.table'
   % 
   % minority.table <- read.table(url, header=T)
   % minority.table$ETHN <- factor(minority.table$ETHN)
   % attach(minority.table)
   % 
   % plot(TEST, JPERF, type='n')
   % points(TEST[(ETHN == 0)], JPERF[(ETHN == 0)], pch=21, cex=2, bg='purple')
   % points(TEST[(ETHN == 1)], JPERF[(ETHN == 1)], pch=25, cex=2, bg='green')


   \begin{frame}
   \frametitle{Minority employment data}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/f89be49cde.png}}    
   \end{center}
   \href{http://stats191.stanford.edu/interactions.html#minority-employment-data}{R code}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Interactions}

   \begin{block}
   {General model}

   \begin{itemize}

   \item In theory, there may be a linear relationship
   between $JPERF$ and $TEST$ but it could be different by group.

   \item Model:
   {\small
   $$
   JPERF_i = \beta_0 + \beta_1 TEST_i + \beta_2 ETHN_i + \beta_3 ETHN_i * TEST_i + \varepsilon_i.
   $$
   }
   \item Regression functions:
   {\small
   $$
   Y_i =
   \begin{cases}
   \beta_0 + \beta_1 TEST_i + \varepsilon_i & \text{if $ETHN_i$=0} \\
   (\beta_0 + \beta_2) + (\beta_1 + \beta_3) TEST_i + \varepsilon_i & \text{if $ETHN_i=1$.} \\
   \end{cases}
   $$
   }
   \end{itemize}
   \end{block}
   \end{frame}

   %CODE
       % url = 'http://stats191.stanford.edu/data/minority.table'
   % 
   % minority.table <- read.table(url, header=T)
   % minority.table$ETHN <- factor(minority.table$ETHN)
   % attach(minority.table)
   % 
   % plot(TEST, JPERF, type='n')
   % points(TEST[(ETHN == 0)], JPERF[(ETHN == 0)], pch=21, cex=2, bg='purple')
   % points(TEST[(ETHN == 1)], JPERF[(ETHN == 1)], pch=25, cex=2, bg='green')
   % 
   % 
   % 
   % 
   % minority.lm1 <- lm(JPERF ~ TEST)
   % summary(minority.lm1)
   % 
   % plot(TEST, JPERF, type='n')
   % points(TEST[(ETHN == 0)], JPERF[(ETHN == 0)], pch=21, cex=2, bg='purple')
   % points(TEST[(ETHN == 1)], JPERF[(ETHN == 1)], pch=25, cex=2, bg='green')
   % abline(minority.lm1$coef, lwd=3, col='blue')


   \begin{frame}
   \frametitle{No difference}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/37c064b804.png}}    
   \end{center}
   \href{http://stats191.stanford.edu/interactions.html#minority-employment-data}{R code}
   \end{frame}

   %CODE
       % minority.lm2 = lm(JPERF ~ TEST + TEST:ETHN)
   % summary(minority.lm2)
   % 
   % plot(TEST, JPERF, type='n')
   % points(TEST[(ETHN == 0)], JPERF[(ETHN == 0)], pch=21, cex=2, bg='purple')
   % points(TEST[(ETHN == 1)], JPERF[(ETHN == 1)], pch=25, cex=2, bg='green')
   % 
   % abline(minority.lm2$coef['(Intercept)'], minority.lm2$coef['TEST'], lwd=3, col='purple')
   % abline(minority.lm2$coef['(Intercept)'], minority.lm2$coef['TEST'] + minority.lm2$coef['TEST:ETHN1'], lwd=3, col='green')


   \begin{frame}
   \frametitle{Different slopes, same intercept}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/628321a694.png}}    
   \end{center}
   \href{http://stats191.stanford.edu/interactions.html#minority-employment-data}{R code}
   \end{frame}

   %CODE
       % minority.lm3 = lm(JPERF ~ TEST + ETHN)
   % summary(minority.lm3)
   % 
   % plot(TEST, JPERF, type='n')
   % points(TEST[(ETHN == 0)], JPERF[(ETHN == 0)], pch=21, cex=2, bg='purple')
   % points(TEST[(ETHN == 1)], JPERF[(ETHN == 1)], pch=25, cex=2, bg='green')
   % abline(minority.lm3$coef['(Intercept)'], minority.lm3$coef['TEST'], lwd=3, col='purple')
   % abline(minority.lm3$coef['(Intercept)'] + minority.lm3$coef['ETHN1'], minority.lm3$coef['TEST'], lwd=3, col='green')


   \begin{frame}
   \frametitle{Different intercepts, same slope}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/78aad55d8a.png}}    
   \end{center}
   \href{http://stats191.stanford.edu/interactions.html#minority-employment-data}{R code}
   \end{frame}

   %CODE
       % minority.lm4 = lm(JPERF ~ TEST * ETHN)
   % summary(minority.lm4)
   % 
   % plot(TEST, JPERF, type='n')
   % points(TEST[(ETHN == 0)], JPERF[(ETHN == 0)], pch=21, cex=2, bg='purple')
   % points(TEST[(ETHN == 1)], JPERF[(ETHN == 1)], pch=25, cex=2, bg='green')
   % abline(minority.lm4$coef['(Intercept)'], minority.lm4$coef['TEST'], lwd=3, col='purple')
   % abline(minority.lm4$coef['(Intercept)'] + minority.lm4$coef['ETHN1'],
   %        minority.lm4$coef['TEST'] + minority.lm4$coef['TEST:ETHN1'], lwd=3, col='green')


   \begin{frame}
   \frametitle{Different intercepts, different slopes}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/645008123b.png}}    
   \end{center}
   \href{http://stats191.stanford.edu/interactions.html#minority-employment-data}{R code}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Interactions}

   \begin{block}
   {Interpreting different models                     }
   \begin{itemize}[<+->]

   \item Both  $\beta_2, \beta_3 \neq 0$ -- main effect for $ETHN$ and interaction effect
   between $TEST$ and $ETHN$.
   \item $\beta_2 \neq 0, \beta_3 = 0$ -- main effect for $ETHN$, no interaction between $TEST$ and $ETHN$.
   \item $\beta_2=0, \beta_3 \neq 0$ -- no main effect for $ETHN$,  interaction between $TEST$ and $ETHN$.


   \item \href{http://stats191.stanford.edu/interactions.html#salary-example}{R code}
   \end{itemize}

   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{ANOVA models}

   \begin{block}
   {General definition of ANOVA model}
   \begin{itemize}

   \item Models with only qualitative variables.



   \item Can be thought of as extensions of ``two-sample'' $t$-test to more than two groups at once, and more than one grouping variable.

   \item Example: in a simple experiment studying blood pressure we might start by considering only the overall health (Poor, Moderate, Good).

   \item Data would then have one categorical variable with three levels.
   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{ANOVA models}

   \begin{block}
   {Example: rehab surgery}

   \begin{itemize}[<+->]

   \item How does prior fitness affect recovery from surgery? Observations: 24 subjects' recovery time.

   \item Three fitness levels: below average, average, above average.

   \item If you are in better shape before surgery, does it take less time to recover?
   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{ANOVA models}

   \begin{block}
   {One-way ANOVA}
   \begin{itemize}

   \item First generalization of two sample $t$-test: more than one level.
   \item One-way ANOVA model: observations: $Y_{ij}, 1 \leq i \leq r, 1 \leq j \leq n_i$: $r$ groups and $n_i$ samples in $i$-th group.
   $$
   Y_{ij} = \mu  + \alpha_i + \varepsilon_{ij}, \qquad \varepsilon_{ij} \sim N(0, \sigma^2).$$

   \item Constraint: $\sum_{i=1}^r \alpha_i = 0$. This constraint is needed for ``identifiability''. This is ``equivalent'' to only adding $r-1$ columns to the design matrix for this qualitative variable.
   \end{itemize}

   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{ANOVA models}

   \begin{block}
   {One-way ANOVA}
   \begin{itemize}


   \item Model is easy to fit:
   $$
   \widehat{Y}_{ij} = \frac{1}{n_i} \sum_{j=1}^{n_i} Y_{ij} = \overline{Y}_{i\cdot}.$$
   If observation is in $i$-th group: predicted mean is just the sample mean of observations in $i$-th group.
   \item Simplest question: is there any group (main) effect?
   $$
   H_0:\alpha_1 = \dots = \alpha_r= 0?$$

   \item Test is based on $F$-test with full model vs. reduced model. Reduced model just has an intercept.

   \item Other questions: is the effect the same in groups 1 and 2?
   $$H_0:\alpha_1=\alpha_2?$$
   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{ANOVA models}

   \begin{block}
   {ANOVA table: One-way}
   \begin{itemize}[<+->]

   \item
   {\tiny
   \begin{tabular}{l|ccc}
   Source  & $SS$    & $df$ &  $E(MS)$ \\ \hline
   Treatments &    $SSTR = \sum_{i=1}^r n_i \left(\overline{Y}_{i\cdot} - \overline{Y}_{\cdot\cdot}\right)^2$ &   $r-1$     & $\sigma^2 + \frac{\sum_{i=1}^r n_i \alpha_i^2}{r-1}$ \\
   Error &  $SSE = \sum_{i=1}^r \sum_{j=1}^{n_i}(Y_{ij} - \overline{Y}_{i\cdot})^2$ & $\sum_{i=1}^r n_i - r$ & $\sigma^2$ \\
   \end{tabular}}


   \item Note that $MSTR$ measures ``variability'' of the ``cell'' means. If there is a group effect we expect this to be large relative to $MSE$.
   \item We see that under $H_0:\alpha_1=\dots=\alpha_r=0$,
   the expected value of $MSTR$ and $MSE$ is $\sigma^2$. This tells us how to
   test $H_0$ using ratio of mean squares, i.e. an $F$ test.

   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{ANOVA models}

   \begin{block}
   {Testing for any main effect}
   \begin{itemize}

   \item Rows in the ANOVA table are, in general, independent.

   \item Therefore, under $H_0$
   $$
   F = \frac{MSTR}{MSE} = \frac{\frac{SSTR}{df_{TR}}}{\frac{SSE}{df_{E}}} \sim F_{df_{TR}, df_E}$$
   the degrees of freedom come from the $df$ column in previous table.
   \item Reject $H_0$ at level $\alpha$ if $F > F_{1-\alpha, df_{TR}, df_{E}}.$
   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{ANOVA models}

   \begin{block}
   {Inference for linear combinations                     }
   \begin{itemize}

   \item Suppose we want to ``infer'' something about
   $$
   \sum_{i=1}^r a_i \mu_i$$
   where $\mu_i = \mu+\alpha_i$ is the mean in the $i$-th group.
   For example:
   $$
   H_0:\mu_1-\mu_2=0 \qquad \text{(same as $H_0:\alpha_1-\alpha_2=0$)}?$$
   Is there a difference between below average and average groups in terms of rehab time?
   \item $$
   \text{Var}\left(\sum_{i=1}^r a_i \overline{Y}_{i\cdot} \right) = \sigma^2 \sum_{i=1}^r \frac{a_i^2}{n_i}.$$

   \item Usual confidence intervals, $t$-tests.

   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{ANOVA models}

   \begin{block}
   {Two categorical variables: kidney failure}

   \begin{itemize}
   \item Time of stay in hospital depends on weight gain between treatments and duration of treatment.

   \item Two levels of duration, three levels of weight gain.

   \item Is there an interaction? Main effects?

   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{ANOVA models}

   \begin{block}
   {Two-way ANOVA}

   \begin{itemize}

   \item Second generalization: more than one grouping variable.
   \item Two-way ANOVA model: observations: $(Y_{ijk}), 1 \leq i \leq r, 1 \leq j \leq m, 1 \leq k \leq n_{ij}$: $r$ groups in first grouping variable, $m$ groups in second and $n_{ij}$ samples in $(i,j)$-``cell'':
   $$
   Y_{ijk} = \mu + \alpha_i + \beta_j + (\alpha \beta)_{ij} +  \varepsilon_{ijk} , \qquad \varepsilon_{ijk} \sim N(0, \sigma^2).$$
   \item In kidney example, $r=3$ (weight gain), $m=2$ (duration of treatment), $n_{ij}=10$ for all $(i,j)$.
   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{ANOVA model}

   \begin{block}
   {Two-way ANOVA: main questions of interest}
   \begin{itemize}
   \item Are there main effects for the grouping variables?
   $$
   H_0:\alpha_1 = \dots = \alpha_r = 0, \qquad H_0: \beta_1 = \dots = \beta_m = 0.$$
   \item Are there interaction effects:
   $$
   H_0:(\alpha\beta)_{ij} = 0, 1 \leq i \leq r, 1 \leq j \leq m.$$
   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{ANOVA models}

   \begin{block}
   {Constraints on the parameters}
   \begin{itemize}

   \item Many constraints are needed, again for identifiability.
   Let's not worry about the details \dots

   \item Constraints:
   \begin{itemize}

   \item $\sum_{i=1}^r \alpha_i = 0$

   \item $\sum_{j=1}^m \beta_j = 0$

   \item $\sum_{j=1}^m (\alpha\beta)_{ij} = 0, 1 \leq i \leq r$
   \item $\sum_{i=1}^r (\alpha\beta)_{ij} = 0, 1 \leq j \leq m.$
   \end{itemize}
   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{ANOVA models}

   \begin{block}
   {Fitting model}
   \begin{itemize}

   \item Easy to fit:
   $$
   \widehat{Y}_{ijk}= \overline{Y}_{ij\cdot} = \frac{1}{n_{ij}}\sum_{k=1}^{n_{ij}} Y_{ijk}.$$
   \item Inference for combinations
   $$\text{Var} \left(\sum_{i=1}^r \sum_{j=1}^m a_{ij} \overline{Y}_{ij\cdot}\right) = \sigma^2 \cdot \sum_{i=1}^r \sum_{j=1}^m \frac{a_{ij}^2}{n_{ij}}.$$

   \item Usual $t$-tests, confidence intervals.
   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{ANOVA models}

   \begin{block}
   {ANOVA table: Two-way (assuming $n_{ij}=n$)}
   {\small
   \begin{tabular}{lc}
   Term & $SS$     \\ \hline
   $A$ &    $SSA = nm\sum_{i=1}^r  \left(\overline{Y}_{i\cdot\cdot} - \overline{Y}_{\cdot\cdot\cdot}\right)^2$ \\
   $B$ &     $SSB = nr\sum_{j=1}^m  \left(\overline{Y}_{\cdot j\cdot} - \overline{Y}_{\cdot\cdot\cdot}\right)^2$ \\
   $AB$ &    $SSAB = n\sum_{i=1}^r \sum_{j=1}^m  \left(\overline{Y}_{ij\cdot} - \overline{Y}_{i\cdot\cdot} - \overline{Y}_{\cdot j\cdot} + \overline{Y}_{\cdot\cdot\cdot}\right)^2$ \\
   Error &  $SSE = \sum_{i=1}^r \sum_{j=1}^m \sum_{k=1}^{n}(Y_{ijk} - \overline{Y}_{ij\cdot})^2$  \\
   \end{tabular}}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{ANOVA models}

   \begin{block}
   {ANOVA table: Two-way (assuming $n_{ij}=n$)}
   \begin{itemize}
   \item {\small
   \begin{tabular}{lcc}
   $SS$    & $df$ &  $E(MS)$ \\ \hline
   $SSA$ &   $r-1$     & $\sigma^2 + nm\frac{\sum_{i=1}^r \alpha_i^2}{r-1}$ \\
   $SSB$ &   $m-1$     & $\sigma^2 + nr\frac{\sum_{j=1}^m \beta_j^2}{m-1}$ \\
   $SSAB$ &   $(m-1)(r-1)$     & $\sigma^2 + n\frac{\sum_{i=1}^r\sum_{j=1}^m (\alpha\beta)_{ij}^2}{(r-1)(m-1)}$ \\
   $SSE$ & $(n-1)mr$ & $\sigma^2$ \\
   \end{tabular}}
   \item For instance, we see that under $H_0:(\alpha\beta)_{ij}=0, \forall i,j$
   the expected value of $SSAB$ and $SSE$ is $\sigma^2$ -- use these for an $F$-test testing for an interaction.
   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Fixed and random effects}

   \begin{block}
   {Random effects}
   \begin{itemize}

   \item In kidney \& rehab examples, the categorical variables are well-defined categories: below average fitness, long duration, etc.

   \item In some designs, the categorical variable is ``subject''.

   \item Simplest example: repeated measures, where more than one (identical) measurement is taken on the same individual.

   \item In this case, the ``group'' effect $\alpha_i$ is best thought of as random because we only sample a subset of the entire population.
   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Fixed and random effects}

   \begin{block}
   {When to use random effects?}
   \begin{itemize}
   \item A ``group'' effect is random if we can think of the levels we observe in that group to be samples from a larger population.

   \item Example: if collecting data from different medical centers, ``center'' might be thought of as random.

   \item Example: if surveying students on different campuses, ``campus'' may be a random effect.

   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Fixed and random effects}

   \begin{block}
   {Example: sodium content in beer}

   \begin{itemize}[<+->]

   \item How much sodium is there in North American beer? How much does this vary by brand?

   \item Observations: for 6 brands of beer, we recorded the sodium content of 8 12 ounce bottles.

   \item Questions of interest: what is the ``grand mean'' sodium content? How much variability is there from brand to brand?

   \item ``Individuals'' in this case are brands, repeated measures are the 8 bottles.
   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{One-way ANOVA (random)}

   \begin{block}
   {One-way random effects model}

   \begin{itemize}

   \item Assuming that cell-sizes are the same, i.e. equal observations for each ``subject'' (brand of beer).
   \item Observations       $$
   Y_{ij} \sim \mu_{\cdot} + \alpha_i + \varepsilon_{ij}, 1 \leq i \leq r, 1 \leq j \leq n$$

   \item $\varepsilon_{ij} \sim N(0, \sigma^2), 1 \leq i \leq r, 1 \leq j \leq n$
   \item $\alpha_i \sim N(0, \sigma^2_{\mu}), 1 \leq  i \leq r.$

   \item Parameters:
   \begin{itemize}
   \item $\mu$ is the population mean;

   \item $\sigma^2$ is the measurement variance (i.e. how variable are the readings from the machine that reads the sodium content?);
   \item $\sigma^2_{\mu}$ is the population variance (i.e. how variable is the  sodium content of beer across brands).
   \end{itemize}
   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{One-way ANOVA (random)}

   \begin{block}
   {Implications for model}
   \begin{itemize}

   \item In random effects model, the observations are no longer independent (even if $\varepsilon$'s are independent
   $$
   {\rm Cov}(Y_{ij}, Y_{i'j'}) = \sigma^2_{\mu} \delta_{i,i'} + \sigma^2 \delta_{j,j'}.$$

   \item In more complicated models, this makes ``maximum likelihood estimation'' more complicated: least squares is no longer the best solution.


   \item Also changes the degrees of freedom for some $t$-statistics.
   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{One-way ANOVA (random)}

   \begin{block}
   {Fitting the model}
   \begin{itemize}
   \item Only one parameter in the mean function $\mu_{\cdot}.$

   \item When cell sizes are the same (balanced),
   $$
   \widehat{\mu}_{\cdot} = \overline{Y}_{\cdot \cdot} = \frac{1}{nr} \sum_{i,j} Y_{ij}.$$

   \item Unbalanced models: slightly more tricky.

   \item This also changes estimates of $\sigma^2$ -- see ANOVA table below. We might guess that $df=nr-1$ and
   $$
   \widehat{\sigma}^2 = \frac{1}{nr-1} \sum_{i,j} (Y_{ij} - \overline{Y}_{\cdot\cdot})^2.$$
   This is {\em not} the case.
   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{One-way ANOVA (random)}

   \begin{block}
   {ANOVA table}

   {\tiny
   \begin{tabular}{l|ccc}
   Source  & $SS$    & $df$ &  $E(MS)$ \\ \hline
   Treatments &    $SSTR = \sum_{i=1}^r n \left(\overline{Y}_{i\cdot} - \overline{Y}_{\cdot\cdot}\right)^2$ &   $r-1$     & $\sigma^2 + n \sigma^2_{\mu}$ \\
   Error &  $SSE = \sum_{i=1}^r \sum_{j=1}^{n}(Y_{ij} - \overline{Y}_{i\cdot})^2$ & $(n-1)r$ & $\sigma^2$ \\
   \end{tabular}}
   \begin{itemize}

   \item Only change here is the expectation of $SSTR$ which reflects randomness of $\alpha_i$'s.

   \item ANOVA table is still useful to setup tests: the same $F$ statistics for fixed or random will work here.

   \item Test for random effect: $H_0:\sigma^2_{\mu}=0$ based on
   $$
   F = \frac{MSTR}{MSE} \sim F_{r-1, (n-1)r} \qquad \text{under $H_0$}.$$
   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{One-way ANOVA (random)}

   \begin{block}
   {Inference for population mean: $\mu_{\cdot}$}
   \begin{itemize}

   \item Easy to check that
   $$
   \begin{aligned}
   E(\overline{Y}_{\cdot \cdot}) &= \mu_{\cdot}   \\
   \text{Var}(\overline{Y}_{\cdot \cdot}) &= \frac{n\sigma^2_{\mu} + \sigma^2}{rn}.
   \end{aligned}
   $$

   \item To come up with a $t$ statistic that we can use for test, CIs, we
   need to find an estimate of $\text{Var}(\overline{Y}_{\cdot \cdot})$.
   ANOVA table says $E(MSTR) = n\sigma^2_{\mu}+\sigma^2.$
   \item Therefore,
   $$
   \frac{\overline{Y}_{\cdot \cdot} - \mu_{\cdot}}{\sqrt{\frac{SSTR}{(r-1)rn}}} \sim t_{r-1}$$
   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{One-way ANOVA (random)}

   \begin{block}
   {Inference for population mean: $\mu_{\cdot}$}
   \begin{itemize}[<+->]

   \item Why $r-1$ degrees of freedom?
   Imagine we could record an infinite number of observations for each individual, so that $\overline{Y}_{i\cdot} \rightarrow \mu_i$, or that $\sigma^2_{\mu}=0$.

   \item To learn anything about $\mu_{\cdot}$ we still only have $r$ observations
   $(\mu_1, \dots, \mu_r)$.

   \item Sampling more within an individual cannot narrow the CI for $\mu_{\cdot}$.

   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{One-way ANOVA (random)}

   \begin{block}
   {Estimating $\sigma^2_{\mu}$}
   \begin{itemize}

   \item From the ANOVA table
   $$
   \sigma^2_{\mu} = \frac{E(SSTR / (r-1)) - E(SSE / ((n-1)r))}{n}.$$

   \item Natural estimate:
   $$
   S^2_{\mu} = \frac{SSTR / (r-1) - SSE / ((n-1)r)}{n}
   $$
   \item Problem: this estimate can be negative! One of the difficulties
   in random effects model.
   \end{itemize}

   \end{block}
   \end{frame}

   \part{Weighted Least Squares, Transformations (Ch. 6 \& 7, RABE)}
   \frame{\partpage}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Outline}

   \begin{block}{Today's class}

   \begin{itemize}

   \item Transformations to achieve linearity.

   \item Transformations to stabilize variance.
   \item Correcting for unequal variance: weighted least squares.
   \end{itemize}

   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Transformations}

   \begin{block}
   {Transformations to achieve linearity                     }
   \begin{itemize}


   \item We have been working with {\em linear} regression models so far in the course.

   \item Many models are nonlinear, but can be {\em transformed} to a linear model.



   \end{itemize}

   \end{block}
   \end{frame}

   %CODE
       % bacteria.table <- read.table('http://stats191.stanford.edu/data/bacteria.table', header=T)
   % attach(bacteria.table)
   % 
   % plot(bacteria.table, pch=23, cex=2, bg='orange')


   \begin{frame}
   \frametitle{Bacteria death}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/19a2fee531.png}}    
   \end{center}
   \href{http://stats191.stanford.edu/transformations.html#bacteria-example}{R code}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Transformations}

   \begin{block}
   {Exponential growth model}
   \begin{itemize}
   \item Suppose the expected
   number of cells grows like
   $$
   E(n_t) = n_0 e^{\beta_1t}, \qquad t=1, 2, 3, \dots
   $$

   \item If we take logs of both sides
   $$
   \log E(n_t) = \log n_0 + \beta_1 t.$$

   \item (Reasonable ?) model:
   $$
   \log n_t = \log n_0 + \beta_1 t + \varepsilon_t, \qquad \varepsilon_t \sim N(0,\sigma^2) \ \text{independent}$$
   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Transformations}

   \begin{block}{Logarithmic transformation}
   \begin{itemize}

   \item This is slightly different than original model:
   $$
   E(\log n_t) \leq \log E(n_t)$$
   but may be approximately true.

   \item If $\varepsilon_t \sim N(0,\sigma^2)$ then
   $$
   n_t = n_0 \cdot \epsilon_t \cdot e^{\beta_1 t}.$$
   \item $\epsilon_t=e^{\varepsilon_t}$ is called a log-normal random $(0,\sigma^2)$ random variable.
   \end{itemize}
   \end{block}
   \end{frame}

   %CODE
       % bacteria.lm <- lm(N_t ~ t)
   % bacteria.log.lm <- lm(log(N_t) ~ t)
   % 
   % par(mfrow=c(1,1))
   % plot(bacteria.table, pch=23, cex=2, bg='orange')
   % lines(t, fitted(bacteria.lm), lwd=2, col='red')
   % lines(t, exp(fitted(bacteria.log.lm)), lwd=2, col='green')


   \begin{frame}
   \frametitle{Bacteria death, fitted values}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/6e2b481af2.png}}    
   \end{center}
   \href{http://stats191.stanford.edu/transformations.html#bacteria-example}{R code}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Transformations}

   \begin{block}
   {Linearizing regression function}

   Some models that can be linearized:
   \begin{itemize}

   \item $y=\alpha x^{\beta}$, use $\tilde{y}=\log(y), \tilde{x}=\log(x)$;

   \item $y=\alpha e^{\beta x}$, use $\tilde{y}=\log(y)$;

   \item $y=x/(\alpha x - \beta)$, use $\tilde{y}=1/y, \tilde{x}=1/x$.
   \item More in textbook.
   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Transformations}

   \begin{block}
   {Caveats}

   \begin{itemize}

   \item Just because expected value linearizes, doesn't
   mean that the errors behave correctly.

   \item In some cases, this can be corrected using
   weighted least squares (more later).


   \item Constant variance, normality assumptions should still be checked.
   \end{itemize}
   \end{block}
   \end{frame}

   %CODE
       % bacteria.lm <- lm(N_t ~ t)
   % par(mfrow=c(2,2))
   % plot(bacteria.lm, cex=2, pch=23, bg='orange')


   \begin{frame}
   \frametitle{Bacteria death, untransformed}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/3afa8ee1c5.png}}    
   \end{center}
   \href{http://stats191.stanford.edu/transformations.html#bacteria-example}{R code}
   \end{frame}

   %CODE
       % bacteria.log.lm <- lm(log(N_t) ~ t)
   % par(mfrow=c(2,2))
   % plot(bacteria.log.lm, cex=2, pch=23, bg='orange')


   \begin{frame}
   \frametitle{Bacteria death, transformed}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/aa5be1ec90.png}}    
   \end{center}
   \href{http://stats191.stanford.edu/transformations.html#bacteria-example}{R code}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Transformations}

   \begin{block}
   {Stabilizing variance                    }
   \begin{itemize}

   \item Sometimes, a transformation can turn non-constant variance errors
   to ``close to'' constant variance.

   \item Example: by the ``delta rule'' (see next slide), if
   $$
   Var(Y) = \sigma^2 E(Y)$$
   then
   $$
   \text{Var}(\sqrt{Y}) \simeq \frac{\sigma^2}{4}.$$
   \end{itemize}

   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Transformations}

   \begin{block}
   {Delta rule}

   \begin{itemize}


   \item Very important tool in statistics.
   \item Taylor series expansion:
   $$
   f(Y) = f(E(Y)) + \dot{f}(E(Y)) (Y - E(Y)) + \dots $$

   \item
   $$
   \text{Var}(f(Y)) \simeq \dot{f}(E(Y))^2 \text{Var}(Y)$$

   \item  Previous example:
   $$\text{Var}(\sqrt{Y}) \simeq \frac{\text{Var}(Y)}{4E(Y)}
   $$

   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Transformations}

   \begin{block}
   {Caveats                     }
   \begin{itemize}

   \item Just because a transformation makes variance
   constant doesn't mean regression function is still linear (or even that it was linear)!

   \item The models are approximations, and once a model is selected
   our standard ``diagnostics'' should be used to assess adequacy of fit.

   \item It is possible to have non-constant variance
   but have the variance stabilizing transformation may destroy linearity of the regression function. {\em Solution:} try weighted least squares (WLS).

   \end{itemize}

   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Correcting for unequal variance}

   \begin{block}
   {Example: education expenditure in 1975                     }
   \begin{itemize}

   \item Variables:
   \begin{enumerate}
   \item $Y$ -- per capita education expenditure by state

   \item $X_1$ -- per capita income in 1973 by state

   \item $X_2$ -- proportion of population under 18

   \item $X_3$ -- proportion in urban areas

   \item ${\tt Region}$ -- which region of the country are the states located in ?
   \end{enumerate}
   \item Hypothesis: weights vary by Region: i.e. variability of expenditure
   varies by rough geographic region.

   \end{itemize}

   \end{block}
   \end{frame}

   %CODE
       % # education expenditure by state and region
   % 
   % education.table <- read.table('http://stats191.stanford.edu/data/education1975.table', header=T)
   % education.table$Region <- factor(education.table$Region)
   % attach(education.table)
   % 
   % education.lm <- lm(Y ~ X1 + X2 + X3, data=education.table)
   % 
   % # Standard plots
   % 
   % par(mfrow=c(2,2))
   % plot(education.lm)


   \begin{frame}

   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/e3ba575273.png}}    
   \end{center}

   \end{frame}

   %CODE
       % par(mfrow=c(1,1))
   % boxplot(rstandard(education.lm) ~ Region, col=c('red', 'green',
   %                                             'blue', 'yellow'))


   \begin{frame}
   \frametitle{Boxplot of residuals}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/690cdb2a3e.png}}    
   \end{center}
   \href{http://stats191.stanford.edu/transformations.html#education-example}{R code}
   \end{frame}

   %CODE
       % # Remove Alaska: 1975 was a big oil year
   % 
   % keep.subset <- (STATE != 'AK')
   % 
   % education.noAK.lm <- lm(Y ~ X1 + X2 + X3, subset=keep.subset, data=education.table)
   % 
   % summary(education.noAK.lm)
   % 
   % # Plot model again
   % 
   % par(mfrow=c(2,2))
   % plot(education.noAK.lm)


   \begin{frame}
   \frametitle{Education expenditure, AK removed}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/c641c3a06d.png}}    
   \end{center}
   \href{http://stats191.stanford.edu/transformations.html#education-example}{R code}
   \end{frame}

   %CODE
       % par(mfrow=c(1,1))
   % boxplot(rstandard(education.noAK.lm) ~ Region[keep.subset], col=c('red', 'green',
   %                                          'blue', 'yellow'))


   \begin{frame}
   \frametitle{Boxplot of residuals, AK removed}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/87ba1626cf.png}}    
   \end{center}
   \href{http://stats191.stanford.edu/transformations.html#education-example}{R code}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Correcting for unequal variance}

   \begin{block}{Reweighting}
   \begin{itemize}


   \item If you have a reasonable guess of variance as a function
   of the predictors, you can use this to ``reweight'' the data.

   \item Hypothetical example
   $$
   Y_i = \beta_0 + \beta_1 X_i + \varepsilon_i, \qquad \varepsilon_i \sim N(0,\sigma^2 X_i^2).$$

   \item Setting $\tilde{Y}_i = Y_i / X_i$, $\tilde{X}_i = 1 / X_i$, model becomes
   $$
   \tilde{Y}_i = \beta_0 \tilde{X}_i + \beta_1 + \epsilon_i, \epsilon_i \sim N(0,\sigma^2).$$
   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Correcting for unequal variance}

   \begin{block}
   {Weighted Least Squares}

   \begin{itemize}

   \item Fitting this model is equivalent to minimizing
   $$
   \sum_{i=1}^n \frac{1}{X_i^2} \left(Y_i - \beta_0 - \beta_1 X_i\right)^2
   $$
   \item Weighted Least Squares
   $$
   SSE(\beta, w) = \sum_{i=1}^n w_i \left(Y_i - \beta_0 - \beta_1 X_i\right)^2, \qquad w_i = \frac{1}{X_i^2}.
   $$

   \item In general, weights should be
     like:
   $$
   w_i = \frac{1}{\text{Var}(\varepsilon_i)}.$$
   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Correcting for unequal variance}

   \begin{block}
   {Common weighting ``schemes''}
   \begin{itemize}

   \item If you have a qualitative variable, then it is easy to
   estimate weight within groups (our example today).

   \item ``Often''
   $$
   \text{Var}(\varepsilon_i) = \text{Var}(Y_i) = V(E(Y_i))$$

   \item Many non-Gaussian models behave like this: logistic, Poisson regression -- upcoming lectures.

   \end{itemize}

   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Correcting for unequal variance}

   \begin{block}
   {Two stage procedure}

   \begin{itemize}


   \item Suppose we have a hypothesis about the weights, i.e.
   they are constant within {\tt Region}, or they are something like
   $$
   w_i^{-1} = \text{Var}(\epsilon_i) =  \alpha_0 + \alpha_1 X_{i1}^2.$$

   \item We pre-whiten:
   \begin{enumerate}
   \item Fit model using OLS (Ordinary Least Squares) to get initial estimate $\widehat{\beta}_{OLS}$

   \item Use predicted values from this model to estimate $w_i$.
   \item Refit model using WLS (Weighted Least Squares).

   \item If needed, iterate previous two steps.
   \end{enumerate}
   \end{itemize}
   \end{block}
   \end{frame}

   %CODE
       % # Estimate weights by region
   % 
   % weights <- 0 * education.table$Y
   % for (region in levels(Region)) {
   %   subset.region <- (Region[keep.subset] == region)
   %   weights[subset.region] <- 1.0 / (sum(resid(education.noAK.lm)[subset.region]^2) / sum(subset.region))
   % }
   % 
   % education.noAK.weight.lm <- lm(Y ~ X1 + X2 + X3, weights=weights, subset=keep.subset, data=education.table)
   % summary(education.noAK.weight.lm)
   % 
   % # Plot again
   % 
   % par(mfrow=c(2,2))
   % plot(education.noAK.weight.lm)


   \begin{frame}
   \frametitle{Education expenditure -- weighted}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/b96a51b608.png}}    
   \end{center}
   \href{http://stats191.stanford.edu/transformations.html#education-example}{R code}
   \end{frame}

   %CODE
       % par(mfrow=c(1,1))
   % boxplot(resid(education.noAK.weight.lm, type='pearson') ~ Region[keep.subset], col=c('red',
   %                                                     'green',
   %                                                     'blue', 'yellow'))


   \begin{frame}
   \frametitle{Boxplot of residuals -- weighted}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/1206bd472b.png}}    
   \end{center}
   \href{http://stats191.stanford.edu/transformations.html#education-example}{R code}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Correcting for unequal variance}

   \begin{block}
   {Unequal variance: effects on inference}
   \begin{itemize}
   \item So far, we have just mentioned that things {\em may} have unequal variance, but not thought about how it affects inference.

   \item In general, if we ignore unequal variance, our estimates of variance
   are no longer unbiased. The covariance has the ``sandwich form''
   $$
   \text{Cov}(\widehat{\beta}_{OLS}) = (X'X)^{-1}(XW^{-1}X)(X'X)^{-1}.
   $$
   with $W=\text{diag}(1/\sigma^2_i)$.
   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Correcting for unequal variance}

   \begin{block}
   {Efficiency}
   \begin{itemize}
   \item The efficiency of an unbiased estimator of $\beta$
    is 1 / variance.

   \item Estimators can be compared by their efficiency: the more efficient, the better.

   \item The other reason to correct for unequal variance (besides so that we get valid inference) is for efficiency.
   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Correcting for unequal variance}

   \begin{block}
   {Efficiency -- example}
   \begin{itemize}
   \item Suppose
   $$
   Z_i = \mu + \varepsilon_i, \qquad \varepsilon_i \sim N(0, i^2 \cdot \sigma^2), 1 \leq i \leq n.$$

   \item Two unbiased estimators of $\mu$:
   $$
   \begin{aligned}
   \widehat{\mu}_1 &= \frac{1}{n}\sum_{i=1}^n Z_i \\
   \widehat{\mu}_2 &= \frac{1}{\sum_{i=1}^n i^{-2}}\sum_{i=1}^n i^{-2}Z_i \\
   \end{aligned}
   $$

   \item The estimator $\widehat{\mu}_2$ will always have lower variance, hence tighter confidence intervals.

   \end{itemize}
   \end{block}
   \end{frame}

   %CODE
       % ntrial <- 2000   # how many trials will we be doing?
   % nsample <- 20   # how many points in each trial
   % sd <- c(1:20)   # how does the variance change
   % mu <- 2.0
   % 
   % get.sample <- function() {
   %   return(rnorm(nsample)*sd + mu)
   % }
   % 
   % unweighted.estimate <- function(cur.sample) {
   %   return(mean(cur.sample))
   % }
   % 
   % unweighted.estimate <- numeric(ntrial)
   % weighted.estimate <- numeric(ntrial)
   % wrongly.weighted.estimate <- numeric(ntrial)
   % 
   % for (i in 1:ntrial) {
   %   cur.sample <- get.sample()
   %   unweighted.estimate[i] <- mean(cur.sample)
   %   weighted.estimate[i] <- sum(cur.sample/sd^2) / sum(1/sd^2)
   %   wrongly.weighted.estimate[i] <- sum(cur.sample/sd) / sum(1/sd)
   % }
   % 
   % data.frame(unweighted=c(mean(unweighted.estimate),
   %                    sd(unweighted.estimate)),
   %                  weighted=c(mean(weighted.estimate),
   %                    sd(weighted.estimate)),
   %                  wrongly.weighted=c(mean(wrongly.weighted.estimate),
   %                    sd(wrongly.weighted.estimate))
   %                  )
   % 
   % 
   % Y <- c(density(unweighted.estimate)$y, density(weighted.estimate)$y, density(wrongly.weighted.estimate)$y)
   % X <- c(density(unweighted.estimate)$x, density(weighted.estimate)$x, density(wrongly.weighted.estimate)$x)
   % 
   % 
   % plot(X, Y, type='n', main='Comparison of densities of the estimators')
   % lines(density(weighted.estimate), col='red', lwd=2)
   % lines(density(unweighted.estimate), col='blue', lwd=2)
   % lines(density(wrongly.weighted.estimate), col='purple', lwd=2)
   % legend(6,0.3, c('optimal', 'mean', 'suboptimal'), col=c('red', 'blue', 'purple'), lwd=rep(2,3))


   \begin{frame}
   \frametitle{Efficiency of estimators}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/307efa66b3.png}}    
   \end{center}
   \href{http://stats191.stanford.edu/transformations.html#efficiency}{R code}
   \end{frame}

   \part{Correlated Errors, Whitening  (Ch. 8, RABE)}
   \frame{\partpage}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Correlated Erorrs}

   \begin{block}
   {Topics}
   \begin{itemize}

   \item Autocorrelation.


   \item Whitening.
   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Correlated errors}

   \begin{block}
   {Autocorrelation                     }

   \begin{itemize}

   \item In the random effects model, outcomes within groups
   were correlated.

   \item Other regression applications also have correlated outcomes (i.e. errors).

   \item Common examples: time series data.

   \item Why worry? Can lead to underestimates of SE $\rightarrow$ inflated $t$'s $\rightarrow$ false positives.

   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Correlated errors}

   \begin{block}
   {Autocorrelation                     }
   \begin{itemize}

   \item Suppose we plot Palo Alto's daily average temperature --
   clearly we would see a pattern in the data.
   \item Sometimes, this pattern can be attributed to a deterministic
   phenomenon (i.e. predictable seasonal fluctuations).

   \item Other times, ``patterns'' are due to correlations in the noise, maybe small time fluctuations in the stock market, economy, etc.



   \item Example: financial time series, monthly bond return.

   \item Example: residuals regressing consumer expenditure on money stock.
   \end{itemize}
   \end{block}
   \end{frame}

   %CODE
       % PA.temp <- read.table('http://www-stat.stanford.edu/~jtaylo/courses/stats191/data/paloaltoT.table', header=F, skip=2)
   % plot(PA.temp[,3], xlab='Day', ylab='Average Max Temp (F)', pch=23, bg='orange')


   \begin{frame}
   \frametitle{Average Max Temp in Palo Alto}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/4a013ee86e.png}}    
   \end{center}

   \end{frame}

   %CODE
       % url = 'http://stats191.stanford.edu/data/nasdaq_2011.csv'
   % nasdaq.data = read.table(url, header=TRUE, sep=',')
   % 
   % plot(nasdaq.data$Date, nasdaq.data$Close, xlab='Date', ylab='NASDAQ close',
   %      pch=23, bg='red', cex=2)


   \begin{frame}
   \frametitle{NASDAQ daily close 2011}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/40a1d58d2b.png}}    
   \end{center}
   \href{http://stats191.stanford.edu/correlated_errors.html#nasdag}{R code}
   \end{frame}

   %CODE
       % acf(nasdaq.data$Close)


   \begin{frame}
   \frametitle{NASDAQ daily close 2011, ACF}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/b8050fb529.png}}    
   \end{center}
   \href{http://stats191.stanford.edu/correlated_errors.html#nasdaq}{R code}
   \end{frame}

   %CODE
       % url = 'http://www-stat.stanford.edu/~jtaylo/courses/stats191/data/expenditure.table'
   % expenditure.table =read.table(url, header=T)
   % attach(expenditure.table)
   % 
   % plot(Stock, Expenditure, pch=23, bg='orange', cex=2)


   \begin{frame}
   \frametitle{Expenditure vs. stock}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/7f6229d84c.png}}    
   \end{center}
   \href{http://stats191.stanford.edu/correlated_errors.html#consumer-expenditure}{R code}
   \end{frame}

   %CODE
       % exp.lm = lm(Expenditure ~ Stock)
   % plot(resid(exp.lm), type='l', lwd=2, col='red')


   \begin{frame}
   \frametitle{Expenditure vs. stock: residuals}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/56669140a8.png}}    
   \end{center}
   \href{http://stats191.stanford.edu/correlated_errors.html#consumer-expenditure}{R code}
   \end{frame}

   %CODE
       % acf(resid(exp.lm))


   \begin{frame}
   \frametitle{ACF of residuals}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/25b26dc356.png}}    
   \end{center}
   \href{http://stats191.stanford.edu/correlated_errors.html#consumer-expenditure}{R code}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Correlated errors}

   \begin{block}
   {AR(1) noise}
   \begin{itemize}

   \item Suppose that, instead of being independent, the errors in our model were
   $$
   \varepsilon_t = \rho \cdot \varepsilon_{t-1} + \omega_t, \qquad -1 < \rho < 1$$
   with $\omega_t \sim N(0,\sigma^2)$ independent.
   \item If $\rho$ is close to 1, then errors are very correlated, $\rho=0$ is independence.
   \item This is ``Auto-Regressive Order (1)'' noise (AR(1)). Many other
   models of correlation exist: ARMA, ARIMA, ARCH, GARCH, etc.
   \end{itemize}

   \end{block}
   \end{frame}

   %CODE
       % ntrial = 500
   % nsample = 100
   % rho = 0.9
   % mu = 1.0
   % 
   % # This little function simulates our time series
   % 
   % get.sample = function() {
   %   return(arima.sim(list(ar=rho), nsample) + mu)
   % }
   % 
   % Z = get.sample()
   % plot(Z, lwd=2, col='red')


   \begin{frame}
   \frametitle{AR(1) noise, $\rho=0.9$}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/07d4c33fd2.png}}    
   \end{center}
   \href{http://stats191.stanford.edu/correlated_errors.html#simulating-time-series}{R code}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Correlated errors}

   \begin{block}
   {Autocorrelation function                     }
   \begin{itemize}
   \item       For a ``stationary'' time series $(Z_t)_{1 \leq t \leq \infty}$
   define
   $$
   ACF(t) = \text{ Cor}(Z_s, Z_{s+t}).$$

   \item Stationary means that correlation above does not depend on $s$.

   \item For AR(1) model,
   $$
   ACF(t) = \rho^t.$$
       \item For a sample $(Z_1, \dots, Z_n)$ from a stationary time series
   $$
   \widehat{ACF}(t) = \frac{\sum_{j=1}^{n-t} (Z_j - \overline{Z})(Z_{t+j} - \overline{Z})}{\sum_{j=1}^n(Z_j - \overline{Z})^2}.$$

   \end{itemize}

   \end{block}
   \end{frame}

   %CODE
       % acf(Z)


   \begin{frame}
   \frametitle{ACF of AR(1) noise, $\rho=0.9$}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/6201fdd299.png}}    
   \end{center}
   \href{http://stats191.stanford.edu/correlated_errors.html#simulating-time-series}{R code}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Correlated errors}

   \begin{block}
   {Effects on inference}
   \begin{itemize}
   \item So far, we have just mentioned that things {\em may} be correlated, but not thought about how it affects inference.

   \item Suppose we are in the ``one sample problem'' setting and we observe
   $$W_i  = Z_i + \mu, \qquad 1 \leq i \leq n$$
   with the $Z_i$'s from an $AR(1)$ time series.
   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Correlated errors}

   \begin{block}
   {Effects on inference}
   \begin{itemize}
   \item It is easy to see that
   $$
   E(\overline{W}) = \mu$$
   {\em BUT}, generally
   $$
   \text{Var}(\overline{W}) >  \frac{\text{Var}(Z_1)}{n}$$
   how much bigger depends on $\rho.$
   \end{itemize}
   \end{block}
   \end{frame}

   %CODE
       % sample.mean = numeric(ntrial)
   % sample.var = numeric(ntrial)
   % 
   % for (i in 1:ntrial) {
   %   cur.sample = get.sample()
   %   sample.mean[i] = mean(cur.sample)
   %   sample.var[i] = var(cur.sample)
   % }
   % 
   % data.frame(mean=mean(sample.mean), sd=sqrt(mean(sample.var)))
   % 
   % xval = seq(-5,5,0.05)
   % Y = c(density(sample.mean)$y, dnorm(xval, mean=mean(sample.mean),
   %                   sd=sqrt(mean(sample.var) / nsample)))
   % X = c(density(sample.mean)$x, xval)
   % 
   % plot(X, Y, type='n', main='Actual and "expected" density of sample mean')
   % lines(xval, dnorm(xval, mean=mean(sample.mean),
   %                   sd=sqrt(mean(sample.var) / nsample)), lwd=2, col='blue')
   % lines(density(sample.mean), lwd=2, col='red')


   \begin{frame}
   \frametitle{Misleading inference ignoring autocorrelation}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/0c676d9a91.png}}    
   \end{center}
   \href{http://stats191.stanford.edu/correlated_errors.html#simulating-time-series}{R code}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Regression with auto-correlated errors}

   \begin{block}
   {Model}
   \begin{itemize}
   \item Observations:
   $$
   Y_t = \beta_0 + \sum_{j=1}^p X_{tj} \beta_j + \varepsilon_t, \qquad 1 \leq t \leq n$$

   \item Errors:
   $$
   \varepsilon_t = \rho \cdot \varepsilon_{t-1} + \omega_t, \qquad -1 < \rho < 1$$

   \item Question: how do we determine if autocorrelation is present?

   \item Question: what do we do to correct for autocorrelation?
   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Regression with auto-correlated errors}

   \begin{block}
   {Graphical checks for autocorrelation }
   \begin{itemize}

   \item A plot of residuals vs. time is helpful.

   \item Residuals clustered above and below 0 line can indicate
   autocorrelation.

   \item Example: regressing consumer expenditure on money stock.

   \end{itemize}
   \end{block}
   \end{frame}

   %CODE
       % exp.lm = lm(Expenditure ~ Stock)
   % plot(resid(exp.lm), type='l', lwd=2, col='red')


   \begin{frame}
   \frametitle{Expenditure vs. stock: residuals}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/56669140a8.png}}    
   \end{center}
   \href{http://stats191.stanford.edu/correlated_errors.html#consumer-expenditure}{R code}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Regression with auto-correlated errors}

   \begin{block}
   {Durbin-Watson   test }

   \begin{itemize}

   \item In regression setting, if noise is AR(1), a simple estimate of $\rho$ is obtained
   by (essentially) regressing $e_t$ onto $e_{t-1}$
   $$
   \widehat{\rho} = \frac{\sum_{t=2}^n \left(e_t e_{t-1}\right)}{\sum_{t=1}^n e_t^2}.$$

   \item To formally test $H_0:\rho=0$ (i.e. whether residuals are independent vs. they are AR(1)), use Durbin-Watson test, based on
   $$
   d \approx 2(1 - \widehat{\rho}).$$
   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Regression with auto-correlated errors}

   \begin{block}
   {Correcting for AR(1)                      }
   \begin{itemize}
   \item Suppose we know $\rho$, if we ``whiten'' the data and regressors
   $$
   \begin{aligned}
   \tilde{Y}_{t+1} &= Y_{t+1} - \rho Y_t, t > 1   \\
   \tilde{X}_{(t+1)j} &= X_{(t+1)j} - \rho X_{tj}, i > 1
   \end{aligned}
   $$
   for $1 \leq t \leq n-1$.
   This model satisfies ``usual'' assumptions, i.e. the errors
   $$
   \tilde{\varepsilon}_t = \omega_{t+1} = \varepsilon_{t+1} - \rho \cdot \varepsilon_t$$
   are independent $N(0,\sigma^2)$.

   \item For coefficients in new model $\tilde{\beta}$, $\beta_0 = \tilde{\beta}_0 / (1 - \rho)$, $\beta_j = \tilde{\beta}_j.$

   \item Problem: in general, we don't know $\rho$.
   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Regression with auto-correlated errors}

   \begin{block}
   {Two-stage regression}
   \begin{itemize}
   \item Step 1: Fit linear model to unwhitened data (OLS: ordinary least squares, i.e. no pre-whitening).

   \item Step 2: Estimate $\rho$ with $\widehat{\rho}$.


   \item Step 3: Pre-whiten data using $\widehat{\rho}$ -- refit the model.
   \end{itemize}
   \end{block}
   \begin{block}
   {Other models of covariance}
   \begin{itemize}
   \item Suppose we model covariance of $\varepsilon$'s differently, i.e.
   $ARMA(p,q)$.

   \item As long as we can estimate parameters of covariance, from residuals
   of the OLS fit, we can use this two-stage procedure.

   \item This is very similar to weighted least squares.
   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Regression with auto-correlated errors}

   \begin{block}
   {Interpreting results of two-stage fit}
   \begin{itemize}
   \item Basically, interpretation is unchanged, but the exact
   degrees of freedom in the error is not exactly clear.

   \item Common argument: ``this works for large degrees of freedom, so we better hope we have enough degrees of freedom so this point is not important.''


   \item Can treat $t$-statistics as $Z$-statistics, $F$'s as $\chi^2$, appealing to asymptotics:
   \begin{itemize}
   \item $t_{\nu}$, with $\nu$ large is like $N(0,1)$;

   \item $F_{j,\nu}$, with $\nu$ large is like $\chi^2_j/j.$
   \end{itemize}
   \end{itemize}
   \end{block}
   \end{frame}

   %CODE
       % library(car) # durbin.watson is in the "car" package
   % durbin.watson(exp.lm)
   % 
   % rho = durbin.watson(exp.lm)$r
   % 
   % # whiten the data "by hand"
   % 
   % wExp = numeric(length(Expenditure) - 1)
   % wStock = numeric(length(Expenditure) - 1)
   % 
   % for (i in 2:length(Expenditure)) {
   %   wExp[i-1] = Expenditure[i] - rho * Expenditure[i-1]
   %   wStock[i-1] = Stock[i] - rho * Stock[i-1]
   % }
   % 
   % 
   % 
   % exp.whitened.lm = lm(wExp ~ wStock)
   % summary(exp.whitened.lm)
   % summary(exp.lm)
   % 
   % 
   % plot(resid(exp.whitened.lm), type='l', lwd=2, col='red')


   \begin{frame}
   \frametitle{Expenditure vs. stock: whitened residuals}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/1970cc2a75.png}}    
   \end{center}
   \href{http://stats191.stanford.edu/correlated_errors.html#consumer-expenditure}{R code}
   \end{frame}

   %CODE
       % acf(resid(exp.whitened.lm))


   \begin{frame}
   \frametitle{Expenditure vs. stock: ACF of whitened residuals}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/234922b033.png}}    
   \end{center}
   \href{http://stats191.stanford.edu/correlated_errors.html#consumer-expenditure}{R code}
   \end{frame}

   \part{Model Selection  (Ch. 11, RABE)}
   \frame{\partpage}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Topics}

   \begin{block}
   {Outline}
   \begin{itemize}


   \item Goals of model selection.

   \item Criteria to compare models.
   \item (Some) model selection.
   \item Bias- variance trade-off.

   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Election data}

   \begin{block}
   {Description}

   \begin{tabular}{c|l}
   Variable & Description \\ \hline
   $V$ & votes for a presidential candidate \\
   $I$ & are they incumbent? \\
   $D$ & Democrat or Republican incumbent? \\
   $W$ & wartime election? \\
   $G$ & GDP growth rate in election year \\
   $P$ & (absolute) GDP deflator growth rate \\
   $N$ & number of quarters in which GDP growth rate $> 3.2\%$
   \end{tabular}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Model selection}

   \begin{block}
   {Problem \& Goals}
   \begin{itemize}
   \item When we have many predictors (with many possible interactions), it can be difficult to find a good model.
   \item Which main effects do we include?
   \item Which interactions do we include?

   \item Model selection procedures try to {\em simplify / automate} this task.

   \item Election data has  $2^6=64$ different models with just main effects!
   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Model selection}

   \begin{block}
   {General comments}
   \begin{itemize}

   \item This is an ``unsolved'' problem in statistics: there are no magic procedures to get you the ``best model.''
   \item In some sense, model selection is ``data mining.''

   \item Data miners / machine learners often work with very many predictors.

   \item Our model selection problem is generally at a much smaller scale than ``data mining'' problems.

   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Model selection}

   \begin{block}
   {Hypothetical example}
   \begin{itemize}
   \item Suppose we fit a a model
   $$
   F: \quad Y_{n \times 1} = X_{n \times (p+1)} \beta_{(p+1) \times 1} + \varepsilon_{n \times 1}$$
   with predictors ${\pmb X}_1, \dots, {\pmb X}_p$.

   \item In reality, some of the $\beta$'s may be zero. Let's suppose that
   $\beta_{j+1}= \dots= \beta_{p+1}=0$.

   \item Then, any model that includes $\beta_0, \dots, \beta_j$ is {\em correct}: which model gives the  {\em best} estimates of $\beta_0, \dots, \beta_j$?

   \item Principle of {\em parsimony} (i.e. Occam's razor) says that the model
   with {\em only} $\pmb{X}_1, \dots, \pmb{X}_j$ is ``best''.

   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Model selection}

   \begin{block}
   {Hypothetical example: continued}
   \begin{itemize}

   \item For simplicity, let's assume that $j=1$ so there is only one coefficient to estimate.

   \item Then, because each model gives an {\em unbiased} estimate
   of $\beta_1$ we can compare models based on
   $$
   \text{Var}(\widehat{\beta}_1).$$

   \item The best model, in terms of
   this variance, is the one containing only ${\pmb X}_1$.

   \item What if we didn't know that only $\beta_1$ was
   non-zero (which we don't know in general)?

   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Model selection}

   \begin{block}
   {Strategies}
   \begin{itemize}

   \item To ``implement'' a model selection procedure, we first need a criterion or benchmark to compare two models.
   \item Given a criterion, we also need a search strategy.

   \item With a limited number of predictors, it is possible to search all possible models ({\tt leaps} in {\tt R}).
   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Model selection}

   \begin{block}
   {Possible criteria   }

   \begin{itemize}[<+->]

   \item $R^2$: not a good criterion. Always increase with model size $\implies$ ``optimum'' is to take the biggest model.

   \item Adjusted $R^2$: better. It ``penalized'' bigger models.
   Follows principle of parsimony / Occam's razor.

   \item Mallow's $C_p$ -- attempts to estimate a model's predictive power, i.e. the power to predict a new observation.
   \end{itemize}
   \end{block}
   \end{frame}

   %CODE
       % url = 'http://stats191.stanford.edu/data/election.table'
   % 
   % election.table = read.table(url, header=T)
   % 
   % pairs(election.table[,2:ncol(election.table)], cex.labels=3, pch=23,
   %       bg='orange', cex=2)
   % 
   % # Leaps takes a design matrix as argument: throw away the intercept
   % # column or leaps will complain
   % 
   % election.lm = lm(V ~ I + D + W +G:I + P + N, election.table)
   % X = model.matrix(election.lm)[,-1]
   % 
   % library(leaps)
   % election.leaps = leaps(X, election.table$V, nbest=3, method='r2')
   % plot(election.leaps$size, election.leaps$r2, pch=23, bg='orange', cex=2)
   % best.model.r2 = election.leaps$which[which((election.leaps$r2 == max(election.leaps$r2))),]
   % best.model.r2


   \begin{frame}
   \frametitle{Best subsets, $R^2$}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/fc8b4ea825.png}}    
   \end{center}
   \href{http://stats191.stanford.edu/selection.html#election-example-best-subset}{R code}
   \end{frame}

   %CODE
       % election.leaps = leaps(X, election.table$V, nbest=3, method='adjr2')
   % plot(election.leaps$size, election.leaps$adjr2, pch=23, bg='orange', cex=2)
   % best.model.adjr2 = election.leaps$which[which((election.leaps$adjr2 == max(election.leaps$adjr2))),]
   % best.model.adjr2


   \begin{frame}
   \frametitle{Best subsets, adjusted $R^2$}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/3cfd04f2cf.png}}    
   \end{center}
   \href{http://stats191.stanford.edu/selection.html#election-example-best-subset}{R code}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Model selection}

   \begin{block}
   {Mallow's $C_p$}
   \begin{itemize}

   \item $$
   C_p({\cal M}) = \frac{SSE({\cal M})}{\widehat{\sigma}^2} + 2 \cdot p({\cal M}) - n.$$

   \item  $\widehat{\sigma}^2=SSE(F)/df_F$ is the ``best'' estimate of $\sigma^2$ we have (use the fullest model), i.e. in the election data it uses all 6 main effects.
   \item  $SSE({\cal M})$ is the $SSE$ of the model ${\cal M}$.

   \item  $p({\cal M})$ is the number of predictors in ${\cal M}$.


   \item This is an estimate of the expected mean-squared error of $\widehat{Y}({\cal M})$, it takes {\em bias} and {\em variance} into account.
   \end{itemize}
   \end{block}
   \end{frame}

   %CODE
       % election.leaps = leaps(X, election.table$V, nbest=3, method='Cp')
   % plot(election.leaps$size, election.leaps$Cp, pch=23, bg='orange', cex=2)
   % best.model.Cp = election.leaps$which[which((election.leaps$Cp == min(election.leaps$Cp))),]
   % best.model.Cp


   \begin{frame}
   \frametitle{Best subsets, Mallow's $C_p$}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/b2d120be4a.png}}    
   \end{center}
   \href{http://stats191.stanford.edu/selection.html#election-example-best-subset}{R code}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Model selection}

   \begin{block}
   {Search strategies                     }


   \begin{itemize}[<+->]


   \item Given a criterion, we now have to decide how we are going
   to search through all possible models.
   \item ``Best subset'': search all possible models and take the one
   with highest $R^2_a$ or lowest $C_p$ {\tt leaps}

   \item Stepwise (forward, backward or both): useful when the number of predictors is large. Choose an initial model and be ``greedy''.

   \item ``Greedy'' means always take the biggest jump (up or down) in your selected criterion.
   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Model selection}

   \begin{block}
   {Implementations in \RR}

   \begin{itemize}

   \item ``Best subset'': use the function {\tt leaps}. Works only for multiple linear regression models.

   \item Stepwise: use the function {\tt step}. Works for any model with
   Akaike Information Criterion (AIC). In multiple linear regression, AIC is (almost) a linear function of $C_p$.


   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Model selection}

   \begin{block}
   {Akaike / Bayes Information Criterion}

   \begin{itemize}

   \item Akaike (AIC) defined as
   $$
   AIC({\cal M}) = - 2 \log L({\cal M}) + 2 p({\cal M})
   $$
   where $L({\cal M})$ is the maximized likelihood of the model.

   \item Bayes (BIC) defined as
   $$
   BIC({\cal M}) = - 2 \log L({\cal M}) + \log n \cdot p({\cal M})
   $$

   \item Strategy can be used for whenever we have a likelihood, so
   this generalizes to many statistical models.

   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Model selection}

   \begin{block}
   {Akaike / Bayes Information Criterion}

   \begin{itemize}


   \item In linear regression with unknown $\sigma^2$
   $$
   -2 \log L({\cal M}) = n \log(2\pi \widehat{\sigma}^2_{MLE}) + n
   $$
   where
   $$
   \widehat{\sigma}^2_{MLE} = \frac{1}{n} SSE(\widehat{\beta})
   $$


   \item In linear regression with known $\sigma^2$
   $$
   -2 \log L({\cal M}) = n \log(2\pi \sigma^2) + \frac{1}{\sigma^2} SSE(\widehat{\beta})
   $$
   so AIC is very much like Mallow's $C_p$.

   \end{itemize}


   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Model selection}

   \begin{block}
   {Akaike / Bayes Information Criterion}

   \begin{itemize}[<+->]

   \item BIC will always choose a model as small or smaller
   than AIC.

   \item As our sample size grows, we can show that
   \begin{itemize}
   \item AIC will (asymptotically) always choose a model that contains
   the true model, i.e. it won't leave any variables out.
   \item BIC will (asymptotically) choose exactly the right model.
   \end{itemize}

   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Cross-validation}

   \begin{block}
   {$K$-fold cross-validation                     }
       \begin{itemize}

       \item Fix a model ${\cal M}$.
       Break data set into $K$ approximately equal sized groups $(G_1, \dots, G_K)$.

       \item {\tt for (i in 1:K)} Use all groups
       except $G_i$ to fit model, predict  outcome in group $G_i$ based on this model $\widehat{Y}_{j,{\cal M}, G_i}, j \in G_i$.

       \item Estimate
   $$
   CV({\cal M}) = \frac{1}{n}\sum_{i=1}^K \sum_{j \in G_i} (Y_j - \widehat{Y}_{j,{\cal M},G_i})^2.$$
       \end{itemize}

   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Cross-validation}

   \begin{block}
   {Some facts about cross-validation.}
       \begin{itemize}

       \item It is a general principle that can be used in other situations to ``choose parameters.''

       \item Pros (partial list): ``objective'' measure of a model.

       \item Cons (partial list): inference is, strictly speaking, ``out the window'' (also true for other model selection procedures).

       \item If goal is not really inference about certain specific parameters, it is a reasonable way to compare models.
       \end{itemize}

   \end{block}
   \end{frame}

   %CODE
       % library(boot)
   % election.leaps = leaps(X, election.table$V, nbest=3, method='Cp')
   % V = election.table$V
   % election.leaps$cv = 0 * election.leaps$Cp
   % for (i in 1:nrow(election.leaps$which)) {
   %     subset = c(1:ncol(X))[election.leaps$which[i,]]
   %     if (length(subset) > 1) {
   %        Xw = X[,subset]
   %        wlm = glm(V ~ Xw)
   %        election.leaps$cv[i] = cv.glm(model.frame(wlm), wlm, K=5)$delta[1]
   %     }
   %     else {
   %        Xw = X[,subset[1]]
   %        wlm = glm(V ~ Xw)
   %        election.leaps$cv[i] = cv.glm(model.frame(wlm), wlm, K=5)$delta[1]
   %     }
   % }
   % plot(election.leaps$Cp, election.leaps$cv, pch=23, bg='orange', cex=2)
   % X=3


   \begin{frame}
   \frametitle{$C_p$ versus 5-fold cross-validation}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/e38c266bdd.png}}    
   \end{center}
   \href{http://stats191.stanford.edu/selection.html#election-example-best-subset}{R code}
   \end{frame}

   %CODE
       % plot(election.leaps$size, election.leaps$cv, pch=23, bg='orange', cex=2)


   \begin{frame}
   \frametitle{5-fold cross-validation for all $C_p$ models}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/dd6ac45a50.png}}    
   \end{center}
   \href{http://stats191.stanford.edu/selection.html#election-example-best-subset}{R code}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Model selection}

   \begin{block}
   {Caveats}

   \begin{itemize}[<+->]

   \item Many other ``criteria'' have been proposed.
   \item Some work well for some types of data, others for different data.

   \item Check diagnostics!

   \item These criteria (except cross-validation) are not ``direct measures'' of predictive power, though Mallow's $C_p$ is a step in this direction.

   \item $C_p$ measures the quality of a model based on both
   {\em bias} and {\em variance} of the model. Why is this important?

   \item {\em Bias-variance} tradeoff is ubiquitous in statistics.
   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Bias-variance tradeoff}

   \begin{block}
   {Comparing estimators}
   \begin{itemize}

   \item When an estimator $\widehat{\beta}_1$ of $\beta_1$ is unbiased:
   $$
   E((\widehat{\beta}_1 - \beta_1)^2) = \text{Var}(\widehat{\beta}_1)$$
   so it makes sense to compare unbiased estimators in terms of variance.

   \item Even for biased estimators, the LHS makes sense, called the
   {\em mean squared error} of $\widehat{\beta}_1$
   $$
   \begin{aligned}
   MSE(\widehat{\beta}_1) &= E((\widehat{\beta}_1 - \beta_1)^2) \\
   &= \text{Var}(\widehat{\beta}_1) + \text{Bias}(\widehat{\beta}_1)^2
   \end{aligned}
   $$
   \item Paradoxically, it is sometimes possible to reduce
   $MSE$ by {\em biasing} the estimator.

   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Bias-variance tradeoff}

   \begin{block}
   {Shrinking toward zero}
   \begin{itemize}
   \item Suppose we observe $$Y_i \sim N(\mu_i, 1), 1 \leq i \leq n$$ and our goal is to estimate the entire vector $\mu$.

   \item Minimum variance unbiased estimator is
   $$
   \widehat{\mu}_i = Y_i, \qquad 1 \leq i \leq n.$$


   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Bias-variance tradeoff}

   \begin{block}
   {Shrinking toward zero}
   \begin{itemize}

   \item How good an estimator is $\widehat{\mu}$?
   $$
   MSE(\widehat{\mu}, \mu) = \frac{1}{n} E(\sum_{i=1}^n (\widehat{\mu}_i -\mu_i)^2)  = 1.$$
   \item {\em However}, we can improve on the MSE very simply by {\em shrinking}
   $\widehat{\mu}$  toward 0.

   \item Define
   $$
   \widehat{\mu}^{\alpha}_i = \alpha \cdot Y_i, \qquad 1 \leq i \leq n, 0 \leq \alpha \leq 1.$$

   \end{itemize}
   \end{block}
   \end{frame}

   %CODE
       % nsample = 100
   % ntrial = 50
   % mu = 5 * c(1:nsample) / nsample
   % mu = mu - mean(mu)
   % 
   % get.sample = function() {
   %   return(rnorm(nsample) + mu)
   % }
   % 
   % MSE = function(mu.hat, mu) {
   %   return(sum((mu.hat - mu)^2) / length(mu))
   % }
   % 
   % alpha = seq(0.0,1,length=20)
   % 
   % mse = numeric(length(alpha))
   % 
   % for (i in 1:ntrial) {
   %   Z = get.sample()
   %   for (j in 1:length(alpha)) {
   %     mse[j] = mse[j] + MSE(alpha[j] * Z, mu) / ntrial
   %   }
   % }
   % 
   % plot(alpha, mse, type='l', lwd=2, col='red', ylim=c(0, max(mse)),
   %      xlab='Shrinkage parameter, alpha', ylab='MSE(alpha)')


   \begin{frame}
   \frametitle{Shrinking an estimator}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/6f7df230c4.png}}    
   \end{center}
   \href{http://stats191.stanford.edu/selection.html#bias-variance-tradeoff}{R code}
   \end{frame}

   \part{Logistic regression (Ch. 12, RABE)}
   \frame{\partpage}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Topics}

   \begin{block}
   {Today's class}
     \begin{itemize}


     \item Binary outcomes.

     \item Logistic regression.

     \item Generalized linear models.

     \item Deviance.

   \end{itemize}

   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Logistic regression}

   \begin{block}
       {Binary outcomes}
       \begin{itemize}
       \item Most models so far have had response $Y$ as continuous.

       \item Many responses in practice fall into the $YES/NO$ framework.

       \item Examples:
         \begin{enumerate}
         \item medical: presence or absence of cancer

         \item financial: bankrupt or solvent

         \item industrial: passes a quality control test or not
         \end{enumerate}
       \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Logistic regression}

   \begin{block}
       {Modelling probabilities}
       \begin{itemize}
       \item For $0-1$ responses we need to model
   $$
   \pi(x_1, \dots, x_p) = P(Y=1|X_1=x_1,\dots, X_p=x_p)
   $$

   \item That is, $Y$ is Bernoulli with a probability that
   depends on covariates $\{X_1, \dots, X_p\}$.

   \item {\bf Note:}
   $$
   \text{Var}(Y) = \pi ( 1 - \pi) = E(Y) \cdot ( 1-  E(Y))
   $$

   \item {\bf Or,} the binary nature forces a relation between
   mean and variance of $Y$.
       \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Logistic regression}

   \begin{block}
       {Flu shot example}
       \begin{itemize}
       \item      A local health clinic sent fliers to its clients to encourage
        everyone, but especially older persons at high risk of
        complications, to get a flu shot in time for protection
        against an expected flu epidemic.


      \item      In a pilot follow-up study, 50 clients were randomly
        selected and asked whether they actually received a flu
        shot. $Y={\tt Shot}$


      \item      In addition, data were collected on their age $X_1={\tt Age}$ and their
        health awareness $X_2={\tt Health.Aware}$

       \end{itemize}

   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Logistic regression}

   \begin{block}
       {Model for probabilities}
       \begin{itemize}
       \item Simplest model
   $$
   \pi(X_1,X_2) = \beta_0 + \beta_1 X_1 + \beta_2 X_2
   $$

   \item Problems:
     \begin{itemize}
     \item We must have $0 \leq E(Y) = \pi(X_1,X_2) \leq 1$

     \item Ordinary least squares will not work because of relation
   between mean and variance.
     \end{itemize}
       \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Logistic regression}

   \begin{block}
       {Logistic model}
       \begin{itemize}
       \item Logistic model
   $$
   \pi(X_1,X_2) = \frac{\exp(\beta_0 + \beta_1 X_1 + \beta_2 X_2)}{1 + \exp(\beta_0 + \beta_1 X_1 + \beta_2 X_2)}
   $$

   \item This automatically fixes $0 \leq E(Y) = \pi(X_1,X_2) \leq 1$.


   \item {\bf Note:}
   $$
   \text{logit}(\pi(X_1, X_2)) = \log\left(\frac{\pi(X_1, X_2)}{1 - \pi(X_1,X_2)}\right) = \beta_0 + \beta_1 X_1 + \beta_2 X_2
   $$

       \end{itemize}
   \end{block}
   \end{frame}

   %CODE
       % g <- function(x) {
   %   return(log(x / (1 - x)))
   % }
   % 
   % g.inv <- function(y) {
   %   return(exp(y) / (1 + exp(y)))
   % }
   % 
   % p = seq(g(0.01), g(0.99), length=200)
   % plot(p, g.inv(p), lwd=2, type='l', col='red')


   \begin{frame}
   \frametitle{Logistic curve}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/9aafc90f62.png}}    
   \end{center}
   \href{http://stats191.stanford.edu/logistic.html#logistic-transform}{R code}
   \end{frame}

   %CODE
       % x = seq(0.01,0.99,length=200)
   % plot(x, g(x), lwd=2, type='l', col='red')


   \begin{frame}
   \frametitle{Logistic transform}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/9b69173c09.png}}    
   \end{center}
   \href{http://stats191.stanford.edu/logistic.html#logistic-transform}{R code}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Logistic regression}

   \begin{block}
     {Binary regression models}

       \begin{itemize}

       \item Models $E(Y)$ as some increasing function of
   $\beta_0 + \beta_1 X_1 + \beta_2 X_2$.
     \item The logistic
   model uses the function $f(x)=e^x/(1+e^x)$.

       \item Can be fit using Maximum Likelihood / Iteratively Reweighted Least Squares.


       \item Coefficients have nice interpretation in terms of {\bf odds ratios}
       \item Inference (?)
   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Logistic regression}

   \begin{block}
   {Odds Ratios}

   \begin{itemize}
   \item     One reason logistic models are popular is that the
       parameters have simple interpretations in terms of {\bf odds}
   $$
   ODDS(A) = \frac{P(A)}{1-P(A)}.
   $$

   \item Logistic model:
   $$
   OR_{X_j} = \frac{ODDS(\dots, X_j=x_j+1, \dots)}{ODDS(\dots, X_j=x_j, \dots)} = e^{\beta_j}
   $$


   \item If $X_j \in {0, 1}$ is dichotomous, then odds for group with $X_j = 1$ are
       $e^{\beta_j}$ higher, other parameters being equal.

   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Logistic regression}

   \begin{block}
         {Rare disease hypothesis}
         \begin{itemize}
         \item When incidence is rare, $P(Y=0)\approxeq 1$ no matter
   what the covariates $X_j$'s are.

   \item In this case, odds ratios are almost ratios of probabilities:
   $$
   OR_{X_j} \approxeq \frac{\Pp(Y=1|\dots, X_j=x_j+1, \dots)}{\Pp(Y=1|\dots, X_j=x_j, \dots)}
   $$

   \item Hypothetical example: in a lung cancer study, if $X_j$ is an indicator of smoking or not, a $\beta_j$ of 5 means for smoking vs. non-smoking means smokers are $e^5 \approx 150$ times more likely to develop lung cancer

   \item In flu example, the odds for a 45 year old with health awareness 50
   compared to a 35 year old with the same health awareness are $e^{2.2178}=9.18$, but ratio of probs is $0.1932/0.0254 \approx 7.61$.

   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Binary regression}

   \begin{block}
   {Deviance}
   \begin{itemize}

   \item
   $$
   DEV(\mu| Y) = -2 \log L(\mu| Y) + 2 \log L(Y| Y)$$
   where $\mu$ is a  location estimator for $Y$.

   \item If $Y$ is Gaussian with independent $N(\mu_i,\sigma^2)$ entries
   $$
   DEV(\mu| Y) = \frac{1}{\sigma^2}\sum_{i=1}^n(Y_i - \mu_i)^2$$

   \item If $Y$ is a binary vector,  with mean vector $\pi$
   $$
   \begin{aligned}
   DEV(\pi| Y) &= -2 \sum_{i=1}^n \left( Y_i \log(\pi_i) + (1-Y_i) \log(1-\pi_i) \right) \\
   \end{aligned}
   $$
   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Binary regression}

   \begin{block}
   {Deviance}
   \begin{itemize}

   \item In the logistic model,
   $$
   \begin{aligned}
   DEV(\beta| Y) &=  -2 \sum_{i=1}^n \left( Y_i \logit(\pi_i(\beta)) + \log(1-\pi_i(\beta)) \right) \\
   &= -2 \sum_{i=1}^n \left(Y_i \left(\beta_0 + \sum_{j=1}^p \beta_j X_{ij} \right) + \log(1 - \pi_i(\beta)) \right)
   \end{aligned}
   $$
   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Logistic regression}

   \begin{block}
         {Fitting the model ($g(\pi) = \text{logit}(\pi)$)}
         \begin{enumerate}
         \item Initialize $\widehat{\pi}_i = \bar{Y}, 1 \leq i \leq n$

         \item Define $Z_i = g(\widehat{\pi}_i) + g'(\widehat{\pi}_i) (Y_i - \widehat{\pi_i})$

         \item Fit weighted least squares model
   $$
   Z_i = \beta_0 + \sum_{j=1}^p \beta_j X_{ij}, \qquad w_i = \widehat{\pi_i} (1 - \widehat{\pi}_i)$$

   \item Set $\widehat{\pi}_i = \text{logit}^{-1} \left(\widehat{\beta}_0 + \sum_{j=1}^p \widehat{\beta}_j X_{ij}\right)$.

   \item Repeat steps 2-4 until convergence.
         \end{enumerate}
    This is {\em basically} Newton-Raphson to minimize deviance.
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Logistic regression}

   \begin{block}
         {Inference}
         \begin{itemize}
         \item The IRLS procedure suggests using approximation
   $$
   \widehat{\beta} \approx N(\beta, (X'WX)^{-1})
   $$

   \item This allows us to construct CIs, test linear hypotheses, etc.

   \item What about comparing ${\cal M}_F$ and ${\cal M}_R$?
         \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Logistic regression}

   \begin{block}
         {Deviance}
         \begin{itemize}
         \item For a model ${\cal M}$, $DEV({\cal M})$ replaces $SSE({\cal M})$.


   \item In least squares regression, we use
   $$
   SSE({\cal M}_R) - SSE({\cal M}_F) \sim \chi^2_{df_R-df_F}$$

   \item This is replaced with
   $$
   DEV({\cal M}_R) - DEV({\cal M}_F) \overset{n \rightarrow \infty}{\sim} \chi^2_{df_R-df_F}$$

   \item Resulting tests {\bf do not} agree with those coming from IRLS (Wald tests). Both are often used.
         \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Logistic regression}

   \begin{block}
         {Other points}
         \begin{itemize}
         \item Diagnostics: similar to least square regression, only residuals
   used are {\em deviance residuals}
   $$
   r_i = \text{sign}(Y_i-\widehat{\pi}_i) \sqrt{DEV(\widehat{\pi}_i|Y_i)}.
   $$

   \item Model selection: because it is fit based on likelihood,
   stepwise selection can be used easily \dots
         \end{itemize}
   \end{block}
   \end{frame}

   %CODE
       % flu.table <- read.table('http://www-stat.stanford.edu/~jtaylo/courses/stats191/data/flu.table', header=T)
   % 
   % 
   % flu.glm = glm(Shot ~ Age + Health.Aware, data=flu.table, family=binomial())
   % print(summary(flu.glm))
   % 
   % 
   % par(mfrow=c(2,2))
   % plot(flu.glm)
   % par(mfrow=c(1,1))


   \begin{frame}
   \frametitle{Diagnostics for logistic model}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/ba8132fa7f.png}}    
   \end{center}
   \href{http://stats191.stanford.edu/logistic.html#flu-example}{R code}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Binary regression}

   \begin{block}
   {Probit transform}

    \begin{itemize}

      \item Probit regression model:
   $$
   \Phi^{-1}(\Ee(Y_i))= \beta_0 + \sum_{j=1}^{p-1} \beta_j X_{ij}$$
   where $\Phi$ is CDF of $N(0,1)$, i.e. $\Phi(t) = {\tt pnorm(t)}$.
   \item Complementary log-log model (cloglog):
   $$
   -log(-log(\Ee(Y_i)) = \beta_0 + \sum_{j=1}^{p-1} \beta_j X_{ij}.
   $$
   \item In logit, probit and cloglog $\V(Y_i)=\pi_i(1-\pi_i)$ but the model
   for the mean is different.

   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Generalized linear models}

   \begin{block}
   {Link \& variance functions}

   Given a dataset $(Y_i, X_{i1}, \dots, X_{ip}), 1 \leq i \leq n$
      we consider a model for the distribution of $Y|X_1, \dots, X_p$.
       \begin{itemize}

   \item If
   $$\eta_i=g(\Ee(Y_i)) = g(\mu_i) = \beta_0 + \sum_{j=1}^k \beta_j X_{ij}$$
   then $g$ is called the {\em link} function for the model.

   \item If
   $$
   \V(Y_i) = \phi \cdot V(\Ee(Y_i)) = \phi \cdot V(\mu_i)$$
   for $\phi > 0$ and some function $V$, then $V$ is the called {\em variance} function for the model.

   \item Canonical reference: {\em Generalized Linear Models}, McCullagh and Nelder.
       \end{itemize}
    \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Binary regression as GLM}

   \begin{block}
      {Binary (again)}
      \begin{itemize}

      \item For a logistic model, $$g(\mu)=\logit(\mu), \qquad V(\mu)=\mu(1-\mu).$$

      \item For a probit model, $$g(\mu)=\Phi^{-1}(\mu), \qquad V(\mu)=\mu(1-\mu).$$

      \item For a cloglog model, $$g(\mu)=-\log(-\log(\mu)), \qquad V(\mu)=\mu(1-\mu).$$

      \end{itemize}
   \end{block}
   \end{frame}

   \part{Poisson regression}
   \frame{\partpage}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Poisson regression}

   \begin{block}
   {Topics}
   \begin{itemize}


     \item Contingency tables.

     \item Poisson regression.


     \item Generalized linear model.

   \end{itemize}

   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Count data}

   \begin{block}
     {Afterlife}
     \begin{itemize}
     \item   Men and women were asked whether they believed in the
   after life (1991 General Social Survey).

   \item
     \begin{tabular}{l|c|c|c}
       & Y & N or U & Total \\ \hline
   M & 435 & 147 & 582 \\
   F & 375 & 134 & 509 \\ \hline
   Total & 810 & 281 & 1091
     \end{tabular}

   \item Question: is belief in the afterlife independent of gender?

     \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Poisson counts}

   \begin{block}
   {Definition}
   \begin{itemize}
   \item A random variable $Y$ is a Poisson random variable
   with parameter $\lambda$ if
   $$
   P(Y=j) = e^{-\lambda} \frac{\lambda^j}{j!}, \qquad \forall j \geq 0.
   $$
   \item Some simple calculations show that
   $$
   E(Y)=\text{Var}(Y)=\lambda.
   $$
   \item Poisson models for counts are analogous to Gaussian for continuous
   outcomes.
   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Count data}

   \begin{block}
       {Contingency table}

       \begin{itemize}
       \item     Model: $Y_{ij} \sim  Poisson(\lambda_{ij} )$.


       \item {\bf Null:}
       $H_0 : \text{independence}, \lambda_{ij} = \lambda \alpha_i \cdot \beta_j , \sum_i \alpha_i = 1,  \sum_j \beta_j = 1.$

     \item {\bf
       Alternative:} $H_a : \text{$\lambda_{ij}$ 's are unrestricted}$

   \item {\bf    Test statistic:} Pearson's $X^2$ :
   $$
   X^2 = \sum_{ij} \frac{(Y_{ij}-E_{ij})^2}{E_{ij}} \approx \chi^2_1 \  \text{under $H_0$}$$

   \item
       Why 1 df ? Independence model has 5 parameters, two
       constraints = 3 df. Unrestricted has 4 parameters.

     \item This is actually a {\em regression model} for the count data.

       \end{itemize}

   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Count data}

   \begin{block}
   {Contingency table as regression model}
   \begin{itemize}
   \item     Under independence
   $$
   \begin{aligned}
     \log(E (Y_{ij} )) &= \log \lambda_{ij} = \log \lambda  + \log \alpha_i + \log \beta_j
   \end{aligned}
   $$

   \item     OR, the model has a {\em log link}.

   \item     What about the variance? Because of Poisson assumption
          $$                  Var(Y_{ij} ) = E (Y_{ij})$$

        \item     OR, the {\em variance function} is
                             $$V (\mu) = \mu.$$
   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Count data}

   \begin{block}
       {Contingency table $(k \times m)$}
       \begin{itemize}
       \item     Suppose we had $k$ categories on one axis, $m$ on the other
       (i.e. previous example $k = m = 2$). We call this as $k \times m$
       contingency table.

     \item Independence model:
   $$        \log(E (Y_{ij} )) = \log \lambda_{ij} = \log \lambda  + \log \alpha_i + \log \beta_j$$



       \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Count data}

   \begin{block}
       {Contingency tables}
       \begin{itemize}
    \item Test for independence: Pearson's
   $$
   X^2 = \sum_{ij} \frac{(Y_{ij}-E_{ij})^2}{E_{ij}} \approx \chi^2_{(k-1)(m-1)} \  \text{under $H_0$}$$


       \item Alternative test statistic
   $$
   G = 2\sum_{ij} Y_{ij} \log \left(\frac{Y_{ij}}{E_{ij}}\right)$$
       \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Count data}

   \begin{block}
       {Independence tests}
       \begin{itemize}[<+->]
       \item Unlike in other cases, in this case the {\em full model}
       has as many parameters as observations (i.e. it's saturated).

       \item This test is known as a {\em goodness of fit} test.
       \item {\em How well does the independence model fit this data}?
       \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Count data}

   \begin{block}
       {Lumber company example}
       \begin{itemize}
       \item $Y$ : number of customers visting store from region;

    \item $X_1$ : number of housing units in region;

    \item $X_2$ : average household income;

    \item $X_3$ : average housing unit age in region;

    \item $X_4$ : distance to nearest competitor;

    \item $X_5$ : distance to store in miles.

       \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Count data}

   \begin{block}
       {Poisson (log-linear) regression model}

       \begin{itemize}
       \item      Given observations and covariates
        $Y_i , X_{ij} , 1 \leq i  \leq n, 1 \leq j  \leq p$.


      \item {\bf Model:}
   $$     Y_{i} \sim Poisson \left(\exp\left(\beta_0 + \sum_{j=1}^p \beta_j X_{ij} \right)\right)$$


   \item Poisson assumption implies the  variance
        function is
   $$ V (\mu) = \mu.$$
       \end{itemize}

   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Poisson regression}

   \begin{block}
   {Interpretation of coefficients}

   \begin{itemize}
   \item   The log-linear model
   means covariates have {\em multiplicative} effect.

   \item Log-linear model model:
   $$
   \frac{E(Y|\dots, X_j=x_j+1, \dots)}{E(Y|\dots, X_j=x_j, \dots)} = e^{\beta_j}
   $$


   \item So, one unit increase in variable $j$ results in $e^{\beta_j}$
   (multiplicative) increase the expected count, all other parameters being equal.

   \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Count data}

   \begin{block}
         {Generalized linear models}
         \begin{itemize}
         \item Logistic model:
   $$ \logit(\pi) = \beta_0 + \sum_j \beta_j X_j \qquad V(\pi)=\pi(1-\pi)$$

   \item Poisson log-linear model:
   $$
   \log(\mu) = \beta_0 + \sum_j \beta_j X_j, \qquad V(\mu) = \mu$$

   \item These are the ingredients to a GLM \dots
         \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Generalized linear models}

   \begin{block}
         {Specifying a model}
         \begin{itemize}
         \item Given $(Y, X_1, \dots, X_p)$, a GLM is specified by the
   (link, variance function) pair $(V, g)$.

   \item Fit using IRLS like logistic.

   \item Inference in terms of deviance or Pearson's $X^2$:
   $$
   X^2({\cal M}) = \sum_{i=1}^n \frac{(Y_i - \widehat{\mu}_{{\cal M},i})^2}{V(\widehat{\mu}_{{\cal M},i})}$$
         \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Generalized linear models}

   \begin{block}
         {Deviance}
         \begin{itemize}
         \item Replaces $SSE$ in least squares

         \item Definition
   $$
   DEV({\cal M}) = -2 \left(\log L(\widehat{\mu}({\cal M})|Y,X) - \log(Y|Y,X) \right)

   \item Difference between fitted values of ${\cal M}$ and "saturated model" with $\widehat{\mu}=Y$.


   \item Poisson deviance
   $$
   DEV({\cal M}|Y) = 2 \sum_{i=1}^n \left( Y_i \log \left(Y_i / \widehat{\mu}_{{\cal M},i} \right) + (Y_i - \widehat{\mu}_{{\cal M},i} ) \right)
   $$
         \end{itemize}
   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Generalized linear models}

   \begin{block}
         {Deviance tests}
         \begin{itemize}
         \item To test $H_0:{\cal M}={\cal M}_R$ vs. $H_a: {\cal M}={\cal M}_F$,
   we use
   $$DEV({\cal M}_R) - DEV({\cal M}_F) \sim \chi^2_{df_R-df_F}$$

   \item In contingency example ${\cal M}_R$ is the independence model
   $$
   \log(E(Y_{ij})) = \lambda + \alpha_i  + \beta_j$$
   with ${\cal M}_F$ being the ``saturated model.''


         \end{itemize}
   \end{block}
   \end{frame}

   \part{Bias-Variance tradeoff: penalized techniques (Ch. 10, RABE)}
   \frame{\partpage}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Penalized regression}

   \begin{block} {Topics}
     \begin{itemize}

     \item Bias-Variance tradeoff.
   \item Penalized regression.
     \item Cross-validation.

   \end{itemize}

   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Penalized regression}

   \begin{block}
   {Bias-variance tradeoff                     }
       \begin{itemize}

       \item Arguably, the goal of a regression analysis
   is to ``build'' a model that predicts well.

   \item What does ``predict well'' mean?
   $$
   \begin{aligned}
   MSE_{pop}(\model) &= \Ee \left((Y_{new} - \widehat{Y}_{new,{\cal M}})^2\right) \\
   &=
   \V(Y_{\new}) + \V(\widehat{Y}_{new,{\cal M}}) +
   \\
   & \qquad \quad \text{Bias}(\widehat{Y}_{new,{\cal M}})^2.
   \end{aligned}
   $$
       \end{itemize}
   \end{block}
   \end{frame}

   %CODE
       % nsample <- 10
   % ntrial <- 100
   % mu <- 0.5
   % 
   % 
   % MSE <- function(mu.hat, mu) {
   %   return(sum((mu.hat - mu)^2))
   % }
   % 
   % alpha <- seq(0.0,1,length=20)
   % 
   % mse <- numeric(length(alpha))
   % 
   % for (i in 1:ntrial) {
   %   Z <- rnorm(nsample) + mu
   %   for (j in 1:length(alpha)) {
   %     mse[j] <- mse[j] + MSE(alpha[j] * mean(Z), mu) / ntrial
   %   }
   % }
   % 
   % plot(alpha, mse, type='l', lwd=2, col='red', ylim=c(0, max(mse)),
   %      xlab='Shrinkage parameter, alpha', ylab='MSE(alpha)')
   % 
   % abline(v=mu^2/(mu^2+1/nsample), col='blue', lty=2)


   \begin{frame}
   \frametitle{Bias-Variance Tradeoff}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/cfa9a2a964.png}}    
   \end{center}
   \href{http://www.stanford.edu/class/stats191/selection.html#bias-variance-tradeoff}{R code}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Penalized regression}

   \begin{block}
     {Shrinkage \& Penalties}
     \begin{itemize}
     \item Shrinkage can be thought of as ``constrained'' minimization.

     \item Minimize
   $$
   \sum_{i=1}^n (Y_i - \mu)^2 \quad \text{subject to $\mu^2 \leq C$} $$

   \item Lagrange: equivalent to minimizing
   $$
   \sum_{i=1}^n (Y_i - \mu)^2 + \lambda \mu^2$$
   for some $\lambda=\lambda_C$
     \end{itemize}

   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Penalized regression}

   \begin{block}
     {Shrinkage \& Penalties}
     \begin{itemize}
   \item Differentiating:
   $$- 2 \sum_{i=1}^n (Y_i - \widehat{\lambda}_C) + 2 \lambda \widehat{\mu}_{\lambda} = 0$$

   \item Solving
   $$
   \widehat{\mu}_{\lambda} = \frac{\sum_{i=1}^n Y_i}{n + \lambda} = \frac{n}{n+\lambda} \overline{Y}.$$
     \item As
   $\lambda \rightarrow 0$,
   $$
   \widehat{\mu}_{\lambda} \rightarrow \Ybar.$$

   \item As $\lambda \rightarrow \infty$
   $$
   \widehat{\mu}_{\lambda} \rightarrow 0.$$

     \end{itemize}

   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Penalized regression}

   \begin{block}
     {Penalties \& Priors}
     \begin{itemize}
     \item Minimizing
   $$
   \sum_{i=1}^n (Y_i - \mu)^2 + \lambda \mu^2$$
   is similar to computing ``MLE'' of $\mu$ if the likelihood
   was proportional to
   $$
   \exp \left(-\frac{1}{2\sigma^2}\left( \sum_{i=1}^n (Y_i - \mu)^2 + \lambda \mu^2\right) \right).
   $$


   \item If $\lambda=m$, an integer, then $\widehat{\mu}_{\lambda}$ is the sample
   mean of $(Y_1, \dots, Y_n,0 ,\dots, 0) \in \mathbb{R}^{n+m}$.
   \item This is equivalent to adding some data with $Y=0$ \dots

     \end{itemize}

   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Penalized regression}

   \begin{block}
   {Biased regression: penalties                     }
       \begin{itemize}[<+->]

       \item Not all biased models are better -- we need a way to find ``good'' biased models.

       \item Inference ($F$, $\chi^2$ tests, etc) is not quite exact for  biased models.

       \item Generalized one sample problem: penalize large values of $\B{}$.
       This should lead to ``multivariate'' shrinkage of the vector  $\B{}$.
       \item Heuristically, ``large $\B{}$'' is interpreted as ``complex model''. Goal is really to penalize ``complex'' models, i.e. Occam's razor.

       \item If truth  really is complex, this may not work! (But, it will then be hard to build a good model anyways ... (statistical lore))

       \end{itemize}

   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Penalized regression}

   \begin{block}
         {How much to shrink, choosing $\lambda$}
         \begin{itemize}
         \item In our one-sample example,

   $$
   \begin{aligned}
   MSE_{pop}(\lambda) &= \V( \lambda \bar{Y}) + \text{Bias}(\lambda \bar{Y})^2 \\
   &= \frac{\lambda^2 \sigma^2}{n} + \mu^2 (1 - \lambda)^2
   \end{aligned}
   $$

   \item Differentiating:
   $$
   \begin{aligned}
   0 &= -2 \mu^2(1 - \lambda^*) + 2 \frac{\lambda^* \sigma^2}{n}  \\
   \lambda^* & = \frac{\mu^2}{\mu^2+\sigma^2/n}
   \end{aligned}
   $$
         \end{itemize}
   \end{block}
   \end{frame}

   %CODE
       % nsample <- 10
   % ntrial <- 100
   % mu <- 0.5
   % 
   % 
   % MSE <- function(mu.hat, mu) {
   %   return(sum((mu.hat - mu)^2))
   % }
   % 
   % alpha <- seq(0.0,1,length=20)
   % 
   % mse <- numeric(length(alpha))
   % 
   % for (i in 1:ntrial) {
   %   Z <- rnorm(nsample) + mu
   %   for (j in 1:length(alpha)) {
   %     mse[j] <- mse[j] + MSE(alpha[j] * mean(Z), mu) / ntrial
   %   }
   % }
   % 
   % plot(alpha, mse, type='l', lwd=2, col='red', ylim=c(0, max(mse)),
   %      xlab='Shrinkage parameter, alpha', ylab='MSE(alpha)')
   % 
   % abline(v=mu^2/(mu^2+1/nsample), col='blue', lty=2)


   \begin{frame}
   \frametitle{Bias-Variance Tradeoff}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/cfa9a2a964.png}}    
   \end{center}
   \href{http://www.stanford.edu/class/stats191/selection.html#bias-variance-tradeoff}{R code}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Penalized regression}

   \begin{block}
   {Ridge regression}
       \begin{itemize}

       \item Assume that columns $(X_j)_{1 \leq j \leq p-1}$ have zero mean, and length 1 and $Y$ has zero mean.

       \item This is called the {\em standardized model}.
       \item Minimize
   {\small
   $$
   SSE_{\lambda}(\B{}) = \sum_{i=1}^n \left(Y_i - \sum_{j=1}^{p-1} X_{ij} \B{j}\right)^2 + \lambda \sum_{j=1}^{p-1} \B{j}^2.$$}

   \item Corresponds (through Lagrange multiplier) to a quadratic constraint on $\B{}$'s.

       \end{itemize}

   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Penalized regression}

   \begin{block}
       {Solving the normal equations}
       \begin{itemize}
   \item Normal equations
   $$
   \frac{\partial}{\partial \B{l}} SSE_{\lambda}(\B{}) = -2  \langle Y - X\B{}, X_l \rangle + 2 \lambda \B{l}$$

       \item $$
   -2 \langle Y - X\Bh{\lambda}, X_l \rangle + 2 \lambda \Bh{l,\lambda} = 0, \qquad 1 \leq l \leq p-1$$

   \item In matrix form
   $$
   -Y^tX + \Bh{\lambda}^t (X^tX + \lambda I) = 0$$

   \item Or
   $$
   \Bh{\lambda} = (X^tX + \lambda I)^{-1} X^tY.$$
       \end{itemize}

   \end{block}
   \end{frame}

   %CODE
       % library(lars)
   % data(diabetes)
   % 
   % library(MASS)
   % 
   % diabetes.ridge <- lm.ridge(diabetes$y ~ diabetes$x, lambda=seq(0,10,0.05))
   % 
   % plot(diabetes.ridge)


   \begin{frame}
   \frametitle{Ridge regression}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/410687e961.png}}    
   \end{center}
   \href{http://www.stanford.edu/class/stats191/penalized.html#ridge-regression}{R code}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Penalized regression}

   \begin{block}
    {LASSO regression}
        \begin{itemize}
        \item Another popular penalized regression technique.

        \item Use the standardized model.
        \item Minimize
    $$
    SSE_{\lambda}(\B{}) = \sum_{i=1}^n \left(Y_i - \sum_{j=1}^{p-1} X_{ij} \B{j}\right)^2 + \lambda \sum_{j=1}^p |\B{j}|.$$

    \item Corresponds (through Lagrange multiplier) to an $\ell^1$ constraint on $\B{}$'s. In theory, it works well when many $\B{j}$'s are 0 and gives ``sparse'' solutions unlike ridge.

        \end{itemize}

   \end{block}
   \end{frame}

   %CODE
       % library(lars)
   % data(diabetes)
   % 
   % diabetes.lasso = lars(diabetes$x, diabetes$y, type='lasso')
   % plot(diabetes.lasso)


   \begin{frame}
   \frametitle{LASSO}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/5405c7af69.png}}    
   \end{center}
   \href{http://www.stanford.edu/class/stats191/penalized.html#lasso}{R code}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Penalized regression}

   \begin{block}
   {Choosing $\lambda$: cross-validation  }
       \begin{itemize}

       \item If we knew $MSE$ as a function of $\lambda$ then we would simply choose the $\lambda$ that minimizes $MSE$.

       \item To do this, we need to estimate $MSE$.

       \item A popular method is ``cross-validation.'' Breaks the data up into smaller groups and uses part of the data to predict the rest.

       \item We saw this in diagnostics: i.e.  Cook's distance measured the fit with and without each point in the data set.
       \end{itemize}

   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Penalized regression}

   \begin{block}
   {$K$-fold cross-validation                     }
       \begin{itemize}

       \item Fix a model (i.e. fix $\lambda$). Break data set into $K$ approximately equal sized groups $(G_1, \dots, G_K)$.

       \item {\tt for (i in 1:K)} Use all groups except $G_i$ to fit model, predict  outcome in group $G_i$ based on this model $\widehat{Y}_{j(i),\lambda}, j \in G_i$.

       \item Estimate
   $$
   CV(\lambda) = \frac{1}{n}\sum_{i=1}^K \sum_{j \in G_i} (Y_j - \widehat{Y}_{j(i),\lambda})^2.$$
       \end{itemize}

   \end{block}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Penalized regression}

   \begin{block}
   {$K$-fold cross-validation   (continued)                  }
       \begin{itemize}

       \item It is a general principle that can be used in other situations, not just for Ridge.


       \item Pros (partial list): ``objective'' measure of a model.

       \item Cons (partial list): inference is, strictly speaking, ``out the window'' (also true for other model selection procedures in theory).

       \item If goal is not really inference about certain specific parameters, it is a reasonable way to compare models.
       \end{itemize}

   \end{block}
   \end{frame}

   %CODE
       % cv.lars(diabetes$x, diabetes$y, K=10, type='lasso')


   \begin{frame}
   \frametitle{LASSO: cross-validation}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/28ed713a9a.png}}    
   \end{center}
   \href{http://www.stanford.edu/class/stats191/penalized.html#lasso}{R code}
   \end{frame}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \begin{frame} \frametitle{Penalized regression}

   \begin{block}
   { Generalized Cross Validation}
       \begin{itemize}

       \item A computational shortcut for $n$-fold cross-validation (also known as leave-one out cross-validation).

       \item Let
   $$
   S_{\lambda} = (X^tX + \lambda I)^{-1} X^t$$
   be the matrix in ridge regression.

   \item Then
   $$
   GCV(\lambda) =  \frac{\|Y - S_{\lambda}Y\|^2}{n - \Tr(S_{\lambda})}.$$

   \item The quantity $\Tr(S_{\lambda})$ is the {\em effective degrees of freedom}.
       \end{itemize}

   \end{block}
   \end{frame}

   %CODE
       % plot(diabetes.ridge$lambda, diabetes.ridge$GCV, xlab='Lambda', ylab='GCV', type='l', lwd=3, col='orange')
   % 
   % 
   % select(diabetes.ridge)


   \begin{frame}
   \frametitle{Ridge regression}
   \begin{center}
   \resizebox{!}{2.7in}{\includegraphics{./images/inline/6fbda6e3b3.png}}    
   \end{center}
   \href{http://www.stanford.edu/class/stats191/penalized.html#ridge-regression}{R code}
   \end{frame}

   \end{document}
