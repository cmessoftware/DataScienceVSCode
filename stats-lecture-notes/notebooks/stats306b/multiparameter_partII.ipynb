{
 "metadata": {
  "name": "multiparameter_partII"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "More examples"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "Exponential families under conditioning"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "Exponential families also behave nicely under conditioning.\n",
      "Specifically, suppose we write $\\eta=(\\eta_1,\\eta_2) \\in \\real^k \\times \\real^{p-k}$ so that\n",
      "$$\n",
      "\\frac{d\\Pp_{\\eta}}{dm_0} = e^{\\eta_1^Tt_1(x) + \\eta_2^T t_2(x) - \\CGF(\\eta_1, \\eta_2)}.\n",
      "$$\n",
      "\n",
      "Then, we might ask about the family of conditional distributions\n",
      "$$\n",
      "\\Pp_{\\eta}(t_1(X) \\in  A | t_2(X)=s_2).\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "If we suppose that $t(X)=(t_1(X), t_2(X))$ has a density $f_{T_1,T_2}$ on $\\real^p$ then we see the conditional\n",
      "density of $T_1|T_2=s_2$ has the form\n",
      "$$\n",
      "\\begin{aligned}\n",
      "\\frac{f_{T_1,T_2}(t_1,s_2)}{\\int_{\\real^{k}} f_{T_1,T_2}(s_1,s_2) \\; ds_1} \n",
      "&  = \\frac{e^{\\eta_1^Tt_1+\\eta_2^Ts_2} \\tilde{m}_0(t_1,s_2)}{\\int_{\\real^{k}} e^{\\eta_1^Ts_1+\\eta_2^Ts_2} \\tilde{m}_0(s_1,s_2)  \\; ds_1} \\\\\n",
      "&= \\frac{e^{\\eta_1^Tt_1} \\tilde{m}_0(t_1,s_2)}{\\int_{\\real^{k}} e^{\\eta_1^Ts_1} \\tilde{m}_0(s_1,s_2)  \\; ds_1} \\\\\n",
      "\\end{aligned}\n",
      "$$\n",
      "where $\\tilde{m}_0$ is the push-forward of $m_0$ under $t:\\Omega \\rightarrow \\real^2$.\n",
      " "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "This is a $k$-parameter exponential family: \n",
      "\n",
      "1. The reference measure has density $\\tilde{m}_0(t_1,s_2)$ with respect to $dt_1$, Lebesgue measure on $\\real^k$.\n",
      "\n",
      "2. The sufficient statistic is $t_1$.\n",
      "\n",
      "3. The CGF is \n",
      "$$\n",
      "\\log \\left(\\int_{\\real^{k}} e^{\\eta_1^Ts_1} \\tilde{m}_0(s_1,s_2)  \\; ds_1 \\right).\n",
      "$$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Example: the Poisson trick"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "Suppose we observe independent $X_i \\sim \\text{Poisson}(\\mu_i), 1 \\leq i \\leq k$.\n",
      "This is a $k$-parameter exponential family of distributions on $\\real^k$ \n",
      "$$\n",
      "\\frac{d\\Pp_{\\eta(\\mu)}}{dx} = \\prod_{i=1}^k e^{x_i \\log \\mu_i - \\mu_i} = \\exp \\left(\\sum_i x_i \\eta_i - e^{\\eta_i} \\right). $$  "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "The reference measure has density\n",
      "$$\n",
      "\\frac{1}{\\prod_{i=1}^k x_i!} \n",
      "$$\n",
      "with respect to counting measure on $\\mathbb{Z}^k$ restricted to the non-negative orthant."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "From this family, we can form a new family of distributions on $\\real^{k+1}$ by considering the push forward under\n",
      "$$\n",
      "f(x_1, \\dots, x_k) = \\left(x_1, \\dots, x_k, \\sum_{i=1}^k x_i\\right).\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "The push forward of $m_0$ under $f$ will be counting measure restricted to\n",
      "$$\n",
      "\\left\\{(x_1, \\dots, x_{k+1}): x_i \\geq 0, \\sum_{i=1}^k x_i = x_{k+1} \\right\\} \n",
      "$$\n",
      "with the same density as above."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "From the general picture for conditioning, we see that $(X_1, \\dots, X_k) | \\sum_{i=1}^k X_i$\n",
      "is a $k$-parameter exponential family with sufficient statistic $(x_1, \\dots, x_k)$.\n",
      "\n",
      "The marginal distributon of $\\sum_{i=1}^k X_i$ is of course $\\text{Poisson}\\left(\\sum_{i=1}^k \\mu_i \\right) = \\text{Poisson} \\left(\\sum_{i=1}^k \\log \\eta_i \\right)$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "Hence, for $(x_1, \\dots, x_k) \\in A_{n,k}$ we see\n",
      "$$\n",
      "\\Pp\\left((X_1, \\dots, X_k)=(x_1, \\dots, x_k) \\bigl| \\sum_{i=1}^k X_i=n \\right) = \n",
      "\\binom{n}{x_1, \\dots, x_k} \\exp \\left(\\sum_{i=1}^k \\eta_i x_i \\right)\n",
      "$$\n",
      "\n",
      "This is just the multinomial p.m.f. No surprise here..."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Example: the Dirichlet from independent Gammas"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "Suppose we observe independent $X_i \\sim \\text{Gamma}(1,\\alpha_i), 1 \\leq i \\leq k$ (i.e. scale 1 but shape parameter\n",
      "$\\alpha_i$. This is an exponential family of distributions on $\\real^k$ \n",
      "$$\n",
      "\\frac{d\\Pp_{\\eta}}{dx} = \\prod_{i=1}^k e^{\\alpha_i \\log(x_i) - \\log \\Gamma(\\alpha_i)} \\frac{e^{-x_i} 1_{[0,\\infty)}(x_i)}{x_i}\\; dx_i.\n",
      "$$                                              "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "From this family, we can form a family of distributions on $\\real^{k+1}$ by considering\n",
      "$$\n",
      "f(x_1, \\dots, x_k) = \\left(x_1, \\dots, x_{k}, \\sum_{i=1}^kx_i \\right).\n",
      "$$\n",
      "\n",
      "This is again an exponential family (see exercise). The marginal density of the sum of independent Gamma random variables (of fixed scale) is another Gamma with scale 1 and shape parameter $\\sum_{i=1}^k \\alpha_i$. \n",
      "\n",
      "This leads to the conclusion that\n",
      "$$\n",
      "(X_1, \\dots, X_k) \\bigl| \\sum_{i=1}^k \\sim \\text{Dirichlet}(\\alpha).\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "exercise": {
       "input": true,
       "start": true
      },
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "### *Exercise: behaviour under affine transformation*\n",
      "\n",
      "Suppose we observe independent $X_i \\sim \\text{Gamma}(1,\\alpha_i), 1 \\leq i \\leq k$ (i.e. scale 1 but shape parameter\n",
      "$\\alpha_i$. Set\n",
      "\n",
      "\n",
      "1. Set\n",
      "$$\n",
      "f(x_1, \\dots, x_k) = \\left(x_1, \\dots, x_{k}, \\sum_{i=1}^kx_i \\right).\n",
      "$$\n",
      "Show that the push-forward of the the original exponential family of distributions on $\\real^k$ is an exponential family of distributions on $\\real^{k+1}$. What is the sufficient statistic? \n",
      "\n",
      "2. What is the dimension of the natural parameter space (i.e. how many parameters are there)?\n",
      "\n",
      "3. What is the reference measure?\n",
      "\n",
      "4. Suppose $g(x)=Ax+b$. Give a sufficient condition on $(A,b)$ so that the push forward of an exponential family is still an exponential family. Give an example of $(A,b)$ for which the push forward fails to be an exponential family."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "exercise": {
       "input": true,
       "start": true
      },
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "### *Exercise: conditioning in the general case*\n",
      "\n",
      "1. Give a general formula for \n",
      "$$\n",
      "\\Pp_{\\eta}\\left(t_1(X) \\in  A \\bigl| t_2(X) \\right).\n",
      "$$\n",
      "Show that it is an exponential family.\n",
      "\n",
      "2. What is its sufficient statistic?\n",
      "\n",
      "3. What is its reference measure? Be formal about it: what is the sample \n",
      "space? What is the measure?\n",
      "\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Ising Models"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "The [Ising model](http://en.wikipedia.org/wiki/Ising_model) is an extensively studied object in statistical physics. In statistical\n",
      "settings, it has \n",
      "applications to [image analysis](http://www.jstor.org/stable/2345426). \n",
      "For us, it is an example. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "The sample space of the Ising model is based on a graph $G=(V,E)$ specified\n",
      "by a set of vertices $V$ and a set of undirected edges $E$. The sample\n",
      "space is $\\Omega_V=\\{-1,1\\}^V \\subset \\mathbb{Z}^V$ with reference measure counting measure \n",
      "restricted to $\\Omega_V$. The density with respect to this reference is\n",
      "$$\n",
      "\\exp \\left(\\sum_{i \\in V} Q^1_i x_i + \\sum_{(i,j) \\in E} Q^2_{ij} x_i x_j \\right).\n",
      "$$\n",
      "The natural parameters are therefore $(Q^1, Q^2) \\in \\real^V \\times \\real^{V \\times V}$ and \n",
      "the sufficient statistics are $(x, xx^T) \\in \\mathbb{Z}^V \\times \\mathbb{Z}^{V \\times V}$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "We see that the CGF is\n",
      "$$\n",
      "\\CGF(Q^1,Q^2) = \\sum_{x \\in \\{-1,1\\}^V} \\exp \\left(\\sum_{i \\in V} Q^1_i x_i + \\sum_{(i,j) \\in E} Q^2_{ij} x_i x_j \\right).\n",
      "$$\n",
      "\n",
      "In general, this is quite a complicated function so we see that not all exponential\n",
      "families have tractable CGF (not that we didn't know this already).\n",
      "                             \n",
      "                          "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "While we won't touch on it too much right now, what makes things possible for this model, and other\n",
      "models with intractable CGFs is that it is often possible to simulate from the distribution\n",
      "$\\Pp_{(Q^1,Q^2)}$ which makes it possible to compute an unbiased estimate of\n",
      "$\\nabla \\CGF_{(Q^1,Q^2)}.$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "This is the basis of *stochastic optimization*.\n",
      "    \n",
      "1. Given a procedure that takes arguments $(Q^1,Q^2)$ and produces a random vector\n",
      "$D(Q^1,Q^2)$ with\n",
      "    $$\n",
      "    \\Ee_{(Q^1,Q^2)} \\left(D(Q^1,Q^2) \\right) = \\nabla \\CGF_{(Q^1,Q^2)}\n",
      "    $$\n",
      "    \n",
      "2. A stochastic optimization procedure has the form\n",
      "$$\n",
      "    (Q^{1 \\; (k+1)}, Q^{2 \\; (k+1)}) = (Q^{1 \\; (k)}, Q^{2 \\; (k)}) - \\alpha_k \\left[D(Q^{1 \\; (k)}, Q^{2 \\; (k)}) - t(X) \\right]\n",
      "$$\n",
      "    where the $\\alpha_k$ satisfy some growth assumptions, usually roughly of the form\n",
      "    $$\n",
      "\\begin{aligned}\n",
      "\\alpha_k &\\overset{k \\rightarrow \\infty}{\\to} 0 \\\\\n",
      "\\sum_{j=1}^k \\alpha_j &\\overset{k \\rightarrow \\infty}{\\to} \\infty \\\\\n",
      "\\sum_{j=1}^k \\alpha_j^2 &\\overset{k \\rightarrow \\infty}{<} \\infty  \\\\\n",
      "\\end{aligned}\n",
      "    $$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Markov random fields"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "The Ising model is an example of a something called a Markov random field. \n",
      "The descriptor Markov relates to a certain type of Markov property, that is a\n",
      "type of conditional independence. \n",
      "\n",
      "In the Ising model, suppose we consider the distribution of $x_i$ for some fixed\n",
      "$i \\in V$, conditional on $x_{V\\setminus i}=x_{-i}$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "As above, this is an exponential family, but what are its parameters?\n",
      "First of all, note that the sample space depends on $x_{-i}$\n",
      "$$\n",
      "\\Omega_{x_{-i}} = \\left\\{y \\in \\{-1,1\\}^V: y_{-i}=x_{-i} \\right\\} \n",
      "$$\n",
      "which is sort of \"equivalent\" to $\\{-1,1\\}^i$ but not quite the same. \n",
      "\n",
      "There are two points in $\\Omega_{x_{-i}}$, and we\n",
      "can take the reference measure to be counting measure on\n",
      "these two points."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "With that, we see that the CGF is\n",
      "$$\n",
      "\\log \\left(\\sum_{y \\in \\Omega_{x_{-i}}} \\exp \\left(\\sum_{j \\in V} Q^1_j y_j + \\sum_{(j,k) \\in E} Q^2_{ij} y_i y_j \\right)\\right).\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "As a function on $\\Omega_{x_{-i}}$,\n",
      "$$\n",
      "\\begin{aligned}\n",
      "\\sum_{j \\in V} Q^1_j y_j &= Q^1_i y_i + C_1 \\\\\n",
      "\\sum_{(j,k) \\in E} Q^2_{ij} y_i y_j &= \\sum_{(i,k) \\in E} Q^2_{ik} y_i y_j + \\sum_{(j,i) \\in E} Q^2_{ji} y_j y_i + C_2 \\\\\n",
      "\\end{aligned}\n",
      "$$\n",
      "where  $C_1, C_2$ are constant on $\\Omega_{x_{-i}}$. If we assume\n",
      "$G^2_{ij}$ is symmetric (which we might as well) then\n",
      "$$\n",
      "\\sum_{(j,k) \\in E} Q^2_{jk} y_j y_k = 2 y_i \\sum_{(i,k) \\in E} Q^2_{ik} y_k  + C.\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "exercise": {
       "input": true,
       "start": true
      },
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Finally, for the Ising model we see that the CGF of $x_i$ under this measure is\n",
      "$$\n",
      "\\log \\left(e^{Q^1_i + 2 \\sum_{(i,k) \\in E} Q^2_{ik} x_k} + e^{-Q^1_i - 2 \\sum_{(i,k) \\in E} Q^2_{ik} x_k} \\right) + C\n",
      "$$\n",
      "Let's write this CGF as\n",
      "$\\CGF\\left(Q^1,Q^2 \\bigl| x_{-i}\\right)$.\n",
      "\n",
      "This is the CGF of a $\\{1,-1\\}$ valued random variable with natural parameter\n",
      "$$\n",
      "\\eta(x_{-i},Q^1, Q^2) = Q^1_i + 2 \\sum_{(i,k) \\in E} Q^2_{ik} x_k.\n",
      "$$\n",
      "and counting measure on $\\{-1,1\\}$ as reference measure.\n",
      " "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "exercise": {
       "input": true,
       "start": false
      },
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "### *Exercise: natural parameter under conditioning*\n",
      "\n",
      "The notation $\\eta(x_{-i}, Q^1, Q^2)$ suggests that the natural parameter \n",
      "corresponding to sufficient statistic $x_i$,\n",
      "when conditioning on $x_{-i}$, has changed. Does this conflict with what we saw earlier about\n",
      "conditioning?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "What we can take away from this picture is that conditioning on\n",
      "$x_{-i}$ yielded a new exponential family is a function of the original\n",
      "natural parameters $(Q^1,Q^2)$ and the value of $x_{-i}$. \n",
      "\n",
      "Also, the natural parameter depends on $x_{-i}$ only through\n",
      "the value at neighbours of $i$ in $G$. This is the afore-mentioned Markov property."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Definition of a Markov random field"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "A *Markov random field* is a generalization of the Ising to more general\n",
      "sample spaces, and more complicated interactions. We will not dwell on them too much\n",
      "here but, in their most natural form, they are exponential families of distributions\n",
      "on something like $\\real^V$ for some set of vertices (though they could take on values other than $\\real$)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "The sufficient statistics are specified by subsets $A \\subset V$\n",
      "$$\n",
      "f_A(x) = g_A(x_i, i \\in A)\n",
      "$$\n",
      "(i.e. $f_A$ is measurable with respect to $\\sigma(x_i, i \\in A)$)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "The general form of the density is\n",
      "$$\n",
      "\\frac{d\\Pp_{\\eta}}{dm_0} = \\exp \\left(\\sum_{A \\subset V} \\eta_A f_A(x) - \\CGF(\\eta) \\right)\n",
      "$$\n",
      "with $\\eta=\\left(\\eta_A\\right)_{A \\subset V}$ where $m_0$ is some\n",
      "reference measure on $\\real^V$. This is a huge natural parameter space, so \n",
      "obviously some restrictions are made by constraining many the $\\eta_A$ to be 0. Let's call this a *model* ${\\cal M}$, i.e. a set of $A$ such that $\\eta_A$ is not constrained to be 0.\n",
      "\n",
      "By default, we assume ${\\cal M}$ is monotone, i.e. $B \\in {\\cal M}, A \\subset B \\implies A \\in {\\cal M}$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "exercise": {
       "input": true,
       "start": true
      },
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "### *Exercise: Ising model as random field*\n",
      "\n",
      "Write the Ising model above as a Markov random field field above. Be specific as possible.\n",
      "\n",
      "1. What are the $f_A$'s?\n",
      "\n",
      "2. What are the $\\eta_A$'s?\n",
      "\n",
      "3. What is the reference measure?\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "The Markov property of these random fields can be expressed in terms of \n",
      "the Markov neighbourhood of $i \\in V$. We define this (assuming monotonicity of ${\\cal M}$) as\n",
      "$$\n",
      "N(i, {\\cal M}) = \\left\\{j:\\{i,j\\} \\in {\\cal M} \\right\\}.\n",
      "$$\n",
      "\n",
      "Then, the Markov property can be stated as \n",
      "$$\n",
      "\\eta(x_{-i},\\eta) \\in \\sigma \\left(x_j, j \\in N(i, {\\cal M}) \\right).\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "Or, conditioning on $x_{-i}$ yields an exponential family whose (random) natural parameter\n",
      "depends only on the neighbours of $i$ in the model ${\\cal M}$ and the true underlying $\\eta$.\n",
      "\n",
      "This property has some very useful consequences, particularly when the \n",
      "resulting exponential family has a simple form."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Pseudo-likelihood"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "In the Ising model, the full CGF is very complicated if $V$ is of any reasonable size. But, \n",
      "the conditional CGFs of $x_i|x_{-i}$ are particularly simple. \n",
      "\n",
      "This is the basis of the *pseudo-(log)likelihood* for $(Q^1,Q^2)$  in the Ising model\n",
      "$$\n",
      "\\ell_{\\text{pseudo}}(Q^1,Q^2) = \\sum_{i \\in V} \\left[\\eta(x_{-i},Q^1,Q^2)x_i - \n",
      "\\CGF(Q^1,Q^2|x_{-i})\\right]. \n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "exercise": {
       "input": true,
       "start": true
      },
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "### *Exercise: Ising model pseudolikelihood*\n",
      "\n",
      "1. Write out the pseudolikelihood for the Ising model as explicitly as possible.\n",
      "\n",
      "2. Is it convex in $(Q^1, Q^2)$?\n",
      "\n",
      "3. Describe a Newton-Raphson algorithm to estimate $(Q^1, Q^2)$ based on maximizing the pseudo-likelihood. Be as specific as possible, i.e. compute gradients and Hessians as explicitly as possible."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Gibbs sampler"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "This simple form of the conditional distributions is also the basis\n",
      "of one of the most natural forms of MCMC algorithms, the [Gibbs sampler](http://en.wikipedia.org/wiki/Gibbs_sampling).\n",
      "\n",
      "The Gibbs sampler (for the Ising model, say) is a Markov chain on $\\{-1,1\\}^V$ whose stationary distribution is \n",
      " $\\Pp_{(Q^1,Q^2)}$. The algorithm continuously cycles through the\n",
      "coordinates of $V$ in some (possibly random) order and each step consists of drawing from \n",
      "$$\n",
      "\\Pp_{(Q^1,Q^2)}(X_i \\in \\cdot \\bigl| X_{-i}).\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "That is, if at the $k$-th step we are updating coordinate $i$, we set\n",
      "$$\n",
      "\\begin{aligned}\n",
      "X^{(k+1)}_i &\\sim \\Pp_{(Q^1,Q^2)}(X_i \\in \\cdot \\bigl| X_{-i})\\\\\n",
      "X_{-i}^{(k+1)}&=X_{-i}^{(k)}.\n",
      "\\end{aligned}\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "exercise": {
       "input": true,
       "start": true
      },
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "### *Exercise: sampling from an Ising model*\n",
      "\n",
      "Consider an Ising model on $L$, the $100 \\times 100$ lattice in $\\mathbb{Z}^2$ with $Q^1=\\alpha \\cdot \\pmb{1}, Q^2=\\beta \\cdot  \\pmb{1} \\pmb{1}^T$. \n",
      "\n",
      "1. For $\\beta=0$, $\\alpha=1$. initialize the Gibbs sampler at some random initial condition. Run the Gibbs sampler Markov chain on $\\{-1,1\\}^L$ for some time. What do you expect the binary image to look like?\n",
      "\n",
      "2. Repeat for $\\beta > 0$ and $\\beta < 0$. \n",
      "\n",
      "Note: I am not asking for an exhaustive simulation, the goal is to just get the basic mechanics of a Gibbs sampler."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}